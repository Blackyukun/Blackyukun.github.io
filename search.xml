<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[八月]]></title>
    <url>%2F2017%2F08%2F27%2F%E5%85%AB%E6%9C%88%2F</url>
    <content type="text"><![CDATA[今天是八月二十七号，还有四天这个月就结束了。当然八月的离去，就是九月的到来。 这个月发生了很多事，吴京的《战狼2》火了，炉石传说出冰冠堡垒了，著名的电影公司暴雪又出新CG了，郭敬明也开始蠢蠢欲动了… 对于我来说，这个月我想通了一件事，决定考研。虽然我知道可能太迟了，但是我还是要去做，如果失败了就接着来。所以博客会在后面很长时间内，更新很慢。其实这也没什么，毕竟过来浏览的也没几个人。看了看考研的科目，和需要的知识范围，心里还算比较清楚自己的状态，因为基本上所有的科目都需要重新去学。这个时候一想到大学几年的荒废就感到头痛，这世界还是很公平的，你快乐过了，就该痛苦了。 当然这种事是不能以事后人的身份去对待的，就比如之前一个高中同学发给我一张高中时的照片。 照片中的我，嘴角微微上扬，闭着双目，很是安详。课座上放着还在背的英语 3500 词，快要高考了，气愤很是压抑，大家都在认真学习。同学接着发了一条信息说，如果你没有在课桌前睡着，是不是现在是在清华了。听后，我颇感有理，心中顿时想到如果十年前，在往前的那个夏天我能够认真呆在家里写暑假作业，是不是现在我已经迎娶白富美，出任 CEO，走上人生巅峰了，想到这丝丝心痛。 其实，这就是典型的事后诸葛亮。生活没有如果当初，活在过去只会止步不前。一代宗师中章子怡说，宁在一思进，莫在一思停。与其悔恨当初不如做好现在，著名的文人鲁迅还是哪个谁说，种一颗树最好的时候是十年前，其次是现在。 下个月就大四了，希望在大四能找到一个实习工作。 明天就是虐狗哦不对，明天就是七夕节了，当然对于我来说是没有什么意义的。其实所有的节日对于我来说都不会有什么意义，毕竟所有的节日，其实都是购物节和秀恩爱节。 最后给大家分享一首歌。 new APlayer({ element: document.getElementById("aplayer0"), narrow: false, autoplay: false, showlrc: 0, music: { title: "逆鳞", author: "周杰伦", url: "http://opxib6gmc.bkt.clouddn.com/nilin.mp3", pic: "http://opxib6gmc.bkt.clouddn.com/nilin.jpg", } });]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我看小说的时候我在看什么]]></title>
    <url>%2F2017%2F08%2F02%2F%E6%88%91%E7%9C%8B%E5%B0%8F%E8%AF%B4%E7%9A%84%E6%97%B6%E5%80%99%E6%88%91%E5%9C%A8%E7%9C%8B%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[最近无聊的时候逛知乎收到几个伙伴的私信，是我之前在专栏写的一个小说网站的代码不能用了。由于我之前说过不在更新那个项目了，所以我也没想去改，我知道大部分原因是爬虫抓取的小说网站更新了，爬虫代码用不了了。然后我看了一下之前的代码，我靠，怎么跟屎一样。这样的代码真的对不起认真阅读的伙伴，于是我将github上的代码全部删了，就重新实现了一下。 实现实现的大部分方式都写在知乎上，如果大家感兴趣不妨看一下：zhihu 实现后的效果如下： 首页： 搜索结果页： 章节页： 文章页： 主要使用xpath加requests抓取顶点网的小说数据，利用flask写web端，使用bootstrap做前端页面。 项目地址： 小说网站：MyNovels 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(13):Scrapy实战抓取网易云音乐]]></title>
    <url>%2F2017%2F07%2F29%2FPython%E7%88%AC%E8%99%AB%E5%8D%81%E4%B8%89%E4%B9%8BScrapy%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%2F</url>
    <content type="text"><![CDATA[前两篇文章我们了解了Scrapy的理论知识，那么我们不能做纸上谈兵的赵括。实践才是检验真理的唯一标准。本篇文章我们来抓取网易云音乐的所有音乐及音乐的热评。 分析站点我们打开浏览器，访问网易云音乐的网页端。如果我们想要抓取到所有的音乐，就得有一个切入口，能够获得到所有的音乐数据。 那么通过观察页面的导航，我们只能通过获取全部的歌手。但是由于歌手详情页并没有全部音乐这个链接，我们只能获取全部的专辑。在通过全部的专辑获得全部的音乐。 爬虫流程以歌手页为索引页，抓取所有的歌手；通过所有的歌手抓取全部专辑；通过全部专辑抓取所有的音乐；分析所有音乐的Ajax，获得所有热评；将音乐名，歌手，专辑，热评，热评作者，热评赞数保存数据库。 开始创建项目scrapy startproject 163music 创建爬虫文件（可以通过命令行创建）：1234567# spiders/spider.pyfrom scrapy import Spiderclass MusicSpider(Spider): name = "music" allowed_domains = ["163.com"] base_url = 'https://music.163.com' 确定数据名称我们先将要保存下来得到数据写到item文件中，虽然这一步不是必须先写，但是我们按照流程来不会错。 1234567891011#items.pyimport scrapyclass MusicItem(scrapy.Item): # define the fields for your item here like: # 我们保存歌曲的id id = scrapy.Field() artist = scrapy.Field() album = scrapy.Field() music = scrapy.Field() comments = scrapy.Field() 分析索引页我们的索引页为歌手页，地址：https://music.163.com/#/discover/artist/cat?id=1001&amp;amp;initial=65 通过图片结合我们对索引页的观察，我们可以看到左侧比如华语男歌手，欧美男歌手是分类，而歌手下的ABCDE也是一个按姓名的分类。 通过观察链接可以发现，id就是左侧分类的值，initial是 ABCDE 链接的值。 我们可以发现 ABCDE 每一个链接是从 65 开始，一直到 90，再加上’其他’链接为 0。这样的规则我们是可以用代码很简单实现的。而左侧的歌手分类的数字相对是不好用代码实现他的规则的。索性他的数目不多，我们一个一个写出来保存集合就可以了。我们将这两个参数写到爬虫类中。 123456class MusicSpider(Spider): name = "music" allowed_domains = ["163.com"] base_url = 'https://music.163.com' ids = ['1001','1002','1003','2001','2002','2003','6001','6002','6003','7001','7002','7003','4001','4002','4003'] initials = [i for i in range(65, 91)]+[0] 起始url很显然歌手页有不同的分类，所有起始页不可能是单独的一个url，所以我们要重写start_requests。也就是构建所有的歌手分类页。 12345def start_requests(self): for id in self.ids: for initial in self.initials: url = '&#123;url&#125;/discover/artist/cat?id=&#123;id&#125;&amp;amp;initial=&#123;initial&#125;'.format(url=self.base_url,id=id,initial=initial) yield Request(url, callback=self.parse_index) 这一步实现起来逻辑还是很清晰的，循环每一个id，在循环每一个initial，将他们通过.format方法组成url。然后使用yield语法糖，将url回调给索引页解析函数。相信大家在前两篇理论的梳理下，对于这步操作没有什么问题。 那么我们在parse_index()函数中打印一下Response： 12def parse_index(self, response): print(response.text) 控制台运行爬虫：scrapy crawl music 由于scrapy不支持lde运行，所以如果我们非要想在比如pycharm中运行的话，我们需要编写一个运行程序： 12345# 163music/entrypoint.py# 注意这个文件在项目的根目录，也就是scrapy.cfg文件所在# 这里的music就是爬虫的名字from scrapy.cmdline import executeexecute(['scrapy', 'crawl', 'music']) 现在我们在pycharm做运行这个文件就相当于运行爬虫了。 运行成功，但是我们好像并没有获得到我们想要的数据。这是怎么回事呢？大家如果回忆使用Requests库请求的时候，我们在请求中有时候会添加一些请求头，那么scrapy中我们要在哪里添加呢。 答案很简单，就是在settings.py文件中。 添加请求头设置我们需要在settings文件中先取消掉DEFAULT_REQUEST_HEADERS的注释，因为scrapy默认我们不需要请求头。我们在里面添加网易云的头部请求，就是我们开发者工具里的数据： 1234567891011121314DEFAULT_REQUEST_HEADERS = &#123; 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6', 'Cache-Control': 'no-cache', 'Connection': 'keep-alive', 'Cookie':'_ntes_nuid=5e2135ea19041c08d61bddbb9009de63; _ntes_nnid=a387121ca9ed891dca82492f6c088c57,1483420952257; __utma=187553192.690483437.1489583101.1489583101.1489583101.1; __utmz=187553192.1489583101.1.1.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided); __oc_uuid=ff821060-097f-11e7-8c2a-73421a9a1bc4; mail_psc_fingerprint=032ad52396a72877e07f21386dee35a2; NTES_CMT_USER_INFO=106964635%7C%E6%9C%89%E6%80%81%E5%BA%A6%E7%BD%91%E5%8F%8B06o2qr%7Chttps%3A%2F%2Fsimg.ws.126.net%2Fe%2Fimg5.cache.netease.com%2Ftie%2Fimages%2Fyun%2Fphoto_default_62.png.39x39.100.jpg%7Cfalse%7CbTE1MTUyMzQ3Mjc3QDE2My5jb20%3D; usertrack=c+5+hlkgTIMgjwa+EDUGAg==; _ga=GA1.2.690483437.1489583101; Province=025; City=05278; NTES_PASSPORT=aXWcpL4bYTLQnXY4eO888VlwXt.v922HPG1pBkj.vkeDwsISwc4gjpib7gtylUsoCy.yIGuJPZg7Uq2lTWqIo3A5ddE7eIf5DP_mjdHrg7ky2KFIZHP60ge8g; P_INFO=m15152347277@163.com|1500267468|1|blog|11&amp;amp;10|jis&amp;amp;1499527300&amp;amp;mail163#jis&amp;amp;320800#10#0#0|151277&amp;amp;1|study&amp;amp;blog&amp;amp;photo|15152347277@163.com; UM_distinctid=15d4ee58fc9483-032aae6568b355-333f5902-100200-15d4ee58fca912; NTES_SESS=35juNvuVAClEtPfwjy5rP5GVXVpRFMmwg2ItfudhfLmyGTk4G2l_fIFHi_xsOJTWQrUJvW3JwsMFyepEs0SR6z1_QnKjbQFaesBY9ABy0TVFP_KIiXNgb89wCGe.3_hmKR90f2ybdvNPWqPX8_YesVlIQrWdw5Nfg6KF0EcoVXO3DgV09cJHAeiE_; S_INFO=1500623480|1|0&amp;amp;80##|m15152347277; ANTICSRF=dd45f2a4489d303de869d820a0dadf05; playerid=64643457; JSESSIONID-WYYY=oR0Q0Ce%2Bhldid%2FFtfsiobsg%5Cecyra1qnHBuFFPNBUW%2BbZ3%5C2uq5%2Fqz4VrhRll0%5CaVCfY%2Fg0%2BC47vS%5Cv6rsyuD76tlqWN%2BUryVxph9fZeCmVIDtu5so7vdcdp%2B92hI3A0R5Zm%2Besa5l3ND%5Cz59WOYTY%2FCUjG%2B8gFSGVyzTpMquPQIxyIM%3A1500647790286; _iuqxldmzr_=32; MUSIC_U=f5333454d16d0f0ca5e59b3a82afaabcb107f5e73a4504bae87278f38158d65dbef309e3badc0bfac257abd5a88c5d62dc7e2cf554b1b3fc233a987fb3c42671e386323209b86ec1bf122d59fa1ed6a2; __remember_me=true; __csrf=5cd5b19efc6ea479e298487216162acf; __utma=94650624.776578804.1489210725.1500604214.1500644866.50; __utmb=94650624.28.10.1500644866; __utmc=94650624; __utmz=94650624.1499960824.48.42.utmcsr=yukunweb.com|utmccn=(referral)|utmcmd=referral|utmcct=/412.html', 'DNT': '1', 'Host': 'music.163.com', 'Pragma': 'no-cache', 'Referer': 'http://music.163.com/', 'Upgrade-Insecure-Requests': '1', 'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'&#125; 注意，网易云音乐的歌曲提取是要我们登录才可以获得数据的，我们直接加上登录后的cookies就可以了。 现在运行爬虫，如果运行成功，那么大家应该就能看到打印出来的数据了。这说明我们的程序是正确的。 编写起始页解析函数这一步就要使用我们的选择器提取信息了，我们打开开发者工具，我们需要的就是歌手a标签中的href信息。对于还不会使用xpath和css选择器的可以使用Chrome开发者工具，右击该标签，如下图的操作： 直接上解析起始页代码： 123456# 获得所有歌手的urldef parse_index(self, response): artists = response.xpath('//*[@id="m-artist-box"]/li/div/a/@href').extract() for artist in artists: artist_url = self.base_url + '/artist' + '/album?' + artist[8:] yield Request(artist_url, callback=self.parse_artist) 歌手专辑详情页url实例如：https://music.163.com/#/artist/album?id=6452 我们解析得到href值后，在将他组合成完整的歌手专辑详情页url。然后回调给下一个解析函数。 提取所有专辑url这一步和上一步一样，由于这一步也没什么难点，不过多赘述。上代码： 123456# 获得所有歌手专辑的urldef parse_artist(self, response): albums = response.xpath('//*[@id="m-song-module"]/li/div/a[@class="msk"]/@href').extract() for album in albums: album_url = self.base_url + album yield Request(album_url, callback=self.parse_album) 提取所有歌曲这一步就有一点不同了，因为如果我们提取了音乐url，那么我们需要的音乐id就在url中。如果我们直接将URL回调给解析音乐页的函数后，我们在后面是获取不到这个id的。大家可以自己观察页面，确定这一步。 所以我们不仅要把url回调给下一个解析函数，还要把音乐id传给下一个函数。那么大家应该会有疑问，为什么不把id直接保存到item呢。 这是因为我们需要的数据结构会是这样： {&#39;id&#39;:123456,&#39;music&#39;:&#39;晴天&#39;,&#39;artist&#39;:&#39;周杰伦&#39;,&#39;album&#39;:&#39;叶美惠&#39;,&#39;comments&#39;:[{&#39;comment_author&#39;:&#39;小明&#39;,&#39;comment_content&#39;:&#39;我爱你&#39;,&#39;comment_like&#39;:&#39;123456&#39;},{...},{}...]} 如果我们现在保存了音乐id，那么后面的信息能否对应我们也不确定。那么怎样才能将数据传给下一个函数呢？ scrapy给我提供了meta参数用来保存我们的数据传给函数，我们来看代码： 12345678# 获得所有专辑音乐的urldef parse_album(self, response): musics = response.xpath('//ul[@class="f-hide"]/li/a/@href').extract() for music in musics: music_id = music[9:] music_url = self.base_url + music yield Request(music_url, meta=&#123;'id': music_id&#125;, callback=self.parse_music) 像这样我们把组合的URL传给解析函数，也将音乐id传给下一个函数。 提取音乐信息，分析评论Ajax对于提取页面的音乐信息，使用选择器提取就可以了，难的是评论区并不在我们获取的源码中。如果大家有疑惑，可以打印一些音乐详情页的源码。那么评论的信息究竟在哪呢，这是相信大家心里都开始怀疑这是不是Ajax加载的呢。 为了验证这个疑惑，我们点击评论区的翻页，可以看到到了第二页浏览器的url并没有变化。这个时候基本上可以知道这是ajax加载的页面了。 我们之前有一篇说过Ajax请求的处理方法，我们这里不多余赘述。打开Chrome开发者工具，点开Network标签的XHR刷新页面，这时候会有几个请求出来。我们一个一个点开看他们的响应内容，发现R_SO_4_186016?csrf_token=请求中包含了评论的信息。数一下热评数在对比页面中的热评信息，完全一致。我们看下面的图片： 通过上面一张图片，红框里框出的Form Data数据，没错，这是一个Post请求信息。接着我们就要将他们构造成字典通过post请求。我们在看图中的referer的url，没错url后面的id就是歌曲的id。上一个函数我们将歌曲id也传过来是不是很方便这一步的处理呢。 我们需要在之前的请求头中加入每个音乐请求的referer参数。 1DEFAULT_REQUEST_HEADERS['Referer'] = self.base_url + '/playlist?id=' + str(music_id) 将Form Data构造字典大家都没问题，构造Ajax请求url就是R_SO_4_后面接上音乐的id。也没问题，那么在scrapy中怎么使用Post请求呢。 答案就是scrapy的FormRequest方法，我们需要导入他，然后用法和Request一样，我们还需要将这个函数提取的所有音乐信息传给下一个提取热评的函数，然后将所有数据一起传给item。 代码如下： 1234567891011121314151617# 获得音乐信息def parse_music(self, response): music_id = response.meta['id'] music = response.xpath('//div[@class="tit"]/em[@class="f-ff2"]/text()').extract_first() artist = response.xpath('//div[@class="cnt"]/p[1]/span/a/text()').extract_first() album = response.xpath('//div[@class="cnt"]/p[2]/a/text()').extract_first() data = &#123; 'csrf_token': '', 'params': 'Ak2s0LoP1GRJYqE3XxJUZVYK9uPEXSTttmAS+8uVLnYRoUt/Xgqdrt/13nr6OYhi75QSTlQ9FcZaWElIwE+oz9qXAu87t2DHj6Auu+2yBJDr+arG+irBbjIvKJGfjgBac+kSm2ePwf4rfuHSKVgQu1cYMdqFVnB+ojBsWopHcexbvLylDIMPulPljAWK6MR8', 'encSecKey': '8c85d1b6f53bfebaf5258d171f3526c06980cbcaf490d759eac82145ee27198297c152dd95e7ea0f08cfb7281588cdab305946e01b9d84f0b49700f9c2eb6eeced8624b16ce378bccd24341b1b5ad3d84ebd707dbbd18a4f01c2a007cd47de32f28ca395c9715afa134ed9ee321caa7f28ec82b94307d75144f6b5b134a9ce1a' &#125; DEFAULT_REQUEST_HEADERS['Referer'] = self.base_url + '/playlist?id=' + str(music_id) music_comment = 'http://music.163.com/weapi/v1/resource/comments/R_SO_4_' + str(music_id) yield FormRequest(music_comment, meta=&#123;'id':music_id,'music':music,'artist':artist,'album':album&#125;, \ callback=self.parse_comment, formdata=data) 提取热评信息传给item这是爬虫部分的最后一步了，这一步从Ajax请求的json数据了提取信息，相信大家都会，就不去多说。我们提取到所有的数据后，就是传给item了。 item的操作和字典是一样的，我们就像保存字典数据一样保存他们就可以了。但是那么多数据写字典那样一步一步的是不是很蠢呢。那有没有方便一点的方法了。这个时候内置的eval方法派上用场，这里不做方法的讲解，用起来很简单，他会动态的获取我们字典的每一个键，然后帮我们保存。我们看代码： 123456789101112131415161718192021222324252627282930313233# 获得所有音乐的热评数据import jsondef parse_comment(self, response): id = response.meta['id'] music = response.meta['music'] artist = response.meta['artist'] album = response.meta['album'] result = json.loads(response.text) comments = [] if 'hotComments' in result.keys(): for comment in result.get('hotComments'): hotcomment_author = comment['user']['nickname'] hotcomment = comment['content'] hotcomment_like = comment['likedCount'] # 这里我们将评论的作者头像也保存，如果大家喜欢这个项目，我后面可以做个web端的展现 hotcomment_avatar = comment['user']['avatarUrl'] data = &#123; 'nickname': hotcomment_author, 'content': hotcomment, 'likedcount': hotcomment_like, 'avatarurl': hotcomment_avatar &#125; comments.append(data) item = MusicItem() # 由于eval方法不稳定，具体的可以自己搜索，我们过滤一下错误 for field in item.fields: try: item[field] = eval(field) except: print('Field is not defined', field) yield item 最后我们将数据传给Item。 Pipeline中处理数据在Pipeline中处理数据，其实我们这里没什么好对数据做什么改动的，这里我们要对数据做数据库的保存。 我们需要创建一个mongodb类。然后在settings中将ITEM_PIPELINES的键改为我们创建的mongdb类，由于我们不需要对数据进行改动，所以直接覆盖就好了。为了方便管理和整体架构的清晰，我们也需要在settings中设置我们的数据库信息。具体代码如下： 123456ITEM_PIPELINES = &#123; 'music163.pipelines.MongoPipeline': 300,&#125;# 添加数据库信息MONGO_URI = 'localhost'MONGO_DB = 'music163' 接下来就是写我们的Mongodb类了。首先我们需要给这个类传入两个参数，也就是我们前面在settings文件定义的数据库uri和数据库名，我们对它们进行一个赋值： 12345678910111213141516171819202122232425class MongoPipeline(object): def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db&lt;/pre&gt;接下来我们定义一个from_crawler类方法，这个方法就相当于将这个类的两个参数通过crawler对象从settings中拿到这两个参数（数据库uri和名称）。&lt;pre class="lang:python decode:true "&gt;class MongoPipeline(object): def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DB') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() 这里的@classmethod装饰器是python中比较常用的一个方法，具体操作大家可以参阅相关资料了解他。 后面的open_spider()和close_spider()方法其实是重定义的一个类方法，意思就是我们在启动爬虫的开始，调用open_spider()方法，在关闭爬虫是调用close_spider()方法。我们给他们添加启动数据库和关闭数据库的操作。 后面是最重要的方法，process_item()方法就是用来对item进行操作的。我们这里主要就是对数据库进行一个插入操作。 首先我们需要在items.py文件中加入一个 table_name = &#39;music&#39;的属性，也就是相当于一个数据库表名。这样做方便我们将这个属性传到process_item()方法，我们需要调用数据库的update方法： 123def process_item(self, item, spider): self.db[item.table_name].update(&#123;'id': item.get('id')&#125;, &#123;'$set': dict(item)&#125;, True) return item 这个方法有三个参数，第一个参数传入数据库查询的字段，我们使用音乐的id来进行查询。 第二个参数就是我们的item数据，我们将他转化为字典形式。 第三个参数至关重要，我们传入True。意思是如果我们查询到相同的数据，我们就做更新操作，如果没有查询到相同的数据就做插入操作。这就相当于我们己做了插入数据库同时有做了去重的操作。 最后好了，这样我们的爬虫就完成了，整理完代码运行起来吧。 项目地址github 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(12):Scrapy组件的用法]]></title>
    <url>%2F2017%2F07%2F29%2FPython%E7%88%AC%E8%99%AB%E5%8D%81%E4%BA%8C%E4%B9%8BScrapy%E7%BB%84%E4%BB%B6%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[大家好，点开文章大家应该可以听到一曲大气恢弘的音乐，仿佛置身于江湖中，而自己是一名行侠仗义的侠客。见多了江湖的纷扰，你早已经累了，功名利禄对你来说不如一壶好酒。你骑马田间，而这时已是傍晚时分，起风了，你看这远方的夕阳，心生感慨。如果当初能够多一点勇气，那么这数十年来就不会是孤身一人了。你叹了口气，猛地抽了自己一巴掌，乱想什么呢，赶紧学习了。 通过上一篇文章，我相信大家对于Scrapy的使用也有了初步的了解。如果说要使用Scrapy写一些爬虫，也并非什么难事了。那么为了能写出更加复杂的爬虫，我们在本篇文章就来了解一下Scrapy的组件及他们的用法。 由于我在浏览Scrapy文档的时候，发现文档说的很是详细，如果我来写这篇文章的话，无疑是班门弄斧。 所以，本篇文章主要分享一下我在学习Scrapy中对于组件整体运作的理解。 理解Scrapy作为一个框架，整体的组件协作我们还是要做到流程清晰的。我们来看一下经典的Scrapy架构图： 我们上一篇文章写的Scrapy项目大致流程就是： 确定爬虫名称和要抓取的初始网站； 请求网站获得响应数据Response； 解析Response提取数据； 有需要跟进的url继续循环执行第二步； 将需要的数据通过管道Pipeline的处理； 处理完的数据传给Item保存。 图中的Scrapy Engine就是Scrapy的引擎，他处于中间位置，主要负责各个组件之间的通信，数据传递。 那上图的Spiders就是来完成爬虫的逻辑，还有网页数据的提取。他将要请求的url通过Scrapy Engine引擎，传给Scheduler（调度器）。 调度器接收了引擎发送来的Requests请求，按顺序排列整理。 由于所有的数据交流都是要通过引擎的，所以调度器是通过把处理好的Requests有发送给引擎，由引擎通过Downloader Middlewares（下载中间件）发送给Downloader（下载器）。 下载器通过下载中间件的设置来下载引擎传过来的Requests的url，并且将获取到的Response又发送给引擎（如果有下载失败的Requests，那么下载器会在后面接着下载）。 引擎收到Response后，再将它发送给Spiders，Spiders对Response解析提取出需要的数据（或者还有需要继续请求的url，会继续执行上面的循环）。 Spiders将这些数据发送给引擎，最后引擎将这些数据发送给Item Pipeline进行数据处理和存储。 那么这个流程就结束了。下面我给大家把相应组件还有Scrapy提供的命令行工具的文档地址整理给大家。 Spiders我们上篇文章有用过Spider，就是我们爬虫类的基类。Spider他主要就是来完成爬虫的逻辑，还有网页数据的提取。他将要请求的url通过Scrapy Engine引擎，传给Scheduler（调度器）， 对spider来说，爬取的循环类似下文: 以初始的URL初始化Request，并设置回调函数。 当该request下载完毕并返回时，将生成response，并作为参数传给该回调函数。 spider中初始的request是通过调用 start_requests() 来获取的。 start_requests()读取 start_urls 中的URL， 并以 parse为回调函数生成Request 。 在回调函数内分析返回的(网页)内容，返回 Item 对象或者 Request 或者一个包括二者的可迭代容器。 返回的Request对象之后会经过Scrapy处理，下载相应的内容，并调用设置的callback函数(函数可相同)。 在回调函数内，您可以使用 选择器(Selectors) (您也可以使用BeautifulSoup, lxml 或者您想用的任何解析器) 来分析网页内容，并根据分析的数据生成item。 最后，由spider返回的item将被存到数据库(由某些 Item Pipeline 处理)或使用 Feed exports 存入到文件中。 文档地址：here Item Pipeline文档地址：here Downloader Middleware文档地址：here Scrapy命令行工具文档地址：here 选择器文档地址：here Scrapy源码源码地址：here 最后学一个框架最好的方法就是看他的文档和阅读他的源码，如大家认真看完上面的文档，那么对于Scrapy使用基本上是了然于胸了。那么下面的文章我们就来使用Scrapy写一些爬虫，知行合一才是最好的学习方法。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(11):Scrapy框架的安装和基本使用]]></title>
    <url>%2F2017%2F07%2F27%2FPython%E7%88%AC%E8%99%AB%E5%8D%81%E4%B8%80%E4%B9%8BScrapy%E6%A1%86%E6%9E%B6%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[大家好，本篇文章我们来看一下强大的Python爬虫框架Scrapy。Scrapy是一个使用简单，功能强大的异步爬虫框架，我们先来看看他的安装。 Scrapy的安装Scrapy的安装是很麻烦的，对于一些想使用Scrapy的人来说，它的安装常常就让很多人死在半路。在此我将我的安装过程和网络上整理的安装方法，分享给大家，希望大家能够安装顺利。 Windows安装开始之前，我们要确定自己安装了Python，本篇文章我们以Python3.5为例。Scrapy有很多依赖的包，我们来一一安装。 首先，使用pip -v，查看pip是否安装正常，如果正常，那么我们进行下一步； pip install wheel这个包我们之前的文章介绍过，安装好他我们就可以安装一些wheel件； lxml安装，之前的文章说过他的安装，那么我们这里在重新整理一下。whl文件地址：here。找到自己对应版本的文件，下载好后，找到文件位置，右键点击文件属性，点击安全标签，复制他的所在路径。打开管理员工具(cmd)，pip install &lt;粘贴whl路径&gt;； PyOpenssl 的whl文件地址：here。点击下载，whl文件安装方式同上； Twisted框架这个框架是一个异步网络库，是Scrapy的核心。whl文件地址：here； Pywin32这是一个Pywin32兼容的库，下载地址：here，选好版本进行下载； 如果上面的库全都安装好了，那么我们就可以安装我们的Scrapy了，pip install scrapy 是不是很麻烦呢，如果大家不喜欢折腾，那么在Windows下也可以很方便的安装。那就要使用我们之前提到的Anaconda了。具体安装大家自己找找，或者在之前的文章中找。那么他的安装Scrapy只需要一行： 1conda install scrapy Linux安装Linux系统安装起来就要简单一点： 1sudo apt-get install build-essential python3-dev libssl-dev libffi-dev libxml2 libxml2-dev libxslt1-dev zlib1g-dev Mac OS安装我们需要先安装一些C++的依赖库，xcode-select --install 需要安装命令行开发工具，我们点击安装。安装完成，那么依赖库也就安装完成了。 然后我们直接使用pip安装pip install scrapy 以上，我们的Scrapy库的安装基本上就解决了。 Scrapy的基本使用Scrapy的中文文档地址：here Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。 他的基本项目流程为： 创建一个Scrapy项目 定义提取的Item 编写爬取网站的spider并提取Item 编写Item Pipeline来存储提取到的Item(即数据) 而一般我们的爬虫流程为： 抓取索引页：请求索引页的URL并得到源代码，进行下一步分析； 获取内容和下一页链接：分析源代码，提取索引页数据，并且获取下一页链接，进行下一步抓取； 翻页爬取：请求下一页信息，分析内容并请求在下一页链接； 保存爬取结果：将爬取结果保存为特定格式和文本，或者保存数据库。 我们一步一步来看看如何使用。 创建项目在开始爬取之前，您必须创建一个新的Scrapy项目。 进入您打算存储代码的目录中，运行下列命令（以知乎日报为例）: 1scrapy startproject zhihurb 该命令将会创建包含下列内容的 zhihu 目录: 12345678910zhihurb/ scrapy.cfg zhihurb/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py ... 这些文件分别是: scrapy.cfg: 项目的配置文件zhihurb/: 该项目的python模块。之后您将在此加入代码。zhihurb/items.py: 项目中的item文件.zhihurb/pipelines.py: 项目中的pipelines文件.zhihurb/settings.py: 项目的设置文件.zhihurb/spiders/: 放置spider代码的目录. 定义Item这一步是定义我们需要获取到的数据信息，比如我们需要获得网站里的一些url，网站文章的内容，文章的作者等。这一步定义的地方就在我们的items.py文件。 12345import scrapyclass ZhihuItem(scrapy.Item): name = scrapy.Field() article = scrapy.Field() 编写Spider这一步就是写我们最熟悉的爬虫了，而我们的Scrapy框架可以让我们不需要去考虑实现的方法，只需要写出爬取的逻辑就可以了。 首先我们需要在 spiders/ 文件夹下创建我们的爬虫文件，比如就叫spider.py。写爬虫前，我们需要先定义一些内容。我们以知乎日报为例：https://daily.zhihu.com/ 123456from scrapy import Spiderclass ZhihuSpider(Spider): name = "zhihu" allowed_domains = ["zhihu.com"] start_urls = ['https://daily.zhihu.com/'] 这里我们定义了什么呢?首先我们导入了Scrapy的Spider组件。然后创建一个爬虫类，在类里我们定义了我们的爬虫名称：zhihu（注意：爬虫名称独一无二的，是不可以和别的爬虫重复的）。还定义了一个网址范围，和一个起始 url 列表，说明起始 url 可以是多个。 然后我们定义一个解析函数： 12def parse(self, response): print(response.text) 我们直接打印一下，看看这个解析函数是什么。 运行爬虫1scrapy crawl zhihu 由于Scrapy是不支持在IDE中执行，所以我们必须在命令行里执行命令，我们要确定是不是cd到爬虫目录下。然后执行，这里的命令顾名思义，crawl是蜘蛛的意思，zhihu就是我们定义的爬虫名称了。 查看输出，我们先看到的是一些爬虫类的输出，可以看到输出的log中包含定义在 start_urls 的初始URL，并且与spider中是一一对应的。我们接着可以看到打印出了网页源代码。可是我们似乎并没有做什么，就得到了网页的源码，这是Scrapy比较方便的一点。 提取数据接着就可以使用解析工具解析源码，拿到数据了。 由于Scrapy内置了CSS和xpath选择器，而我们虽然可以使用Beautifulsoup，但是BeautifulSoup的缺点就是慢，这不符合我们Scrapy的风格，所有我还是建议大家使用CSS或者Xpath。 由于之前我并没有写过关于Xpath或者CSS选择器的用法，那么首先这个并不难，而且熟悉浏览器的用法，可以很简单的掌握他们。 我们以提取知乎日报里的文章url为例： 123456from scrapy import Requestdef parse(self, response): urls = response.xpath('//div[@class="box"]/a/@href').extract() for url in urls: yield Request(url, callback=self.parse_url) 这里我们使用xpath解析出所有的url（extract()是获得所有URL集合，extract_first()是获得第一个）。然后将url利用yield语法糖，回调函数给下一个解析url的函数。 使用item后面详细的组件使用留在下一章讲解，这里假如我们解析出了文章内容和标题，我们要将提取的数据保存到item容器。 Item对象相当于是自定义的python字典。 您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性)。 1234567891011# 假如我们下一个解析函数解析出了数据def parse_url(self, response): # name = xxxx # article = xxxx # 保存 item = DmozItem() item['name'] = name item['article'] = article # 返回item yield item 保存爬取到的数据这里我们需要在管道文件pipelines.py里去操作数据，比如我们要将这些数据的文章标题只保留 5 个字，然后保存在文本里。或者我们要将数据保存到数据库里，这些都是在管道文件里面操作。我们后面在详细讲解。 那么最简单的存储方法是使用命令行命令： scrapy crawl zhihu -o items.json 这条命令就会完成我们的数据保存在根目录的json文件里，我们还可以将他格式保存为msv,pickle等。改变命令后面的格式就可以了。 最后本篇教程仅介绍了Scrapy的基础，还有很多特性没有涉及到，那么我会在下一篇文章分享一下我对于Scrapy组件的学习理解。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(10):Selenium+PhantomJS基本操作]]></title>
    <url>%2F2017%2F07%2F26%2FPython%E7%88%AC%E8%99%AB%E5%8D%81%E4%B9%8BSelenium%2BPhantomJS%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[大家好，这篇文章我们来看一下Selenium库结合PhantomJs，Chrome等一些浏览器的操作。那么我们在之前的文章中，有提到过Selenium库和PhantomJ，说他们结合使用是万能的利器。那么，他们真的那么厉害吗，我们一起来看看Selenium库的用法吧。 什么是SeleniumSelenium是一个自动化测试工具，支持包括Chrome，Firefox，Safari，PhantomJs等一些浏览器。如果用于爬虫中，我们主要用来解决一些JavaScript渲染的问题。 我们在使用Requests库去请求一些网页的时候，比如 163music，我们获得的响应数据呢，并不全是我们在浏览器中看到的信息。他可能是通过js渲染出来的。那么，我们如果使用Selenium库，就不会再去关心如何去解决这种问题了。 因为我们的浏览器，比如PhantomJs，他就是一个无界面的浏览器，他用来渲染解析js，而Selenium库就负责给浏览器发送一些命令，模拟一些比如下拉，拖拽，翻页，输入表单等动作。这样他们两个结合，对于那些 JS 的渲染问题是不是完美解决了。 文档地址：here 注意虽然Selenium库加上PhantomJs很好用，但是他毕竟是驱动一个浏览器，然后获取数据。所以在我们使用中，会发现他并没有我们使用一些解析库速度快。这其实就是他的弊端，所以我还是建议大家，不到实在找不到解决办法的时候，不去使用他们。 安装准备pip直接安装Selenium库：pip install selenium 浏览器驱动的安装： Chrome浏览器驱动：地址 PhantomJs浏览器驱动：地址 我们需要把安装好的浏览器驱动配置到我们的环境变量。对于Windows用户，配置环境变量比较麻烦。我们需要找到下载好的驱动位置，然后复制他的文件位置，见他粘贴到环境变量即可。 使用样例1234567891011121314151617181920from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitbrowser = webdriver.Chrome()try: browser.get('http://www.yukunweb.com') input = browser.find_element_by_id('s') input.send_keys('Python') input.send_keys(Keys.ENTER) wait = WebDriverWait(browser, 10) wait.until(EC.presence_of_element_located((By.ID, 'main'))) print(browser.current_url) print(browser.page_source)finally: browser.close() 如果我们运行上面的代码，会看到本地打开了一个Chrome浏览器，然后在浏览器地址栏输入了我的博客网址，然后他会自动的在搜索栏输入‘Python’，并且点击了回车搜索。并且将结果页的url和源代码打印出来。 我们的例子都是使用Chrome浏览器来操作，因为PhantomJs是无界面的，不方便查看到效果。如果大家运行错误的话，一般情况是浏览器并没有打开，那么应该是大家没有安装好Chrome浏览器，或者没有将驱动配置环境变量。 那么这几行代码究竟是什么意思呢，我们究竟赋予了什么指令呢？ 声明浏览器对象123456789101112from selenium import webdriverbrowser = webdriver.Chrome()# 声明其他浏览器browser = webdriver.PhantomJs()browser = webdriver.Firefox()&lt;/pre&gt;这就相当于我们调用了Selenium库的webdriver方法，实例化一个Chrome浏览器给我们调用。&lt;h2&gt;访问页面&lt;/h2&gt;&lt;pre class="lang:python decode:true "&gt;from selenium import webdriver browser = webdriver.Chrome()browser.get('http://www.yukunweb.com') 我们将要访问的url传给get方法。调用浏览器访问url。 查找元素1input = browser.find_element_by_id('s') 这句代码调用find_element_by_id方法，顾名思义，就是查找id为‘s’的标签，那么如果是操作class为‘s’的话，就是find_element_by_class(&#39;s&#39;)。 当然，我们还可以使用 CSS选择器和xpath选择器查找元素： 1234input = browser.find_element_by_css_selector("#s")print(input)input = browser.find_element_by_xpath('//*[@id="s"]')print(input) 通过打印结果，可以看到不管使用什么选择器，查找结果都是一样的。下面是一些查找api： find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector 查找多个元素如果我们查找的元素是网页中的li标签，是很多的元素。那么我们的查找方式和单个元素是相同的，只是对于查找的api我们需要在element后面加个复数形式 s。即是： find_elements_by_name find_elements_by_xpath find_elements_by_link_text find_elements_by_partial_link_text find_elements_by_tag_name find_elements_by_class_name find_elements_by_css_selector 元素交互操作即是对于我们获取的元素下达指令，调用交互的方法。 1234browser.get('http://www.yukunweb.com')input = browser.find_element_by_id('s')input.send_keys('Python')input.send_keys(Keys.ENTER) 这段代码中，我们首先查找到了id为‘s’的元素，然后传给他‘Python’值，然后调用交互方法，敲了回车。 当然，在大多是情况下，我们不能直接使用敲击回车的方法，因为我们不确定是不是敲了回车，表单就提交了。我们需要使用查找器查找到提交按钮元素，然后模拟点击: 1234button = browser.find_element_by_class_name('xxxx')button.click()# 清除表单信息button.clear() 那么，我们可以看到在模拟登陆时候，直接让我们手动的输入账号，密码，如果有验证码的话直接给一个input方法，我们手动输入验证码传给表单，是不是很简单的就模拟登录了了。 交互动作元素交互动作与上面的操作是不同的。上面的操作需要获得一个特定的元素。然后对这个特定的元素调用一些指令，才可以完成交互。而这个交互是将这些动作附加到动作链中串行执行。 我们以拖拽元素为例(我们需要导入ACtionChains方法)： 12345678910rom selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()browser.get(url)source = browser.find_element_by_name("source")target = browser.find_element_by_name("target")actions = ActionChains(browser)actions.drag_and_drop(source, target).perform() 这里的sourcs是我们要拖拽的元素，我们使用查找器找到他，target就是我们要拖拽到的位置元素。然后调用ActionChains方法，实现拖拽操作。 更多的操作可以查看文档：here 执行JavaScript有些动作呢，Selenium库并没有为我们提供特定的api，比如说将浏览器进度条下拉，这个实现起来是很难的。那么我们就可以通过让Selenium执行JS来实现进度条的下拉，这个得需要一些js的知识，不过还是很简单的。 123456from selenium import webdriverbrowser = webdriver.Chrome()browser.get('http://www.yukunweb.com')browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')browser.execute_script('alert("到达底部")') 这就相当于我们将一些JS命令传给Selenium的execute_script这个api，我们运行就可以看到浏览器下拉到底部，然后弹出会话框。 获取元素文本值如果我们查找得到一个元素，我们要怎样获得元素的一些属性和文本信息呢？ 12345678from selenium import webdriverbrowser = webdriver.Chrome()browser.get('http://www.yukunweb.com')name = browser.find_element_by_css_selector('#kratos-logo &amp;gt; a')print(name.text)print(name.get_attribute('href')) 运行结果可以看到，他打印出了‘意外’和他的url。 Frame框架有些网页在我们直接使用Selenium驱动浏览器打印源码的时候，并没有如期获得想要的数据，那在我们查看网页源码的时候，可以看到网页的iframe标签包裹的一个一个的框架。那么这就需要我们请求对应框架，拿到源码了。 我们以网易云音乐的歌手栏为例。 123456from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://music.163.com/#/discover/artist/signed/')print(page_source) 可以查看结果，并没有我们想要的信息。 1234567from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://music.163.com/#/discover/artist/signed/')browser.switch_to.frame('contentFrame')print(page_source) 这次打印，我们就可以看到我们需要的信息了，是不是很简单。 显示等待在文章开始的时候，我们运行的那段代码中有一段代码是不是还没有说。那就是我们命令浏览器等待的操作。 等待有两种方式，一种是隐士等待，一种是显示等待。当使用了隐士等待执行时，如果浏览器没有找到指定元素，将继续等待，如果超出设定时间就会抛出找不到元素的异常。而大多数情况我们建议使用显示等待。 显示等待是你指定一个等待的条件，还指定一个最长等待时间。那么程序会在最长等待时间内，判断条件是否成立，如果成立，立即返回。如果不成立，他会一直等待，直到最长等待时间结束，如果条件仍然不满足，就返回异常。 12wait = WebDriverWait(browser, 10)wait.until(EC.presence_of_element_located((By.ID, 'main'))) 这里的By.ID方法实际上就是一个查找的万能方法，而我们直接查找或者使用CSS、xpath查找足够满足，我也不过多介绍，想要了解可以查看官方文档。 这里是知道查找到id为‘main’就返回。 显示等待的一些条件还有： title_is 标题是某内容 title_contains 标题包含某内容 presence_of_element_located 元素加载出，传入定位元组，如(By.ID, ‘p’) visibility_of_element_located 元素可见，传入定位元组 visibility_of_element_located 元素可见，传入定位元组 visibility_of_element_located 元素可见，传入定位元组 visibility_of 可见，传入元素对象 presence_of_all_elements_located 所有元素加载出 text_to_be_present_in_element 某个元素文本包含某文字 text_to_be_present_in_element_value 某个元素值包含某文字 frame_to_be_available_and_switch_to_it frame加载并切换 invisibility_of_element_located 元素不可见 element_to_be_clickable 元素可点击 staleness_of 判断一个元素是否仍在DOM，可判断页面是否已经刷新 element_to_be_selected 元素可选择，传元素对象 element_located_to_be_selected 元素可选择，传入定位元组 element_selection_state_to_be 传入元素对象以及状态，相等返回True，否则返回False element_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回False alert_is_present 是否出现Alert 窗口选择如果我们在表单输入关键词，提交表单后浏览器新打开了一个窗口，那么我们要怎么去操作新的窗口呢？索性Selenium为我们提供了对应的api. 1234567891011121314import timefrom selenium import webdriverfrom selenium.webdriver.common.keys import Keysbrowser = webdriver.Chrome()browser.get('http://www.23us.cc/')input = browser.find_element_by_id('bdcs-search-form-input')input.send_keys('斗破苍穹')input.send_keys(Keys.ENTER)browser.switch_to_window(browser.window_handles[1])print(browser.current_url)time.sleep(1)browser.switch_to_window(browser.window_handles[0])print(browser.current_url) 通过打印结果，不难看出先打印了搜索结果窗口url，然后打印了索引页url。要注意窗口的索引是从 0 开始的哦，这个大家都明白。 异常处理异常处理和普通的异常处理一样，没有什么要说的，大家自己查看官方异常 api.地址 最后好了，通过本篇文章希望大家可以基本上了解Selenium库结合浏览器驱动的一些使用方法。我们例子里使用的是Chrome，但是大家在实际的代码里最好是使用PhantomJs，因为他是无界面的，运行起来相对好一点。 文章开始说过一般情况下不建议大家使用Selenium，因为他很慢。但是即使是慢，也很爽啊，是不是。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(9):Cookie介绍和模拟登录]]></title>
    <url>%2F2017%2F07%2F21%2FPython%E7%88%AC%E8%99%AB%E4%B9%9D%E4%B9%8BCookie%E4%BB%8B%E7%BB%8D%E5%92%8C%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[大家好，这篇文章我们来看一下Cookie是什么，和他的一些用法。 什么是Cookie？ 在计算机术语中是指一种能够让网站服务器把少量数据储存到客户端的硬盘或内存，或是从客户端的硬盘读取数据的一种技术。 先看看Cookie长什么样子，我们以知乎为例。打开Chrome的开发者工具(F12)，点开Network，点击一个请求url，就可以看到请求头(Request Headers)里面的一个Cookie信息了。当然我们从上面的响应头中可以看到一个Set-Cookie的信息，这就是服务器向浏览器设置Cookie的一些信息，比如Cookie的作用域，时间等。 Cookie的用途 存储用户登录信息，判断用户是否登录； 保存用户浏览足迹； Cookie是服务器在客户端存储的信息。这些信息是有服务器生成服务器解释的。 请求时，客户端需要把未超时的Cookies发送给服务器。服务器需要解析Cookies判断用户信息。 我们的浏览器会在你每次浏览网页是自动存储cookie，比如你打开清除浏览器浏览记录时，就会有cookie信息。 cookie给我们带来了很多便利，他还可以记录我们的浏览足迹，停留页面时间。比如你狂淘宝的时候，淘宝的推荐你喜欢的宝贝，就是根据你的cookie，获取你浏览过哪些商品，而生成的。 模拟登录演示我们还是以知乎为例。https://www.zhihu.com/people/yu-kun-73/answers 这个URL是我的知乎信息页，当然大家可以使用自己的知乎页做例子。如果我们直接访问他，会看到上方的登录按钮，而是我已经登录了在访问呢。就不会显示登录按钮，而是个人编辑资料一些链接。 那么，如果我们使用爬虫去访问呢。 12345678import requestsheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'&#125;url = 'https://www.zhihu.com/people/yu-kun-73/answers'resp = requests.get(url, headers=headers).textprint(resp) 通过打印的结果，我们可以看到，html 代码里是有登录按钮的。这就证明我们并没有登录这个页面。那么，如果我们在爬取一些需要登录账号才可以爬取数据的网站时，我们必然是要解决登录问题的。那么如何实现登录呢。 答案就是我们的Cookie了。我们在之前的文章中说过，requests库在处理Cookie时，是非常简单的。我们只需要向添加‘User-Agent’一样添加请求cookies就可以了。 123456789import requestsheaders = &#123; 'Cookie': # 你的登录过后的浏览器cookies, 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'&#125;url = 'https://www.zhihu.com/people/yu-kun-73/answers'resp = requests.get(url, headers=headers).textprint(resp) 我们再次看打印信息，就可以看到有编辑个人资料的链接了。这就说明我们已经成功登录了。是不是很简单呢，别急，难的在后面呢。 最后那么，如果我们要登录一些复杂的网站，比如新浪微博这种必须要登录才能获取信息的网站，使用cookie登录是肯定行不通的。因为这些网站会经常更新一些网站的算法，我们的cookie就会在工作一会失效，那么这些网站我们就需要向登录页post我们的登录信息。 这些登录信息参数一般都会存放在登录页的请求头下面的Form Data里面，我们只要携带这些信息Post，就可以顺利登录。但是如果遇到验证码，那么就很麻烦了。验证码的问题一直是考验爬虫的一个门槛，这个大家自己研究。 但是我们有一个最后的大杀器，Selenium+PhantomJS。这是什么呢？我们后面单独介绍。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(8):分析Ajax请求爬取果壳网]]></title>
    <url>%2F2017%2F07%2F17%2FPython%E7%88%AC%E8%99%AB%E5%85%AB%E4%B9%8B%E5%88%86%E6%9E%90Ajax%E8%AF%B7%E6%B1%82%E7%88%AC%E5%8F%96%E6%9E%9C%E5%A3%B3%E7%BD%91%2F</url>
    <content type="text"><![CDATA[本篇文章我们来研究一下怎么分析网页的Ajax请求。 我们在平时爬取网页的时候，可能都遇到过有些网页直接请求得到的 HTML 代码里面，并没有我们需要的数据，也就是我们在浏览器中看到的内容。 这就是因为这些信息是通过Ajax加载的，并且通过js渲染生成的。这个时候我们就需要分析这个网页的请求了。 什么是Ajax AJAX即“Asynchronous Javascript And XML”（异步JavaScript和XML），是指一种创建交互式网页应用的网页开发技术。AJAX = 异步 JavaScript和XML（标准通用标记语言的子集）。AJAX 是一种用于创建快速动态网页的技术。AJAX 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。 简单的说就是网页加载，浏览器地址栏的网址并没有变，是javascript异步加载的网页，应该是ajax。AJAX一般是通过XMLHttpRequest 对象接口发送请求的，XMLHttpRequest 一般被缩写为 XHR。 分析果壳网站点我们目标网站就以果壳网来进行分析。地址 我们可以看到这个网页并没有翻页按钮，而当我们一直往下拉请求，网页会自动的给我们加载出更多内容。但是，当我们观察网页url时，发现它并没有随着网页的加载请求而变化。而当我们直接请求这个url时，显然我们只能获得到第一页的html内容。 那我们要怎么获得所有页的数据呢？我们在Chrome中打开开发者工具(F12)。我们点击Network，点击XHR标签。然后我们刷新网页，往下拉请求。这个时候我们就可以看到XHR标签，在网页每一次加载的时候就会跳出一个请求。 我们点击第一个请求，可以看到他的参数： retrieve_type:by_subjectlimit:20offset:18-:1500265766286 在点击第二个请求，参数如下： retrieve_type:by_subjectlimit:20offset:38-:1500265766287 limit参数是网页每一页限制加载的文章数，offset就是页数了。接着往下看，我们会发现每一个请求的offset参数都会加 20。 我们接着看每一个请求的响应内容，这是一个就是格式的数据。我们点开result键，可以看到一个 20 篇文章的数据信息。这样我们就成功找到我们需要的信息位置了，我们可以在请求头中看到存放json数据的url地址。http://www.guokr.com/apis/minisite/article.json?retrieve_type=by_subject&amp;amp;limit=20&amp;amp;offset=18 爬取流程 分析Ajax请求获得每一页的文章url信息； 解析每一篇文章，获得需要数据； 将获得的数据保存数据库； 开启多进程，大量抓取。 开始我们的工具仍然使用requests请求，BeautifulSoup解析。 首先我们要通过分析Ajax请求，获得所有页的信息，通过对上面对网页的分析，可以得到Ajax加载的json数据的URL地址为：http://www.guokr.com/apis/minisite/article.json?retrieve_type=by_subject&amp;amp;limit=20&amp;amp;offset=18 我们需要构造这个 URL。 123456789101112131415161718192021222324# 导入可能要用到的模块import requestsfrom urllib.parse import urlencodefrom requests.exceptions import ConnectionError# 获得索引页的信息def get_index(offset): base_url = 'http://www.guokr.com/apis/minisite/article.json?' data = &#123; 'retrieve_type': "by_subject", 'limit': "20", 'offset': offset &#125; params = urlencode(data) url = base_url + params try: resp = requests.get(url) if resp.status_code == 200: return resp.text return None except ConnectionError: print('Error.') return None 我们把上面分析页面得到的请求参数构造成一个字典data，然后我们可以手动的构造这个url，但是urllib库已经给我们提供了一个编码方法，我们直接使用，就可以构造出完整的url了。然后是标准的requests请求页面内容。 123456789101112import json# 解析json，获得文章urldef parse_json(text): try: result = json.loads(text) if result: for i in result.get('result'): # print(i.get('url')) yield i.get('url') except: pass 我们使用josn.loads方法解析json，将其转化成一个json对象。然后直接通过字典的操作，获得文章的url地址。这里使用yield，每次请求返回一个url，降低内存的消耗。由于我在后面抓取的时候出跳出一个json解析的错误，这里直接过滤就好。 这里我们可以试着打印看看，是不是成功运行。 既然获得了文章的url，那么对于获得文章的数据就显得很简单了。这里不在进行详细的叙述。我们的目标是获得文章的标题，作者和内容。由于有的文章里面包含一些图片，我们直接过滤掉文章内容里的图片就好了。 123456789101112131415161718192021from bs4 import BeautifulSoup# 解析文章页def parse_page(text): try: soup = BeautifulSoup(text, 'lxml') content = soup.find('div', class_="content") title = content.find('h1', id="articleTitle").get_text() author = content.find('div', class_="content-th-info").find('a').get_text() article_content = content.find('div', class_="document").find_all('p') all_p = [i.get_text() for i in article_content if not i.find('img') and not i.find('a')] article = '\n'.join(all_p) # print(title,'\n',author,'\n',article) data = &#123; 'title': title, 'author': author, 'article': article &#125; return data except: pass 这里在进行多进程抓取的时候，BeautifulSoup也会出现一个错误，依然直接过滤。我们把得到的数据保存为字典的形式，方便保存数据库。 接下来就是保存数据库的操作了，这里我们使用Mongodb进行数据的存储。具体的方法在上一篇文章里有说过。不在对他进行详细叙述。 1234567891011import pymongofrom config import *client = pymongo.MongoClient(MONGO_URL, 27017)db = client[MONGO_DB]def save_database(data): if db[MONGO_TABLE].insert(data): print('Save to Database successful', data) return True return False 我们把数据库的名字，和表名保存到config配置文件中，在把配置信息导入文件，这样会方便代码的管理。 最后呢，由于果壳网数据还是比较多的，如果想要大量的抓取，我们可以使用多进程。 123456789101112131415161718from multiprocessing import Pool# 定义一个主函数def main(offset): text = get_index(offset) all_url = parse_json(text) for url in all_url: resp = get_page(url) data = parse_page(resp) if data: save_database(data)if __name__ == '__main__': pool = Pool() offsets = ([0] + [i*20+18 for i in range(500)]) pool.map(main, offsets) pool.close() pool.join() 函数的参数offset就是页数了。经过我的观察，果壳网最后一页页码是 12758，有 637 页。这里我们就抓取 500 页。进程池的map方法和Python内置的map方法使用类似。 好了，对于一些使用Ajax加载的网页，我们就可以这么抓取了。 项目地址here 如果觉得有帮助，不妨star。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一切都会好的]]></title>
    <url>%2F2017%2F07%2F12%2F%E4%B8%80%E5%88%87%E9%83%BD%E4%BC%9A%E5%A5%BD%E7%9A%84%2F</url>
    <content type="text"><![CDATA[博客好久没更新了，但是最近真的有很多话想说，但是又不知如何表达才显得自然。虽然有时候会想反正也没有几个人看，但是话到键盘上，又不知该说什么。这种只可让人意会，而不可言传的感觉真是憋人。 我的大三在十天前结束，转眼就要大四。三年来一事无成，现在想想都觉得后背发凉。这个后背发凉是真实的感受，毕竟气温30多度，感觉到凉是我对自己迷雾般的未来的担忧。 今天早晨163muc给我推荐了一首歌，听着觉得音乐很是伤感，但是没人歌唱，想着是纯音乐就点进去看看什么名字。但是空空的歌词面板却有一句歌词。 一切都会好的，城南的花都开了。 当时窗外是烈日炎炎，远处那颗高高的树上知了无忧无虑的叫。不知道是被这个歌词触动了，还是被音乐的旋律触动了，我只感觉鼻子莫名的酸。饶有兴趣的点开评论区，才知道这音乐是作者写给一个骨癌晚期还有半年时间的听友的。城南花已开是他的网易云id。出于好奇，开始一条一条的翻看评论。作者3.26发布的音乐，城南花已开这个 id 从 3.26 开始一直更新他的动态，回复这评论区里的问候。4.10 日是我翻到的这个 id 最后一条评论，没有说身体状况，直说他很感谢他的父母。 现在7月份了，不知道他现在怎么样，评论去里每天都有人出来问君是否安好。伤感又充满希望的旋律，总会让人心生感伤，每个人都留着每个人的故事，但是再也没有他的回复。 音乐分享由于网易云版权问题，外链不支持。所以只能分享下他的链接。 城南花已开 Simpleblog最近，利用空余的时间写了个社交型网站。有兴趣的可以前往看看，由于没有着力去做移动端页面，所以建议不要用手机点开。 网站是模仿简书的风格，追求一种简单但是功能完备的优雅。适用于多人创作交流，界面清晰自然的社交型网站。 大致功能 管理员功能（包含管理文章，文章评论，添加网站公告等）； 用户设置个人头像、资料，相互关注； 发布文章（支持markdown），评论交流； 文章点赞； 评论，点赞，关注消息显示； 站内私信； … 项目地址here，喜欢不妨 star 谢谢阅读]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>音乐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[忙忙碌碌]]></title>
    <url>%2F2017%2F06%2F20%2F%E5%BF%99%E5%BF%99%E7%A2%8C%E7%A2%8C%2F</url>
    <content type="text"><![CDATA[临近学期末，各项考试也陆陆续续开始了。忙完这门课的作业，接着忙那门课的复习。可是忙不单行，科目二考试就定在6月19日，两边都很是考试，只忙一边，另一边肯定会生变故。如果两边一起忙，那是必然不可能的。 因为驾校离学校有40分钟的公交路程（最快最方便），如果骑ofo单车大约35分钟，如果步行的话，时间待实践。 18号去考场模拟考试，和室友坐地铁到考场附近，骑着单车，看着手机导航。可能天气太热，网络都很困乏。按着导航骑了半天，确认位置，发现我们早已偏离方向。室友悲愤不已，盯着手机屏幕，大有一种高举之，猛掷之的冲动。 索性我们发现的及时，找对方向骑了将近10分钟，又不知路在何方。室友见前方有加油站，便上前询问，师傅摇摇头，表示并不知道附近有什么车管所什么科目二考场。 由于时间紧迫，我们很是焦躁，加上天气炎热，路边卡车叫的响亮。一个室友暗叹一声掉头准备返回，没走多久便在一个路口喊我们过去。我们骑过去见他表情古怪，然后便看到路口一个大大的蓝色路牌，一个大大的箭头指向我们的目的地。 都说好事多磨，以为赶到考场能立即进去模拟，然后成绩不错，明天考试轻松通过。可现实总是很残酷，考场等待的人真是奇多。等到我们上车模拟时早已是傍晚时分，教练看了我一眼，问我吃饭了没，我说没有，他说那赶紧上车。我大为感动，以为遇上一个好教练，能给我一个好一点的模拟体验。 我上车，教练见我准备好后，匆匆扔掉手里没抽几口的烟，暗示我速度开始。我心领神会，开始考试项目。可能是不熟悉这辆车，也可能是考场外的大卡车喇叭声过于刺耳。刚准备我的表演，机器就报我左车轮压到黄线。由于是刚开始，犯错犯得太快，我很是惭愧望向教练，以为他会很愤怒，然后告诉我出错原因，教导我如何规避这样的错误。但我只看到他若有若无的笑，他似乎感觉到我在望他，脸色瞬间变得严肃，然后问我吃完饭没，催促我赶紧继续下一个项目。我甚是感动，这教练不仅不会骂我，还很关怀的一直问我吃没吃饭。 后面的模拟不出意外的犯了很多错，教练不出意外的多次提醒我吃晚饭。模拟结束，情况很是糟糕，感觉明天的考试败券在握。临走时教练坐在车内略带同情的对我说情况不怎么样建议我继续交钱模拟，顺带告诉我他去吃晚饭了。晚上难以入睡，如果考试不通过，那这几天的太阳就白晒了。一想到我这熊猫胳膊就很是伤感。和室友交流一会，心中暗示自己模拟不好只不过是当时心情糟糕，卡车的喇叭声太吵。明天只要按照平常的练习肯定能过。 19号正常考试，起床特意穿了平时练车穿的鞋子。可能是鞋子真的很舒服，也可能是早晨喝的脉动让我找回自己，考试异乎寻常的顺利。走出考场后长舒一口气，感觉很是轻松。觉得南京的天空异常的蓝，对着考生指指点点的考官特别的和蔼，一个个教练生气的对着自己学员说话，都在询问他们吃没吃午饭。 虽然科目二的考试通过，但是车技并不会有多厉害。不过有付出必然有回报，我现在的自行车技术倒是一流。也算是收获。 哈哈]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[返老还童]]></title>
    <url>%2F2017%2F06%2F15%2F%E8%BF%94%E8%80%81%E8%BF%98%E7%AB%A5%2F</url>
    <content type="text"><![CDATA[昨天晚上在知乎上看到 你听过最有力量的歌词有哪些？ 看了第一条回答，就很喜欢这句台词，于是找了这部电影，静静的看了一下午。影片名《本杰明.巴顿奇事》，又叫《返老还童》。 “一件事无论太晚或者对于我来说太早，都不会阻拦你成为你想成为的那个人，这个过程没有时间的期限，只要你想，随时都可以开始，要改变或者保留原状都无所谓，做事本不应该有所束缚，我们可以办好这件事却也可以把它搞砸，但我希望最终你能成为你想成为的人。我希望你有时能驻足于这个令你感到惊叹的世界，体会你从未有过的感觉。我希望你能见到其他与你观点不同的人们。我希望你能有一个值得自豪的人生。如果你想象的生活不一样，我希望你能有勇气重新启程。” 这是本杰明写给她女儿的话。这个时候他已经饱经人生变迁，可身型却已经是少年模样。他本可以在花甲之年和他的爱人和自己的女儿幸福生活，但是他明白女儿需要的是 一个父亲，而不是玩伴。此时的他外表是一个青年。站在窗外望着黛西和女儿，丝丝无奈，很是孤独。在生命的最后日子里，他已经老年痴呆，不记得任何人任何事。他不喜欢别人碰他，他不相信养老院的人，但对他的爱人黛西格外信任。他终于变成了婴儿，躺在黛西怀里安然睡去。 这本是两条不该相交的平行线，却在命运之神的手上，奇迹的相遇。孩童时期的本杰明，只能像养老院里普通的老人一样，坐在轮椅里，望着天空。享受一天一天的时光。十几岁的本杰明第一次遇到黛西，就在也忘不掉她蓝色的眼睛。本应该是青梅竹马的相遇，却让他们成了忘年之交。他们第一次在床下谈话，被戴西的奶奶拉走。本杰明第一次明白，有些人是不能够被人们理解的。 后来他遇到了改变它一生的迈克船长，他带他嫖妓，带他成为海军。他目睹了战争的残酷，1328条生命死去。迈克船长死去的时候看着自己被子弹打穿的身体，气愤的说敌人破坏了他的艺术纹身。他看着远远的大海，目光逐渐暗淡，说：“有时事与愿违，你大可以像疯狗一样咆哮，怨天尤人，诅咒命运，悔不当初……但走到最后一刻，还是不得不安静地放手归去…… ”。 而此时的黛西正值事业巅峰，没经历过挫折，挥霍青春。而中年的本杰明，从二战中回到家乡无法融入她青春的激流中，默默离去。“生命并不是如此复杂，要是你喜欢，大可以说你是在探索生命。” 而命运总是有很多意外，如果不是黛西朋友的鞋带没断，如果那辆货车早些开走，女士的包裹早早准备好，男孩和女孩没有分手，男人定了闹钟和平时一样起床，司机没有停车喝咖啡。。。黛西就不会出车祸，还在继续跳舞。康复后的黛西找到本杰明，过了幸福的生活。而女儿的出生打破了平静的生活。已是少年模样的本杰明选择离开。而之后的黛西也有了新的丈夫。 “时间是1985年4月4日 地点是新奥尔良.这是我最后的遗言，也是遗嘱。我没留下什么，没财产，没钱，真的。我孤独的来，孤独的去。生不带来，死不带去。我仅有的就是我的故事，我现在就写着，趁我还记得的时候。” 这是本杰明迟暮之年写下的日记开篇。 但是正如他自己说的一般。他的出生不寻常，在一战结束的那晚，他的母亲难产去世。他的父亲觉得他是怪物将他抛弃。好在被善良的黑人女子奎尼收养。刚出生的本杰明相当于80岁老人，身体蹉跎干枯，医生说他没有多久时日。 影片并不是本杰明返老还童的无聊历程，而是通过战争，亲情，爱情，这些普通的方面，以回忆的形式讲述出来。1918年火车站建成，请来了最好的钟表匠盖图先生。因为战争失去儿子的盖图先生造出了一个倒着走的大钟，他说他希望所有的不幸全都消散，战场上死了的儿子可以回到家乡耕种，安度余生。但是这并不可能实现，倒着走的大钟继续走着，而该老去的依旧老去，该死去的依旧要死去。 影片中道斯先生曾被雷电劈七次，他说：“当我瞎了一个眼睛，几乎听不见晨间喧嚣，动不动就抽搐，总是丢三落四，上帝却依然扔下闪电来提醒我，能够活着已经是桩幸运的事了”。 或许你在生活中不善交流与别人格格不入，但是我们在空余的时间里一个人读读书，听听音乐。可怕的不是孤独，而是惧怕孤独。其实孤独没什么不好，至少你会有大把时间去满足自己的好奇心，去丰富自己的生活。世间没有什么是永恒的，遇到在困难的事都是会过去的，我们要知道生活本就是奇迹，没人知道明天会发生什么。 打开门窗，外面一片阳光。 -Goodnight, Daisy.-Goodnight, Benjamin.]]></content>
      <categories>
        <category>电影</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>电影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(7):多进程抓取拉钩网十万数据]]></title>
    <url>%2F2017%2F06%2F12%2FPython%E7%88%AC%E8%99%AB%E4%B8%83%E4%B9%8B%E5%A4%9A%E8%BF%9B%E7%A8%8B%E6%8A%93%E5%8F%96%E6%8B%89%E9%92%A9%E7%BD%91%E5%8D%81%E4%B8%87%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[由于拉钩网页面做了一些更新，之前的程序无法正常工作，本篇文章做一次更新。只更新一些程序和一些程序的实现方法。由于没有仔细修改，可能前后语言不通顺，大家谅解。 大家好，几天没有更新了。相信大家经过前两篇的练手爬虫，大家已经知道如何抓取一个简单网站。 这篇文章我们来抓取 拉钩网 的招聘信息。全部抓取大概十几万条全国招聘信息，并且保存数据库。 准备安装Mongodb数据库其实不是一定要使用MongoDB，大家完全可以使用MySQL或者Redis，全看大家喜好。这篇文章我们的例子是Mongodb，所以大家需要 下载 它。 在Windows中。由于MongoDB默认的数据目录为C:\data\db，建议大家直接在安装的时候更改默认路径为C:\MongoDB. 然后创建如下目录文件： 12C:\data\log\mongod.log //用于存储数据库的日志C:\data\db //用于存储数据库数据 然后在C:\MongoDB文件夹下（安装 Mongodb 路径）创建配置文件mongod.cfg。并且在配置文件里写入以下配置： 12345systemLog: destination: file path: C:\data\log\mongod.logstorage: dbPath: C:\data\db 大家记住要打开文件后缀名，不然我们可能创建了一个mongod.cfg.txt文件。 最后我们需要打开管理员权限的 CMD 窗口，执行如下命令，安装数据库成服务： 1&quot;C:\mongodb\bin\mongod.exe&quot; --config &quot;C:\mongodb\mongod.cfg&quot; --install 设置为服务后，需要在管理员权限打开的windows cmd窗口用服务的方式启动或停止MongoDB。 12net start mongodb //启动mongodb服务net stop mongodb //关闭mongodb服务 好了，安装好Mongodb数据库后，我们需要安装PyMongo，它是MongoDB的Python接口开发包。 1pip install pymongo 开始准备完成后，我们就开始浏览拉勾网。我们可以发现拉勾网所有的招聘职位都在左侧分类里。如图： 我们先获取首页HTML文件: 1234567891011121314151617import requestsfrom bs4 import BeautifulSoupfrom requests.exceptions import RequestExceptionurl = 'https://www.lagou.com/'# 获取页面源码函数def get_page_resp(url): headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'&#125; try: resp = requests.get(url, headers=headers) if resp.status_code == 200: return resp.text return None except RequestException: return Nonesoup = BeautifulSoup(get_page_resp(url), 'lxml') 然后我们打开开发者工具，找到招聘职业的位置。 大家还记得BeautifulSoup的CSS选择器吧，我们直接使用.select()方法获取标签信息。 123all_positions = soup.select('div.menu_sub.dn &gt; dl &gt; dd &gt; a')print(all_positions)print(len(all_positions)) 输出结果： 12[&lt;a class="curr" href="https://www.lagou.com/zhaopin/Java/" data-lg-tj-cid="idnull" data-lg-tj-id="4O00" data-lg-tj-no="0101"&gt;Java&lt;/a&gt;, &lt;a class="curr" href="https://www.lagou.com/zhaopin/C%2B%2B/" data-lg-tj-cid="idnull" data-lg-tj-id="4O00" data-lg-tj-no="0102"&gt;C++&lt;/a&gt;, # ... 省略部分 &lt;a class="" href="https://www.lagou.com/zhaopin/fengxiankongzhizongjian/" data-lg-tj-cid="idnull" data-lg-tj-id="4U00" data-lg-tj-no="0404"&gt;风控总监&lt;/a&gt;, &lt;a class="" href="https://www.lagou.com/zhaopin/zongcaifuzongcai/" data-lg-tj-cid="idnull" data-lg-tj-id="4U00" data-lg-tj-no="0405"&gt;副总裁&lt;/a&gt;] 260 获取到所有职位标签的a标签后，我们只需要提取标签的href属性和标签内内容，就可以获得到职位的招聘链接和招聘职位的名称了。我们准备信息生成一个字典。方便我们后续程序的调用。 123456789101112131415# 解析首页获得所有职位信息的函数def parse_index(): url = 'https://www.lagou.com/' soup = BeautifulSoup(get_html(url), 'lxml') all_positions = soup.select('div.menu_sub.dn &gt; dl &gt; dd &gt; a') joburls = [i['href'] for i in all_positions] jobnames = [i.get_text() for i in all_positions] for joburl, jobname in zip(joburls, jobnames): data = &#123; 'url' : joburl, 'name' : jobname &#125; # 这里使用yield语法糖，不熟悉的同学自己查看资料哦 yield data 这里我们用zip函数，同时迭代两个list。生成一个键值对。 接下来我们可以随意点击一个职位分类，分析招聘页面的信息。 分页我们首先来分析下网站页数信息。经过我的观察，每个职位的招聘信息最多不超过 30 页。也就是说，我们只要从第 1 页循环到第 30 页，就可以得到所有招聘信息了。但是也可以看到有的职位招聘信息，页数并不到 30 页。以下图为例： 如果我们访问页面：https://www.lagou.com/zhaopin/Java/31/ 也就是第 31 页。我们会得到 404 页面。所以我们需要在访问到404页面时进行过滤。 12if resp.status_code == 404: pass 这样我们就可以放心的 30 页循环获得每一页招聘信息了。 我们的每一页url使用format拼接出来： 1link = '&#123;&#125;&#123;&#125;/'.format(url, str(pages)) 获取信息 我们可以看到上面划线的信息。这就是我们要抓取的信息了。 当然。抓取的方法千篇一律，我们可以使用find()或find_all() 选择器，当然也可以使用 CSS选择器。但是 CSS选择器相对于前两者代码量稍微少一些。这里大家自己动手抓取，我们直接上代码供大家借鉴。 12345678910111213141516171819# 职位信息 positions = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_top &gt; a &gt; h3') # 工作地址 adds = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_top &gt; a &gt; span &gt; em') # 发布时间 publishs = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_top &gt; span') # 薪资信息 moneys = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_bot &gt; div &gt; span') # 工作需求 needs = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_bot &gt; div') # 发布公司 companys = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.company &gt; div.company_name &gt; a') tags = [] # 由于我发现有的招聘信息没有标签信息，if判断防止没有标签报错 if soup.find('div', class_='li_b_l'): # 招聘信息标签 tags = soup.select('ul &gt; li &gt; div.list_item_bot &gt; div.li_b_l') # 公司福利 fulis = soup.select('ul &gt; li &gt; div.list_item_bot &gt; div.li_b_r') 获取到全部信息后，我们同样的把他们组成键值对字典。 123456789101112for position,add,publish,money,need,company,tag,fuli in \ zip(positions,adds,publishs,moneys,needs,companys,tags,fulis): data = &#123; 'position' : position.get_text(), 'add' : add.get_text(), 'publish' : publish.get_text(), 'money' : money.get_text(), 'need' : need.get_text().split('\n')[2], 'company' : company.get_text(), 'tag' : tag.get_text().replace('\n','-'), 'fuli' : fuli.get_text() &#125; 组成字典的目的是方便我们将全部信息保存到数据库。 保存数据库保存数据库前我们需要配置数据库信息： 12345import pymongoclient = pymongo.MongoClient('localhost', 27017)lagou = client['lagou']url_list = lagou['url_list'] 这里我们导入了pymongo库，并且与MongoDB建立连接，这里是默认连接本地的MongoDB数据。创建并选择一个数据库lagou，并在这个数据库中，创建一个table，即url_list。然后，我们进行数据的保存： 12if url_list.insert_one(data): print('保存数据库成功', data) 如果保存成功，打印出成功信息。 多进程抓取十万多条数据是不是抓取的有点慢，有办法，我们使用多进程同时抓取。由于Python的历史遗留问题，多线程在Python中始终是个美丽的梦。 12345678910111213from multiprocessing import Pooldef main(pages): datas = parse_index() for i in datas: url = i['url'] print(url) mongo_table = i['name'] # 因为有的职位是以'.'开头的，比如.Net，数据库表名不能以.开头 if mongo_table[0] == '.': mongo_table = mongo_table[1:] # 我们把之前抓取职位所有招聘信息的程序整理为parse_link()函数 # 这个函数接收职位url，页码，和数据库表名为参数 parse_link(url, pages, mongo_table) 我们把之前提取职位招聘信息的代码，写成一个函数，方便我们调用。这里的parse_link()就是这个函数，他就收职位的 url 和所有页数为参数。我们get_alllink_data()函数里面使用for循环 30 页的数据。然后这个作为主函数传给多进程内部调用。 123456if __name__ == '__main__': pool = Pool(processes=6) pages = ([p for p in range(1, 31)]) pool.map(main,pages) pool.close() pool.join() 这里是一个pool进程池，我们调用进程池的map方法. map(func, iterable[,chunksize=None]) 多进程Pool类中的map方法，与Python内置的map函数用法行为基本一致。它会使进程阻塞，直到返回结果。需要注意，虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。join() 方法等待子进程结束后再继续往下运行，通常用于进程间的同步. 针对反爬如果大家就这样整理完代码，直接就开始抓取的话。相信在抓取的不久后就会出现程序中止不走了。我刚刚第一次中止后，我以为是网站限制了我的 ip。于是我做了如下改动。 12345678910111213141516171819202122import randomimport timeuser_agent_list = [ 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36', ' Mozilla/5.0 (Windows; U; Windows NT 5.2) Gecko/2008070208 Firefox/3.0.1', 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.2)']user_agent = random.choice(user_agent_list)headers = &#123; 'User-Agent':user_agent, 'Connection':'keep-alive'&#125;proxy_list = [ 'http://140.224.76.21:808', 'http://60.178.14.90:8081', 'http://121.232.146.13:9000', ]proxy_ip = random.choice(proxy_list)proxies = &#123;'http': proxy_ip&#125;resp = requests.get(url, headers=headers, proxies=proxies)time.sleep(1) 这里我是直接在在网上找了一些免费的ip，还自己找了几个浏览器的user-agent。利用Python内置的random库，开始随机选择列表里的user-agent和ip。并且将参数传入requests内继续抓取。为了防止请求频率过快，我们设置每次请求结束停留一秒。然后我以为问题这么结束了。就开始继续抓取，但是在之前中断的位置突然有中断了。 于是，我在代码抓取信息位置添加了一句打印url的代码。我得到了中断爬取的url，然后我手动多点进去，发现了这个网页。当然出现这个情况并不是更换ip不可以，而是我们的ip太少了，一个ip可能仍然出现多次抓取的情况，后续我将会和大家一起写一个自己的ip代理池。 模拟登录没错这是一个登录界面，不知道是不是这里的浏览器头，或者ip请求过多。如果继续增多ip和user_agent就不会出现这种情况。 但是如何面对这种需要登录才能持续爬取的网站呢，很简单，我们只要打开浏览器的开发者工具。登录自己的账号，在Network标签找一个请求，查看你的请求头部信息，找到自己的cookies。这个cookies就是你的登录信息了，我们需要将他和你的user-agent一样，添加到请求头就可以了。如果大家不明白这个cookies是什么，没关系，后面会有专门的讲解。 1234headers = &#123; 'Cookie':'user_trace_token=20170603115043-d0c257a054ee44f99177a3540d44dda1; LGUID=20170603115044-d1e2b4d1-480f-11e7-96cf-525400f775ce; JSESSIONID=ABAAABAAAGHAABHAA8050BE2E1D33E6C2A80E370FE9167B; _gat=1; PRE_UTM=; PRE_HOST=; PRE_SITE=; PRE_LAND=https%3A%2F%2Fwww.lagou.com%2F; index_location_city=%E5%85%A8%E5%9B%BD; login=false; unick=""; _putrc=""; _ga=GA1.2.922290439.1496461627; X_HTTP_TOKEN=3876430f68ebc0ae0b8fac6c9f163d45; _ga=GA1.3.922290439.1496461627; LGSID=20170720174323-df1d6e50-6d2f-11e7-ac93-5254005c3644; LGRID=20170720174450-12fc5214-6d30-11e7-b32f-525400f775ce; Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1500541369; Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1500543655', 'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',&#125; 好了我们继续抓取，发现这次程序并没有中止。~但是由于博主网速过慢，抓取太慢了，还没有全部抓取下来，不知道后面会不会出现问题。~ 这次更新，我是完全抓取玩全站才过来的，没错我已经抓取了全部的信息。用时1412 .9008133411407 秒。 好了，说了这么多，基本也都说完了。 项目地址对整理代码有疑惑的伙伴可以点击 这里。 最后这是我已经抓取的一部分。这里如果大家希望看到Mongodb数据库里的保存内容。 我们需要安装一个Mongodb可视化应用 robomango。 安装没什么要说的，大家链接到我们的lagou数据库，就可以看到里面的数据了。 如果博主后面把全部数据抓下来后，可以和大家一起进行数据分析，分析Python招聘的一些信息啊什么的。大家加油。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(6):煎蛋网全站妹子图爬虫]]></title>
    <url>%2F2017%2F06%2F04%2FPython%E7%88%AC%E8%99%AB%E5%85%AD%E4%B9%8B%E7%85%8E%E8%9B%8B%E7%BD%91%E5%85%A8%E7%AB%99%E5%A6%B9%E5%AD%90%E5%9B%BE%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[上一篇文章中我们抓取了豆瓣图书的数据，如果大家运行成功，并且看到文件夹下的 txt 文件了。是不是有一种刚接触编程，第一次输出Hello world!时的欣喜。和上一篇实践不同，我们这一次来爬取 煎蛋网 全站妹子图，并且保存到指定文件夹下。 爬取流程 从煎蛋网妹子图第一页开始抓取； 爬取分页标签获得最后一页数字； 根据最后一页页数，获得所有页URL； 迭代所有页，对页面所有妹子图片url进行抓取；访问图片URL并且保存图片到文件夹。 开始通过上一篇文章的爬取过程，我们基本上理解了抓取一个网站的大致流程。因为一个网站虽然有很多页，但是大部分网站每一页的HTML标签内容都是相同的。我们只要获取到一页的内容，就可以获得所有页的内容了。那么开始之前，我们来分析一下煎蛋网妹子图页面的URL。 第一页的 url：http://jandan.net/ooxx/page-1 第二页：http://jandan.net/ooxx/page-2 最后一页：http://jandan.net/ooxx/page-93 不难发现，煎蛋网的 url 的规律是比较简单的，每一页后面 page 的数字就是几。那么我们可以通过一个循环就可以获得所有的页面 URL 了。但是大家应该想到，这个网站每天都会更新，今天是 93 页，明天就会增加到94页了。如果每一都要爬一次的话，那么每次都要改一下代码页数信息了。这样实现起来虽然可以，但是不免有些愚蠢。 所以我们需要通过页面的标签信息让程序自己获得页数，我们访问http://jandan.net/ooxx/这个页面时，就相当于我们直接访问了最后一页。大家可以自己试试看。 图中我们可以明确的看到最后一页的数字是94.只要通过这个页面的URL进行抓取就可以得到。我们先获得源码： 12345678import requestsfrom bs4 import BeautifulSoupurl = 'http://jandan.net/ooxx/'headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'&#125;resp = requests.get(url, headers=headers)soup = BeautifulSoup(resp.text, 'lxml') 我们按下f12，从页面源码中找到最后一页 94 所在的标签： 原来 94 就在这个span标签啊。接下来是不是很简单了： 12# 获得最高页码数allpage = soup.find('span', class_="current-comment-page").get_text()[1:-1] 由于标签内的 94 边包含一个[]，大家不要以为只是一个list，只要[0]就可以获得的。我们完全可以使用type(),看一下他的属性，就知道它是一个字符串，我们利用切片去掉第一个和最后一个字符，就得到了页数了。得到页数后，我们利用循环就可以得到所有页的url了： 12345urllist = []# for循环迭代出所有页面，得到urlfor page in range(1, int(allpage)+1): allurl = base_url + 'page-' + str(page) urllist.append(allurl) 我们把它保存到一个list中。那么现在，我们得到所有页面的 url，就可以来获取每一页的内容了。我们以最后一页为例来进行抓取。 我们仍然使用审查元素，找到图片 url 所在的标签。仍然是老方法，获取到页面所有包含图片的img标签： 12# css选择器allimgs = soup.select('div.text &amp;gt; p &amp;gt; img') 只要一行代码，我们就成功获得所有标签了。这里使用了 CSS选择器，大家是否还记得这个方法呢。可以查看之前的文章或者BeautifulSoup的官方文档了解哦。如果大家对 CSS 不是很熟悉，或者根本不知道。也没关系，反正find_all()和find()方法也是可以实现的。不过这里我教大家一个简单的CSS选择器方法。 我们只要按f12打开浏览器的开发者工具，找到标签的位置，右击标签。就可以看到这个情况： 没错，我们直接Copy selector的内容，粘贴出来就是这样的字符串：#comment-3468457 &gt; div &gt; div &gt; div.text &gt; p &gt; img 我们稍微去掉前面的一些标签，大多数情况下保留到父标签后面的内容就可以了。就是这样：div.text &gt; p &gt; img 我们放到代码中，运行一下就知道是不是成功了。 结果只一个列表： 1[&lt;img onload="add_img_loading_mask(this, load_sina_gif);" org_src="//wx4.sinaimg.cn/mw690/6adc108fly1fg90v6vzipg20ak05xqv5.gif" src="//wx4.sinaimg.cn/thumb180/6adc108fly1fg90v6vzipg20ak05xqv5.gif"/&gt;, &lt;img onload="add_img_loading_mask(this, load_sina_gif);" org_src="//wx4.sinaimg.cn/mw690/6adc108fly1fg90qymd8pg20dc0dcnph.gif" src="//wx4.sinaimg.cn/thumb180/6adc108fly1fg90qymd8pg20dc0dcnph.gif"/&gt;, &lt;img onload="add_img_loading_mask(this, load_sina_gif);" org_src="//wx4.sinaimg.cn/mw690/6adc108fly1fg90s98qsbg207e08mu10.gif" src="//wx4.sinaimg.cn/thumb180/6adc108fly1fg90s98qsbg207e08mu10.gif"/&gt;, &lt;img onload="add_img_loading_mask(this, load_sina_gif);" org_src="//wx3.sinaimg.cn/mw690/6adc108fly1fg90v51p5eg20a00dchdv.gif" src="//wx3.sinaimg.cn/thumb180/6adc108fly1fg90v51p5eg20a00dchdv.gif"/&gt;, &lt;img src="//wx4.sinaimg.cn/mw600/a1b56627gy1fdb7851js0j20ku4xcx6q.jpg"/&gt;, &lt;img src="//wx4.sinaimg.cn/mw600/a1b56627gy1fdb78fw14yj20dc46pb2a.jpg"/&gt;, &lt;img src="//wx3.sinaimg.cn/mw600/a1b56627gy1fdb78hqr1kj20dc1i0x02.jpg"/&gt;, &lt;img src="//wx4.sinaimg.cn/mw600/a1b56627gy1fdb78ldnwbj20ku3ei4qq.jpg"/&gt;, &lt;img src="//wx4.sinaimg.cn/mw600/a82b014bly1fg8gkj95toj21kw1kwh3s.jpg"/&gt;, &lt;img src="//wx4.sinaimg.cn/mw600/a82b014bly1fg8gkhmfraj21kw11xgth.jpg"/&gt;, &lt;img src="//wx2.sinaimg.cn/mw600/a82b014bly1fg8gklclivj21kw2dc1dg.jpg"/&gt;, &lt;img src="//wx2.sinaimg.cn/mw600/a82b014bly1fg8eohjekwj21421jkah5.jpg"/&gt;] 很显然，这一页的图片URL都在这里。接下来就是提取img标签的src属性了。1234567for img in list: urls = img['src'] # 判断url是否完整 if urls[0:5] == 'http:': img_url = urls else: img_url = 'http:' + urls 由于有的标签内url并不完整，所以这里我们进行一次判断。如果不完整就给他补全。 好了，图片的 url 都获得了，接下来就是保存图片了。大家还记得之前我们介绍Requests模块的时候，有过保存图片的演示。因为万维网中每个图片，每个视频都有唯一的 url 指向它们。所以我们只要访问这个 url，并且获得图片的二进制数据，保存到本地就可以了。 12345678910imgs = requests.get(img_url，headers=headers)filename = img_url.split('/')[-1]# 保存图片with open(filename, 'wb') as f: # 直接过滤掉保存失败的图片，不终止程序 try: f.write(imgs.content) print('Sucessful image:',filename) except: print('Failed:',filename) 大家注意，获取图片二进制数据是.content方法，而不是.text。这里我们有一个错误过滤，因为保存的过程中会有一些文件保存错误的情况，我们直接过滤掉，不终止程序的运行。 好了，爬虫程序到这里基本上已经全部实现了。但是我们如果把所有的图片存放在一个文件夹中，而且还是代码所在文件夹，不免有些难看。我们可以自己指定他们存放的位置。这里需要用的Python内置的os库了，不清楚的伙伴可以自己查看资料哈。 1234567891011121314151617# 创建文件夹的函数，保存到D盘def mkdir(path): # os.path.exists(name)判断是否存在路径 # os.path.join(path, name)连接目录与文件名 isExists = os.path.exists(os.path.join("D:\jiandan", path)) # 如果不存在 if not isExists: print('makedir', path) # 创建文件夹 os.makedirs(os.path.join("D:\jiandan", path)) # 切换到创建的文件夹 os.chdir(os.path.join("D:\jiandan", path)) return True # 如果存在了就返回False else: print(path, 'already exists') return False 我们只要传给这个函数一个路径参数就可以创建文件夹了。全部的功能都已经实现了，如果不出现以外的话，大家就可以在 d 盘看到这个文件夹了。 如果程序出现错误，可能是我们的程序访问过于频繁，网站封禁了我们的ip。这时，我们就要使用一个代理了。网上免费的代理很多，大家可以自己找，这里简单做一个使用代理的演示。由于是免费ip，不会存活太久就会不能使用，大家不要直接使用代码中的ip。后续可以带着大家一起代建一个自己的代理池。 1234567proxies = &#123;'http': '111.23.10.27:8080'&#125;try: # Requests库的get请求 resp = requests.get(url, headers=headers)except: # 如果请求被阻，就使用代理 resp = requests.get(url, headers=headers, proxies=proxies) 代码好了，最后上一下完整代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# -*- coding:utf-8 -*-# author: yukunimport requestsimport osimport timefrom bs4 import BeautifulSoup# 发出请求获得HTML源码def get_html(url): # 指定一个浏览器头 headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'&#125; # 代理，免费的代理只能维持一会可能就没用了，自行更换 proxies = &#123;'http': '111.23.10.27:8080'&#125; try: # Requests库的get请求 resp = requests.get(url, headers=headers) except: # 如果请求被阻，就使用代理 resp = requests.get(url, headers=headers, proxies=proxies) return resp# 创建文件夹的函数，保存到D盘def mkdir(path): # os.path.exists(name)判断是否存在路径 # os.path.join(path, name)连接目录与文件名 isExists = os.path.exists(os.path.join("D:\jiandan", path)) # 如果不存在 if not isExists: print('makedir', path) # 创建文件夹 os.makedirs(os.path.join("D:\jiandan", path)) # 切换到创建的文件夹 os.chdir(os.path.join("D:\jiandan", path)) return True # 如果存在了就返回False else: print(path, 'already exists') return False# 获得图片地址调用download函数进行下载def get_imgs(): # 调用函数获得所有页面 for url in all_page(): path = url.split('-')[-1] # 创建文件夹的函数 mkdir(path) # 调用请求函数获得HTML源码 html = get_html(url).text # 使用lxml解析器，也可以使用html.parser soup = BeautifulSoup(html, 'lxml') # css选择器 allimgs = soup.select('div.text &amp;gt; p &amp;gt; img') # 调用download函数下载保存 download(allimgs) # 执行完毕打出ok print('ok')# 获得所有页面def all_page(): base_url = 'http://jandan.net/ooxx/' # BeautifulSoup解析页面得到最高页码数 soup = BeautifulSoup(get_html(base_url).text, 'lxml') # 获得最高页码数 allpage = soup.find('span', class_="current-comment-page").get_text()[1:-1] urllist = [] # for循环迭代出所有页面，得到url for page in range(1, int(allpage)+1): allurl = base_url + 'page-' + str(page) urllist.append(allurl) return urllist# 保存图片函数，传入的参数是一页所有图片url集合def download(list): for img in list: urls = img['src'] # 判断url是否完整 if urls[0:5] == 'http:': img_url = urls else: img_url = 'http:' + urls filename = img_url.split('/')[-1] # 保存图片 with open(filename, 'wb') as f: # 直接过滤掉保存失败的图片，不终止程序 try: f.write(get_html(img_url).content) print('Sucessful image:',filename) except: print('Failed:',filename)if __name__ == '__main__': # 计时 t1 = time.time() # 调用函数 get_imgs() print(time.time() - t1) 经过计时，我们只需要146秒就爬取了全站的妹子图片了，大家注意身体。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(5):豆瓣读书练手爬虫]]></title>
    <url>%2F2017%2F06%2F03%2FPython%E7%88%AC%E8%99%AB%E4%BA%94%E4%B9%8B%E8%B1%86%E7%93%A3%E8%AF%BB%E4%B9%A6%E7%BB%83%E6%89%8B%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[我们在之前的文章中基本上掌握了Python爬虫的原理和方法，不知道大家有没有练习呢。今天我就来找一个简单的网页进行爬取，就当是给之前的兵书做一个实践。不然不就是纸上谈兵的赵括了吗。 好了，我们这次的目标是豆瓣图书Top250，地址是：https://book.douban.com/top250?start=0 准备爬一个网页我们至少要知道自己需要什么信息，大家看截图： 红色箭头标记的地方就是我们要获取的信息了，包括书的名字，作者和出版社信息，豆瓣评分和一句话简介。我们有了目标信息，就需要找到信息所在的页面源码，然后通过解析源码来获取到信息数据。那么，我们怎样获得页面 HTML 源代码呢？翻阅兵书，我们知道可以使用requests之计。代码实现如下： 1234import requestsresp = requests.get('https://book.douban.com/top250?start=0')print(resp.text) 运行程序，我们就轻松的获得了敌军的 HTML 信息了。但是问题又来了，我们得到 HTML 信息后，怎样得到我们的目标数据呢？ 深夜了，一轮弯月躲在云朵后面，窗外下着雨，我们坐在烛火前，翻阅兵书，顿时茅塞顿开，BeautifulSoup大法好。 我们打开浏览器，按f12到开发者工具，我们从网页源码里查找到数据位置，截图如下： 可以看到书名信息包含在class=&#39;pl2&#39; div里面的a标签内，是a标签的title属性。发现目标位置后，就简单多了。我们利用BeautifulSoup来获得一个对象，按找标准的缩进显示的html代码： 123from bs4 import BeautifulSoupsoup = BeautifulSoup(resp.text, 'lxml') 推荐大家使用lxml解析器，因为他快。如果安装lxml遇到问题的可以参考 上一篇文章 的方法。当然，如果大家怕麻烦，也完全可以使用Python的内置标准库html.parser.对我们获得结果并没有影响。 开始工作现在我们要用到BeautifulSoup的find_all()选择器，因为我们这一页有很多书，而每一本书的信息都包含在class=pl2的div标签内，我们使用find_all()就可以直接得到本页所有书的书名了。我们用find()方法和find_all()方法来做一个比较： 12345678910# find_all()方法，# 注意class是Python关键词，后面要加下划线_：alldiv = soup.find_all('div', class_='pl2')for a in alldiv: names = a.find('a')['title'] print('find_all():', names)# find()方法：alldiv2 = soup.find('div', class_='pl2')names2 = alldiv2.find('a')['title']print('find():', names2 ) 运行结果： 123456789find_all(): 追风筝的人find_all(): 小王子# ...# ...省略部分# ...find_all(): 三体Ⅲfind(): 追风筝的人Process finished with exit code 0 我们通过结果就可以看到两者之间的差距了，前者输出了一页的数据，而后者只输出了第一条数据。所以包括后面的信息，由于每一天数据所在标签是一样的，我们都是用find_all()方法。 上面的代码写的优雅点，就是这样实现,注意结果是一个 list： 1234# 书名, 注意是L小写，不是阿拉伯数字1alldiv = soup.find_all('div', class_='pl2')names = [a.find('a')['title'] for a in alldiv]print(names) 这样书名数据我们就得到了，接下来是作者信息。方法和获取书名方法一样： 1234# 作者,由于信息在一个p标签内部，# 我们获取到标签直接get_text()方法获得文本内容allp = soup.find_all('p', class_='pl')authors = [p.get_text() for p in allp] 运行结果： 123456789[&apos;[美] 卡勒德·胡赛尼 / 李继宏 / 上海人民出版社 / 2006-5 / 29.00元&apos;, &apos;[法] 圣埃克苏佩里 / 马振聘 / 人民文学出版社 / 2003-8 / 22.00元&apos;, &apos;钱锺书 / 人民文学出版社 / 1991-2 / 19.00&apos;, &apos;余华 / 南海出版公司 / 1998-5 / 12.00元&apos;, # ...# ...省略部分结果# ...&apos;高铭 / 武汉大学出版社 / 2010-2 / 29.80元&apos;, &apos;刘慈欣 / 重庆出版社 / 2010-11 / 38.00元&apos;] 后面的评分内容和简介内容也是一样获得，只是标签不同，但是方法一样，具体也不需要多余赘述。直接看实现代码： 123456# 评分starspan = soup.find_all('span', class_='rating_nums')scores = [s.get_text() for s in starspan]# 简介sumspan = soup.find_all('span', class_='inq')sums = [i.get_text() for i in sumspan] 程序运行成功，我们就获得了4个list，分别是书名，作者，评分和简介内容。我们要把他们放在一起，打印出来，就是一页的数据信息了。这里我们使用zip()函数，zip()函数在运算时，会以一个或多个序列做为参数，返回一个元组的列表。同时将这些序列中并排的元素配对。 123456for name, author, score, sum in zip(names, authors, scores, sums): name = '书名：' + str(name) + '\n' author = '作者：' + str(author) + '\n' score = '评分：' + str(score) + '\n' sum = '简介：' + str(sum) + '\n' data = name + author + score + sum 我们使用换行符’\n‘给数据信息一点整齐的样式。我们可以查看到打印的结果，并没有所有数据黏在一起，显得丑陋。获得信息后，就是保存数据了。保存数据也很简单，Python的文件读写操作就可以实现。代码如下： 1234567# 文件名filename = '豆瓣图书Top250.txt'# 保存文件操作with open(filename, 'w', encoding='utf-8') as f: # 保存数据 f.writelines(data + '=======================' + '\n')print('保存成功') 运行成功，我们就可以看到项目文件下面的 txt 文件了，上面保存了我们上面打印出来的内容。 123456789101112书名：追风筝的人作者：[美] 卡勒德·胡赛尼 / 李继宏 / 上海人民出版社 / 2006-5 / 29.00元评分：8.8简介：为你，千千万万遍==================# ...# ...书名：活着作者：余华 / 南海出版公司 / 1998-5 / 12.00元评分：9.1简介：活着本身就是人生最大的意义================== 但是，我们要的是 250 条数据，而不是一页的十几条数据，那么要怎么获得到所有的数据呢。我们可以检查页面的信息，可以看到页面一共 10 页，第一页的URL是https://book.douban.com/top250?start=0。而最后一页的 URL 是https://book.douban.com/top250?start=225我们接着多看几页，第二页是https://book.douban.com/top250?start=25，第三页是https://book.douban.com/top250?start=50。 规律已经很清晰了，我们的页面的页数信息是最后的start=后面的数字。而且数字从0开始到225，每一页数字加 25.这就很简单了，我们以https://book.douban.com/top250?start=为基层URL，每一页在后面加页面的页数数字。就可以得到所有的页面 url 了。再以for循环迭代每一个 url，使用上面获取数据的方法，获得所有的数据信息。 获取所有页面URL的代码如下： 123456base_url = 'https://book.douban.com/top250?start='urllist = []# 从0到225，间隔25的数组for page in range(0, 250, 25): allurl = base_url + str(page) urllist.append(allurl) 我们把他保存在 list 里面，好用循环迭代。 代码那么，所有的功能都实现了。现在，我们只要将所有的代码组合起来，就可以实现我们需要的所有功能了。上代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# -*- coding:utf-8 -*-# author: yukunimport requestsfrom bs4 import BeautifulSoup# 发出请求获得HTML源码的函数def get_html(url): # 伪装成浏览器访问 headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'&#125; resp = requests.get(url, headers=headers).text return resp# 解析页面，获得数据信息def html_parse(): # 调用函数，for循环迭代出所有页面 for url in all_page(): # BeautifulSoup的解析 soup = BeautifulSoup(get_html(url), 'lxml') # 书名 alldiv = soup.find_all('div', class_='pl2') names = [a.find('a')['title'] for a in alldiv] # 作者 allp = soup.find_all('p', class_='pl') authors = [p.get_text() for p in allp] # 评分 starspan = soup.find_all('span', class_='rating_nums') scores = [s.get_text() for s in starspan] # 简介 sumspan = soup.find_all('span', class_='inq') sums = [i.get_text() for i in sumspan] for name, author, score, sum in zip(names, authors, scores, sums): name = '书名：' + str(name) + '\n' author = '作者：' + str(author) + '\n' score = '评分：' + str(score) + '\n' sum = '简介：' + str(sum) + '\n' data = name + author + score + sum # 保存数据 f.writelines(data + '=======================' + '\n')# 获得所有页面的函数def all_page(): base_url = 'https://book.douban.com/top250?start=' urllist = [] # 从0到225，间隔25的数组 for page in range(0, 250, 25): allurl = base_url + str(page) urllist.append(allurl) return urllist# 文件名filename = '豆瓣图书Top250.txt'# 保存文件操作f = open(filename, 'w', encoding='utf-8')# 调用函数html_parse()f.close()print('保存成功。') 我们只用了36行的代码（去掉空行和注释）就实现了抓取豆瓣图书的数据了。大家是不是觉得很简单了，不要兴奋，这只是一个小白最基础的练手项目，大家快去找更有挑战性的项目实现吧。大家加油。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行走世间都是妖怪]]></title>
    <url>%2F2017%2F06%2F02%2F%E8%A1%8C%E8%B5%B0%E4%B8%96%E9%97%B4%E9%83%BD%E6%98%AF%E5%A6%96%E6%80%AA%2F</url>
    <content type="text"><![CDATA[热的时候喊热是无济于事的。顶着火红的太阳，看着姗姗来迟的 138 路公交，无奈的翻翻白眼。 南京的天气一如既往的热，由于科目二考试将至，连安静呆在宿舍的想法都无法满足。不过年轻人就是应该多出来走走，不然就无法看到外面美丽的姑娘们了。钱钟书说暴露的姑娘是局部的真理，我觉得这再合适不过了，真理总是赤裸的。正值躁动青春期的室友，掩饰不住自己对真理渴望的心，时不时露出莫名其妙的笑。 倒车，转弯，刹车，踩离合。时间就在这些机械的步骤中流过，真让人兴奋，终于可以回到凉爽的宿舍了。室友说他要在中途有个事，说不坐公交回去了。我说开什么玩笑，不坐公交怎么回去，难道请我打车。然后在我惊讶的目光下，他推来辆自行车。 其实我一直搞不清楚共享单车的盈利方式，扫个码就可以免费骑，而且还可以扫两辆。于是我们骑着自行车，行驶在烈日之下，行驶在公交车行驶30分钟的路程上，挥汗如雨。路边不断的有单身的或者不是单身的姑娘走过，为了自己的形象，即使在疲惫也要摆出轻松的姿势，面露沉稳的微笑，匆匆驶过，流下一阵燥热的风。 我今年二十多岁了，一直单身。我仍然不明白有些人究竟是有多么的空虚，多么的寂寞。分个手吵个架，遇到了不舒服的事就发个朋友圈，发个微博，感时运之不济，叹人性之不真。然而我在这条通往目的地的道路上，面对着周围美丽的真理，我竟没有一点想法。 我想那些天天矫情的分享人生感悟的人，并不是他们在人生的道路上遇到了磨难，而是他们都太闲了。走出房门，扫个码骑个自行车，行驶个40分钟，自然会明白还是屋里舒服。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(4):Beautiful Soup的常用方法]]></title>
    <url>%2F2017%2F06%2F01%2FPython%E7%88%AC%E8%99%AB%E5%9B%9B%E4%B9%8BBeautifulSoup%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Requests库的用法大家肯定已经熟练掌握了，但是当我们使用Requests获取到网页的 HTML 代码信息后，我们要怎样才能抓取到我们想要的信息呢？我相信大家肯定尝试过很多办法，比如字符串的 find 方法，还有高级点的正则表达式。虽然正则可以匹配到我们需要的信息，但是我相信大家在匹配某个字符串一次一次尝试着正则匹配的规则时，一定很郁闷。 那么，我们就会想有没有方便点的工具呢。答案是肯定的，我们还有一个强大的工具，叫BeautifulSoup。有了它我们可以很方便地提取出HTML或XML标签中的内容，这篇文章就让我们了解下BeautifulSoup的常用方法吧。 什么是BeautifulSoup？Python的网页解析可以用正则表达式去完成，那么我们在写的时候，要挨个的去把代码拿出来匹配，而且还要写匹配的规则，整体实现起来就很复杂。BeautifulSoup呢，它是一个方便的网页解析库，处理高效，支持多种解析器。大部分情况下，利用它我们不在需要编写正则表达式就可以方便的实现网页信息的提取。 官方文档 安装：$ pip install beautifulsoup4 BeautifulSoup是一个网页解析库，它支持很多解析器，不过最主流的有两个。一个是Python标准库，一个是lxml HTML 解析器。两者的使用方法相似： 1234567from bs4 import BeautifulSoup# Python的标准库BeautifulSoup(html, 'html.parser')# lxmlBeautifulSoup(html, 'lxml') Python内置标准库的执行速度一般，但是低版本的Python中，中文的容错能力比较差。lxmlHTML 解析器的执行速度快，但是需要安装 C语言的依赖库。 lxml的安装由于lxml安装需要依赖C语言库，所以当lxml在Windows上安装时，我们会发现各种奇怪的报错，当然脸好的使用pip install lxml 安装也是可以成功的。不过大部分人都是会倒在这里。 这里推荐大家使用lxml的.whl文件来安装。首先我们需要安装一下wheel库，有了这个库我们才可以正常安装.whl文件。pip install wheel 从官方网站下载与系统，Python版本匹配的lxml文件：地址。 另外，不知道自己系统和python版本信息的伙伴。需要进入系统管理员工具（CMD）或者python的 IDLE，输入以下代码： 123import pipprint(pip.pep425tags.get_supported()) 这时我们就可以看到打印出来的Python版本信息了。下载好lxml的文件后，我们需要找到文件的位置，然后进入管理员工具，使用pip安装：pip install whl文件的全名 安装完成后，可以进入Python，import一下，如果没有报错，那么恭喜你安装成功。如果有的伙伴觉得麻烦，那我推荐大家安装anaconda 下载地址（如果安装速度慢，可以找国内镜像），不知道是什么的小伙伴可以谷歌一下，有了他，那些在windows上pip安装出错的问题将不再存在。 BeautifulSoup的基本标签选择方法虽然Python内置的标准库解析器还不错，但是我还是推荐大家使用lxml，因为它够快。那么后面的代码我们都是用lxml解析器来进行演示。我们先导入官方文档的例子: 12345678910111213html_doc = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;""" HTML 代码,我们能够得到一个BeautifulSoup的对象,并能按照标准的缩进格式的结构输出: 12from bs4 import BeautifulSoupsoup = BeautifulSoup(html_doc, 'lxml') 我们可以看到上面的 HTML 代码并不完整，接下来我们使用prettify()方法来进行自动补全，注释部分就是运行的输出： 123456789101112131415161718192021222324252627282930313233print(soup.prettify())# &lt;html&gt;# &lt;head&gt;# &lt;title&gt;# The Dormouse's story# &lt;/title&gt;# &lt;/head&gt;# &lt;body&gt;# &lt;p class="title"&gt;# &lt;b&gt;# The Dormouse's story# &lt;/b&gt;# &lt;/p&gt;# &lt;p class="story"&gt;# Once upon a time there were three little sisters; and their names were# &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;# Elsie# &lt;/a&gt;# ,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;# Lacie# &lt;/a&gt;# and# &lt;a class="sister" href="http://example.com/tillie" id="link2"&gt;# Tillie# &lt;/a&gt;# ; and they lived at the bottom of a well.# &lt;/p&gt;# &lt;p class="story"&gt;# ...# &lt;/p&gt;# &lt;/body&gt;# &lt;/html&gt; 获取标签12print(soup.title)# &lt;title&gt;The Dormouse's story&lt;/title&gt; 通过输出结果，我们可以看到获取内容的属性，实际上就是 HTML 代码里的一个title标签。 获取名称12print(soup.title.name)# 'title' 实际上就是标签的名称。 获取属性12345print(soup.p.attrs['class'])# 'title'print(soup.p['class'])# 'title' 获取标签的属性我们可以使用attrs方法，传给他属性名，就可以得到标签的属性。通过结果我们可以看到，直接传给p标签属性名，一样可以获取到标签属性。 获取内容12print(soup.title.string)# 'The Dormouse's story' 我们还可以使用嵌套的选择，比如我们获得body标签里面p标签的内容： 12print(soup.body.p.string)# 'The Dormouse's story' 常见用法标准选择器虽然BeautifulSoup的基本用法，标签获取，内容获取，可以解析一些 html代码。但是在遇到很多复杂的页面时，上面的方法是完全不足的，或者是很繁琐的，因为有时候有的标签会有几个属性（class、id等）。 索性BeautifulSoup给我们提供了很方便的标准选择器，也就是 API 方法，这里着重介绍2个: find() 和 find_all() 。其它方法的参数和用法类似,大家举一反三吧。 find_all()find_all(name, attrs, recursive, text, **kwargs)可以根据标签，属性，内容查找文档。find_all()其实和正则表达式的原理很相似，他能找出所有能满足匹配模式的结果，在把结果以列表的形式返回。仍然是文档的例子： 12345678910111213141516html_doc = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html_doc, 'lxml') 过滤器文档参考介绍 find_all() 方法前,大家可以参考一下过滤器的类型。过滤器只能作为搜索文档的参数,或者说应该叫参数类型更为贴切。这些过滤器贯穿整个搜索的API。过滤器可以被用在 tag 的name中,节点的属性中,字符串中或他们的混合中。 find_all() 方法搜索当前 tag 的所有 tag 子节点,并判断是否符合过滤器的条件。这里有几个例子: 12345678910111213soup.find_all("title")# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]soup.find_all("p", "title")# [&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;]soup.find_all("a")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]soup.find_all(id="link2")# [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;] 有几个方法很相似,还有几个方法是新的,参数中的 string 和id是什么含义? 为什么 find_all(&quot;p&quot;, &quot;title&quot;) 返回的是CSS Class为”title”的标签? 我们来仔细看一下find_all()的参数: name参数name 参数可以查找所有名字为 name 的 tag,字符串对象会被自动忽略掉。 12soup.find_all("title")# [The Dormouse's story] 搜索 name 参数的值可以使任一类型的过滤器,字符窜,正则表达式,列表,方法或是True 。我们常用的 name 参数是搜索文档的标签名。 keyword参数如果我们的 HTML代码中有几个div标签，但是我们只想获取到class属性为top的div标签，我们怎么出来呢。 12soup.find_all('div', class_='top')# 这里注意下，class是Python的内部关键词，我们需要在css属性class后面加一个下划线'_'，不然会报错。 仍然以上面的代码实例： 12soup.find_all('a', id='link2')# [&lt;a id="link2" href="http://example.com/lacie"&gt;Lacie&lt;/a&gt;] 这样我们就只获取到id为link2的a标签。 limit参数find_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢。如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量。效果与 SQL 中的limit关键字类似,当搜索到的结果数量达到limit的限制时,就停止搜索返回结果。 比如我们要搜索出a标签，但是满足的有3个，我们只想要得到2个： 123soup.find_all("a", limit=2)# [&lt;a id="link1" class="sister" href="http://example.com/elsie"&gt;Elsie&lt;/a&gt;,# &lt;a id="link2" class="sister" href="http://example.com/lacie"&gt;Lacie&lt;/a&gt;] 其他的参数，不是经常用到，大家如需了解可以参考官方文档。 find()find_all()返回的是所有元素列表，find()返回单个元素。 find( name , attrs , recursive , string , **kwargs ) find_all()方法将返回文档中符合条件的所有 tag,尽管有时候我们只想得到一个结果。比如文档中只有一个标签,那么使用find_all() 方法来查找标签就不太合适, 使用find_all方法并设置limit=1参数不如直接使用find()方法。下面两行代码是等价的: 12345soup.find_all('title', limit=1)# [The Dormouse's story]soup.find('title')#The Dormouse's story 唯一的区别是find_all()方法的返回结果是值包含一个元素的列表,而find()方法直接返回结果。find_all()方法没有找到目标是返回空列表, find()方法找不到目标时,返回None。 CSS选择器Beautiful Soup支持大部分的 CSS选择器。在Tag或BeautifulSoup对象的.select()方法中传入字符串参数, 即可使用 CSS选择器的语法找到 tag。我们在写 css 时，标签 class类名加”.“，id属性加”#“。 12soup.select("title")# [The Dormouse's story] 通过 tag标签逐层查找: 1234567soup.select("body a")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]soup.select("html head title")# [&lt;title&gt;The Dormouse's story&lt;/title&gt;] 找到某个 tag标签下的直接子标签: 12345678910111213soup.select("head &gt; title")# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]soup.select("p &gt; a")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]soup.select("p &gt; #link1")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]soup.select("body &gt; a")# [] 通过 CSS 的 class类名查找: 1234soup.select(".sister")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;] 通过 tag 的 id 查找: 12345soup.select("#link1")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]soup.select("a#link2")# [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;] 同时用多种 CSS选择器查询元素，使用逗号隔开: 123soup.select("#link1,#link2")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;] 提取标签内容如果我们得到了几个标签： 12345list = [&lt;a href="http://www.baidu.com/"&gt;百度&lt;/a&gt;,&lt;a href="http://www.163.com/"&gt;网易&lt;/a&gt;,&lt;a href="http://www.sina.com/"新浪&lt;/a&gt;] 我们要怎样提取他里面的内容呢。我们开始的时候有提及。 1234for i in list: print(i.get_text()) # 我们使用get_text()方法获得标签内容 print(i.get['href'] # get['attrs']方法获得标签属性 print(i['href']) # 简写结果一样 结果： 123456789百度网易新浪http://www.baidu.com/http://www.163.com/http://www.sina.com/http://www.baidu.com/http://www.163.com/http://www.sina.com/ 总结 BeautifulSoup的解析库，推荐使用lxml，如果出现乱码的情况下，可以使用html.parser； BeautifulSoup的标签选择筛选方法，虽然弱但是速度快； 推荐使用find_all(),find()方法搜索标签，当然如果对css选择器熟悉，推荐使用.select()方法； get_text()方法获取标签文本内容，get[attrs]方法获取标签属性值。 本篇我们就基本上整理了BeautifulSoup的常用方法。如果大家希望了解更高级的用法，可以查看BeautifulSoup的官方文档。 最后，大家可以结合Requests库写出自己的爬虫吧。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(3):Requests的高级用法]]></title>
    <url>%2F2017%2F05%2F29%2FPython%E7%88%AC%E8%99%AB%E4%B8%89%E4%B9%8BRequests%E7%9A%84%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[上一篇文章我们整理了Requests库的基本用法，相信大家已经经过爬取一些简单网页的练习，已经很熟练了。 这一篇文章我们来 看一下Requests库的高级操作。 高级操作1.文件上传12345import requestsfiles = &#123;'file' : open('logo.gif','rb')&#125;resp = requests.post('http://httpbin.org/post', files=files)print(resp.text) 文件上传的操作只要我们从文件夹中把文件读取出来，并且赋值给 files 参数，就可以了，打印出源代码我们就可以看待上传文件的字节流了。 2.获取Cookie12345678&gt;&gt;&gt;import requests&gt;&gt;&gt;resp = requests.get('http://www.baidu.com')&gt;&gt;&gt;print(resp.cookies)&lt;RequestsCookieJar[]&gt;&gt;&gt;&gt;for key, value in resp.cookies.items():... print(key + '=' + value)BDORZ=27315 我们可以通过获取字典的键值对来查看cookie. 3.会话维持我们获得到了cookie就可以做一个会话维持，可以维持一个登录的状态，也就是做模拟登录。我们来看实现方式： 123456import requestss = requests.Session()s.get('http://httpbin.org/cookies/set/number/123456789') # 设置了一个cookieresp = s.get('http://httpbin.org/cookies')print(resp.text) 这就相当于模拟了一个会话，比如做登陆验证，可以用session，POST 一下，登陆一下，然后保持会话信息，在访问登录过页面的话，就可以正常获取登录后的页面了。如果你要模拟登录，可以通过申明Session对象，再用Session对象发起两次get请求，那么这两次请求相当于在一个浏览器里面，先访问set cookie页面，在访问get cookie页面。当然，cookie是自动处理的，不需要担心写一些处理cookies的方法。 建议模拟登录用requests的Session对象。 4.SSL证书验证Requests可以为 HTTPS 请求验证 SSL 证书，就像 web浏览器一样。要想检查某个主机的 SSL证书，你可以使用 verify参数: 1234&gt;&gt;&gt;import requests&gt;&gt;&gt;requests.get('https://kennethreitz.com', verify=True) # verify参数默认值为Truerequests.exceptions.SSLError: hostname 'kennethreitz.com' doesn't match either of '*.herokuapp.com', 'herokuapp.com' 如果不想他报这个错误，我们可以把参数verify的值设为False.运行后发现程序没有报错，但是会出现警告信息，警告我们要验证 SSL证书。如果要消除这个警告，我们需要调用原生包： 12345&gt;&gt;&gt;import requests&gt;&gt;&gt;from requests.packages import urllib3urllib3.disable_warnings()&gt;&gt;&gt;requests.get('https://kennethreitz.com', verify=False) 我们还可以自己指定一个证书： 12345&gt;&gt;&gt;import requests&gt;&gt;&gt;resp = requests.get('https://kennethreitz.com', cert=('/path/server.crt', '/path/key'))&gt;&gt;&gt;print(resp.status_code)200 5.代理设置有些网站会限制 IP 访问频率，超过频率就断开连接。这个时候我们就需要使用到代理，我们可以通过为任意请求方式提供proxies参数来配置单个请求。 12345678import requestsproxies = &#123; "http": "http://10.10.1.10:3128", "https": "http://10.10.1.10:1080",&#125;resp = requests.get('http://www.baidu.com', proxies=proxies)print(resp.status_code) 也可以通过环境变量 HTTP_PROXY 和 HTTPS_PROXY 来配置代理。有些代理需要加上用户名和密码的，代理可以使用http://user:password@host/语法，比如： 123proxies = &#123; "http": "http://user:pass@10.10.1.10:3128/",&#125; 除了基本的 HTTP代理，Requests还支持SOCKS协议的代理，如果需要用的，可以安装带三方库： 1$ pip install requests[socks] 安装好依赖以后，使用 SOCKS 代理和使用 HTTP 代理一样简单： 1234proxies = &#123; "http": 'socks5://user:pass@host:port', "https": 'socks5://user:pass@host:port'&#125; 6.超时设置超时设置就是设置请求的时间，如果在规定的时间内没有返回应答，就抛出异常. 1234import requestsresp = requests.get('http://www.baidu.com', timeout=0.5)print(resp.status_code) 如果在0.5秒内没有返回，就会报出ReadTimeout的异常。如果远端服务器很慢，你可以让Request永远等待，传入一个None作为timeout值，然后就冲咖啡去吧。 7.认证设置有一些网站在访问的时候需要我们输入用户名和密码，那么这种网站我们要怎样处理呢。 12345import requestsfrom requests.auth import HTTPBasicAuthresp = requests.get(url, auth=HTTPBasicAuth('username','password'))print(resp.status_code) 调用HTTPBasicAuth类，直接传入用户名和密码就可以了。 8.异常处理如果你遇到无法访问的网站，或者是你的网速不够快，你的访问超时，就会导致程序的中断。显然我们在实际的抓取中不愿意看到爬取到一半的程序突然中断的情况，那么我们能够避免这种程序中断的情况吗，答案是肯定的： 123456789101112import requestsfrom requests.exceptions import ReadTimeout, ConnectionError, RequestExceptiontry: resp = requests.get('http://httpbin.org/get', timeout=0.5) print(resp.status_code)except ReadTimeout： # 访问超时的错误 print('Timeout')except ConnectionError: # 网络中断连接错误 print('Connect error')except RequestException: # 父类错误 print('Error') 这样我们就可以把requests抓取过程中常见的异常都处理捕获了，捕获错误应该先捕获子类异常在捕获父类异常，这样做能够更加直观清楚的应对程序中出现的错误了。 如果我们能够自己捕获了这些异常，就可以保证我们的爬虫一直运行了。 好了，Requests的大部分用法已经全部说完了，大家是否已经学会了这门屠龙之术了呢。快找个网页练练手吧。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(2):Requests的基本用法]]></title>
    <url>%2F2017%2F05%2F29%2FPython%E7%88%AC%E8%99%AB%E4%BA%8C%E4%B9%8BRequests%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[虽然Python有内置的urllib库，可以实现网络的请求，但是我并不推荐。因为urllib在很多时候使用起来不方便，比如加一个代理，处理Cookie时API都很繁琐，再比如发送一个POST请求也很麻烦。 而Requests就相当于urllib的升级版本，简化了urllib的使用方法。有了Requests，我们可以用几句代码实现代理的设置，Cookie的设置，非常方便。下面我就给大家整理了Requests库的使用方法和细节。详细可以参考Requests官方文档。 什么是Requests？Requests是Python语言编写，基于urllib3，采用Apache2 Licensed开源协议的HTTP库。 它比urllib更加方便，可以节约我们大量的工作，完全满足HTTP测试需求。是Python实现的简单易用的HTTP库。 安装也很简单：pip install requests Requests的语法操作1.实例引入1234567import requestsresponse = requests.get('http://www.baidu.com/')print(response.status_code)print(type(response.text))print(response.text)print(response.cookies) 运行结果：12345200&lt;class &apos;str&apos;&gt; # ...HTML网页源码..&lt;RequestsCookieJar[]&gt; 可以看到，我们非常方便的就获取到了Cookies. 2.各种请求方式123456import requestsrequests.get('http://httpbin.org/get') # 发送get请求requests.post('http://httpbin.org/post') # 发送post请求，只要调用post方法，传入一个url参数requests.put('http://httpbin.org/put')requests.delete('http://httpbin.org/delete') 官方文档里提供的这个网址足够我们测试这些请求方式了。 请求1.基本GET请求1234import requestsresp = requests.get('http://httpbin.org/get')print(resp.text) 这个我们前面有使用过，也是最常用的方法。运行成功就可以看到网页的源码了。 2.带参数的GET请求12345678import requestsdata = &#123; 'name' : 'jack', 'age' : 20&#125;resp = requests.get('http://httpbin.org/get', params=data)print(resp.text) 传入参数只需要我们把数据生成一个字典，然后调用params参数，赋值给他就可以，是不是很方便。 3.解析json12345678import requestsimport jsonresp = requests.get('http://httpbin.org/get')print(resp.text)print(resp.json())print(json.loads(resp.text))print(type(resp.json())) 运行结果： 可以看出Requests的jaon解析和json的loads方法解析出来的结果是完全一样的。所以Requests可以很方便的解析json数据。 4.获取二进制数据12345import requestsresp = requests.get('http://www.baidu.com/img/baidu_jgylogo3.gif')print(resp.content)print(resp.text) 运行成功我们可以看到content方法获取的图片页面源码是二进制数据，而text获取的则是字符串代码。显然获取图片这种二进制数据需要使用content方法。 12with open('logo.gif','wb') as f: f.write(resp.content) 这样我们就保存了图片，我们可以在文件夹下看到这张图片。 5.添加headers12345import requestsheaders = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'&#125;resp = requests.get('http://www.baidu.com', headers=headers)print(resp.text) 有些网页如果我们直接去请求的话，他会查看请求的对象是不是浏览器，如果没有浏览器信息就会禁止我们爬虫的访问，这个时候我们就要给爬虫加一个headers，加一个浏览器的user-agent信息。这样我们就可以正常访问了。如果有的伙伴不知道怎么得到User-Agent，可以打开浏览器的审查元素，找到network，随便点击一个链接就可以看到User-Agent的信息了。 6.基本POST请求12345678import requestsdata = &#123; 'name' : 'jack', 'age' : 20&#125;resp = requests.post('http://httpbin.org/post', data=data)print(resp.text) 一个POST必然是要有一个Form Data的表单提交的，我们只要把信息传给data参数就可以了。一个POST请求只需要调用post方法，是不是特别方便呢。如果不觉得方便的话，可以去参考urllib的使用方法。 响应1.response属性123456789import requestsresponse = requests.get('http://www.baidu.com/')print(type(response.status_code)) # 状态码print(type(response.text)) # 网页源码print(type(response.headers)) # 头部信息print(type(response.cookies)) # Cookieprint(type(response.url)) # 请求的urlprint(type(response.history)) # 访问的历史记录 获取这些信息只需要简单的调用就可以实现了。 2.状态码判断12345&gt;&gt;&gt;import requests &gt;&gt;&gt;response = requests.get('http://www.baidu.com/')&gt;&gt;&gt;exit() if not resp.status_code == 200 else print('Sucessful')Sucessful 如果发送了一个错误请求(一个4XX客户端错误，或者5XX服务器错误响应)，我们可以通过 Response.raise_for_status() 来抛出异常： 12345678&gt;&gt;&gt;bad_r = requests.get('http://httpbin.org/status/404')&gt;&gt;&gt;bad_r.status_code404&gt;&gt;&gt;bad_r.raise_for_status()Traceback (most recent call last): File "requests/models.py", line 832, in raise_for_status raise http_errorrequests.exceptions.HTTPError: 404 Client Error 好了，这篇文章我们了解了Requests库的基本语法操作，相信大家对Requests库的请求和响应已经很清楚了，大家完全可以抓取一些网页了。 纸上得来终觉浅，绝知此事要躬行，大家加油！]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(1):基本原理]]></title>
    <url>%2F2017%2F05%2F28%2FPython%E7%88%AC%E8%99%AB%E4%B8%80%E4%B9%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[大家好，今天周末，希望大家周末愉快。 这篇文章我来梳理一下爬虫的基本原理。用过Python的伙伴都知道Python用来写爬虫是件很简单很爽的事情。但是有些伙伴不了解爬虫到底是什么，会纳闷为什么爬虫要设置什么请求头、请求体啊，到底要怎么样去发送这个请求啊。当然有的伙伴使用过很多爬虫的请求库，解析库，写过很多爬虫，但是可能他们的思路脉络也不是太清晰。那么，这篇文章就来帮大家理清爬虫原理的脉络。 什么是爬虫？ 爬虫即网络爬虫，英文是Web Spider。翻译过来就是网络上爬行的蜘蛛，如果把互联网看作一张大网，那么爬虫就是在大网上爬来爬去的蜘蛛，碰到想要的食物，就把他抓取出来。 我们在浏览器中输入一个网址，敲击回车，看到网站的页面信息。这就是浏览器请求了网站的服务器，获取到网络资源。那么，爬虫也相当于模拟浏览器发送请求，获得到HTML代码。HTML代码里通常包含了标签和文字信息，我们就从中提取到我们想要的信息。 通常爬虫是从某个网站的某个页面开始，爬取这个页面的内容，找到网页中的其他链接地址，然后从这个地址爬到下一个页面，这样一直不停的爬下去，进去批量的抓取信息。那么，我们可以看出网络爬虫就是一个不停爬取网页抓取信息的程序。 爬虫的基本流程1.发起请求：通过HTTP库向目标站点发起请求，即发送一个Request，请求可以包含额外的headers等信息，然后等待服务器响应。这个请求的过程就像我们打开浏览器，在浏览器地址栏输入网址：www.baidu.com，然后点击回车。这个过程其实就相当于浏览器作为一个浏览的客户端，向服务器端发送了 一次请求。 2.获取响应内容：如果服务器能正常响应，我们会得到一个Response，Response的内容便是所要获取的内容，类型可能有HTML、Json字符串，二进制数据(图片，视频等）等类型。这个过程就是服务器接收客户端的请求，进过解析发送给浏览器的网页HTML文件。 3.解析内容：得到的内容可能是HTML，可以使用正则表达式，网页解析库进行解析。也可能是Json，可以直接转为Json对象解析。可能是二进制数据，可以做保存或者进一步处理。这一步相当于浏览器把服务器端的文件获取到本地，再进行解释并且展现出来。 4.保存数据：保存的方式可以是把数据存为文本，也可以把数据保存到数据库，或者保存为特定的jpg，mp4 等格式的文件。这就相当于我们在浏览网页时，下载了网页上的图片或者视频。 Request1.什么是Request？浏览器发送信息给该网址所在的服务器，这个过程就叫做HTTP Request。 2.Request中包含什么？ 请求方式：请求方式的主要类型是GET，OST两种，另外还有HEAD、PUT、DELETE等。GET 请求的请求参数会显示在URL链接的后面，比如我们打开百度，搜索“图片”，我们会看到请求的URL链接为https://www.baidu.com/s?wd=图片。而 POST 请求的请求参数会存放在Request内，并不会出现在 URL 链接的后面，比如我们登录知乎，输入用户名和密码，我们会看到浏览器开发者工具的Network页，Request请求有Form Data的键值对信息，那里就存放了我们的登录信息，有利于保护我们的账户信息安全； 请求 URL：URL 全称是统一资源定位符，也就是我们说的网址。比如一张图片，一个音乐文件，一个网页文档等都可以用唯一URL来确定，它包含的信息指出文件的位置以及浏览器应该怎么去处理它； 请求头(Request Headers)：请求头包含请求时的头部信息，如User-Agent（指定浏览器的请求头），Host，Cookies等信息； 请求体：请求体是请求是额外携带的数据，比如登录表单提交的登录信息数据。 Response1.什么是Response？服务器收到浏览器发送的信息后，能够根据浏览器发送信息的内容，做出相应的处理，然后把消息回传给浏览器，这个过程就叫做HTTP Response。 2.Response中包含什么？ 响应状态：有多种响应状态，比如200代表成功，301 跳转页面，404 表示找不到页面，502 表示服务器错误； 响应头(Response Headers)：比如内容类型，内容长度，服务器信息，设置Cookie等； 响应体：响应体最主要的部分，包含了请求资源的内容，比如网页 HTML 代码，图片二进制数据等。 简单演示1234567import requests # 导入requests库，需要安装# 模拟成浏览器访问的头headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'&#125;resp = requests.get('https://www.baidu.com',headers=headers)print(resp.text) # 打印出网页源代码print(resp.status_code) # 打印出状态码 运行成功后可以看到打印出来的 html 源代码和 200 状态码了。这就基本上实现了爬虫的Request和Response的过程。 能抓到什么样的数据？ 网页文本：如 HTML 文档，Ajax加载的Json格式文本等； 图片，视频等：获取到的是二进制文件，保存为图片或视频格式； 其他只要能请求到的，都能获取。 123456789import requestsheaders = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'&#125;resp = requests.get('http://www.baidu.com/img/baidu_jgylogo3.gif',headers=headers)print(resp.content) # 二进制文件使用content# 保存图片with open('logo.gif','wb') as f: f.write(resp.content) print('Ok') 成功运行就可以看到打印出来的图片的二进制数据，可保存成功后打印的 OK，这个时候我们打开文件夹就可以看到下载下来的图片了。这几行代码就简单的演示了爬虫保存文件的过程。 解析方式有哪些？ 直接处理，比如简单的页面文档，只要去除一些空格的数据； Json解析，处理Ajax加载的页面； 正则表达式； BeautifulSoup库； PyQuery； XPath。 看到这里，大家是不是已经对爬虫的基本工作原理有了清晰的认识了呢。当然，罗马并不是一天建成的，只要积累了足够多的经验，大家肯定能成为爬虫大神的。 接下来，我会给大家继续分享一些我在学习爬虫时的经验总结。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五月的事]]></title>
    <url>%2F2017%2F05%2F25%2F%E4%BA%94%E6%9C%88%E7%9A%84%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[之前无聊的时候写过很多博客，但是那时候这个网站还没有上线，大多博客都写在知乎上。但是一些牢骚话自然不会发在知乎的，都是保存了草稿，然后自己删掉，发布的都是 Python 的分享。毕竟申请的是 Python 专栏，专栏地址。其实这种感觉其实特别难受，好比打了一半的嗝又被咽回去了。 转眼五月就要结束了，时间就像一匹欢快的小马，你永远抓不住他的尾巴。人家说天上一日，地上一年，大概就是说快乐地时光总是过得很快。上一次有时间的概念是网站备案的时候，阿里云说要等十几天才能备案成功，我才感觉时间漫长。一天一天过去，备案的十几天似乎还很遥远。可渐渐的忘记这件事的时候，时间的小马就匆匆跑过来告诉你，十几天又过去了，备案通过了，马上到六月了。 回想这个月，我也想不到自己到底干了些什么。处了这个网站成功上线，在知乎写了两篇博客，其他都没什么印象。每天重复的3点式生活占去了大部分记忆。不过也很好，毕竟这样也让我少去很多不必要的打扰。这个月窝在宿舍倒是把廖雪峰 JavaScript 的教程看了一遍，也算不枉这个月的时光。写到这我突然明白，如果一直这样与美丽的姑娘们毫无交集，可能就永远单身了=。=。 这个月开始天气开始热起来，越来越有夏天的感觉了。天气一热，心就开始浮躁，连看一部完整电影的耐心都没有。希望今后能静下心来，非宁静无以致远吗。毕竟专业课还有很多作业要补，下个月在不全部上交估计这门课要挂，欠下的总是要还的，这话说的一点没错。 今天是五月二十五日，在过五天就是端午节，虽然不会有多少人可以看到，我还是要提前衷心的祝大家端午快乐。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>

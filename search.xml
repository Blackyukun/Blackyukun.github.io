<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python爬虫(2):Requests的基本用法]]></title>
    <url>%2F2017%2F09%2F15%2FPython%E7%88%AC%E8%99%AB%E4%BA%8C%E4%B9%8BRequests%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言虽然Python有内置的urllib库，可以实现网络的请求，但是我并不推荐。因为urllib在很多时候使用起来不方便，比如加一个代理，处理Cookie时API都很繁琐，再比如发送一个POST请求也很麻烦。 而Requests就相当于urllib的升级版本，简化了urllib的使用方法。有了Requests，我们可以用几句代码实现代理的设置，Cookie的设置，非常方便。下面我就给大家整理了Requests库的使用方法和细节。详细可以参考Requests官方文档。 什么是Requests？Requests是Python语言编写，基于urllib3，采用Apache2 Licensed开源协议的HTTP库。 它比urllib更加方便，可以节约我们大量的工作，完全满足HTTP测试需求。是Python实现的简单易用的HTTP库。 安装也很简单：pip install requests Requests的语法操作1.实例引入1234567import requestsresponse = requests.get('http://www.baidu.com/')print(response.status_code)print(type(response.text))print(response.text)print(response.cookies) 运行结果：12345200&lt;class &apos;str&apos;&gt; # ...HTML网页源码..&lt;RequestsCookieJar[]&gt; 可以看到，我们非常方便的就获取到了Cookies. 2.各种请求方式123456import requestsrequests.get('http://httpbin.org/get') # 发送get请求requests.post('http://httpbin.org/post') # 发送post请求，只要调用post方法，传入一个url参数requests.put('http://httpbin.org/put')requests.delete('http://httpbin.org/delete') 官方文档里提供的这个网址足够我们测试这些请求方式了。 请求1.基本GET请求1234import requestsresp = requests.get('http://httpbin.org/get')print(resp.text) 这个我们前面有使用过，也是最常用的方法。运行成功就可以看到网页的源码了。 2.带参数的GET请求12345678import requestsdata = &#123; 'name' : 'jack', 'age' : 20&#125;resp = requests.get('http://httpbin.org/get', params=data)print(resp.text) 传入参数只需要我们把数据生成一个字典，然后调用params参数，赋值给他就可以，是不是很方便。 3.解析json12345678import requestsimport jsonresp = requests.get('http://httpbin.org/get')print(resp.text)print(resp.json())print(json.loads(resp.text))print(type(resp.json())) 运行结果： 可以看出Requests的jaon解析和json的loads方法解析出来的结果是完全一样的。所以Requests可以很方便的解析json数据。 4.获取二进制数据12345import requestsresp = requests.get('http://www.baidu.com/img/baidu_jgylogo3.gif')print(resp.content)print(resp.text) 运行成功我们可以看到content方法获取的图片页面源码是二进制数据，而text获取的则是字符串代码。显然获取图片这种二进制数据需要使用content方法。 12with open('logo.gif','wb') as f: f.write(resp.content) 这样我们就保存了图片，我们可以在文件夹下看到这张图片。 5.添加headers12345import requestsheaders = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'&#125;resp = requests.get('http://www.baidu.com', headers=headers)print(resp.text) 有些网页如果我们直接去请求的话，他会查看请求的对象是不是浏览器，如果没有浏览器信息就会禁止我们爬虫的访问，这个时候我们就要给爬虫加一个headers，加一个浏览器的user-agent信息。这样我们就可以正常访问了。如果有的伙伴不知道怎么得到User-Agent，可以打开浏览器的审查元素，找到network，随便点击一个链接就可以看到User-Agent的信息了。 6.基本POST请求12345678import requestsdata = &#123; 'name' : 'jack', 'age' : 20&#125;resp = requests.post('http://httpbin.org/post', data=data)print(resp.text) 一个POST必然是要有一个Form Data的表单提交的，我们只要把信息传给data参数就可以了。一个POST请求只需要调用post方法，是不是特别方便呢。如果不觉得方便的话，可以去参考urllib的使用方法。 响应1.response属性123456789import requestsresponse = requests.get('http://www.baidu.com/')print(type(response.status_code)) # 状态码print(type(response.text)) # 网页源码print(type(response.headers)) # 头部信息print(type(response.cookies)) # Cookieprint(type(response.url)) # 请求的urlprint(type(response.history)) # 访问的历史记录 获取这些信息只需要简单的调用就可以实现了。 2.状态码判断12345&gt;&gt;&gt;import requests &gt;&gt;&gt;response = requests.get('http://www.baidu.com/')&gt;&gt;&gt;exit() if not resp.status_code == 200 else print('Sucessful')Sucessful 如果发送了一个错误请求(一个4XX客户端错误，或者5XX服务器错误响应)，我们可以通过 Response.raise_for_status() 来抛出异常： 12345678&gt;&gt;&gt;bad_r = requests.get('http://httpbin.org/status/404')&gt;&gt;&gt;bad_r.status_code404&gt;&gt;&gt;bad_r.raise_for_status()Traceback (most recent call last): File "requests/models.py", line 832, in raise_for_status raise http_errorrequests.exceptions.HTTPError: 404 Client Error 好了，这篇文章我们了解了Requests库的基本语法操作，相信大家对Requests库的请求和响应已经很清楚了，大家完全可以抓取一些网页了。 纸上得来终觉浅，绝知此事要躬行，大家加油！]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(1):基本原理]]></title>
    <url>%2F2017%2F09%2F15%2FPython%E7%88%AC%E8%99%AB%E4%B8%80%E4%B9%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[前言大家好，今天周末，希望大家周末愉快。 这篇文章我来梳理一下爬虫的基本原理。用过Python的伙伴都知道Python用来写爬虫是件很简单很爽的事情。但是有些伙伴不了解爬虫到底是什么，会纳闷为什么爬虫要设置什么请求头、请求体啊，到底要怎么样去发送这个请求啊。当然有的伙伴使用过很多爬虫的请求库，解析库，写过很多爬虫，但是可能他们的思路脉络也不是太清晰。那么，这篇文章就来帮大家理清爬虫原理的脉络。 什么是爬虫？ 爬虫即网络爬虫，英文是Web Spider。翻译过来就是网络上爬行的蜘蛛，如果把互联网看作一张大网，那么爬虫就是在大网上爬来爬去的蜘蛛，碰到想要的食物，就把他抓取出来。 我们在浏览器中输入一个网址，敲击回车，看到网站的页面信息。这就是浏览器请求了网站的服务器，获取到网络资源。那么，爬虫也相当于模拟浏览器发送请求，获得到HTML代码。HTML代码里通常包含了标签和文字信息，我们就从中提取到我们想要的信息。 通常爬虫是从某个网站的某个页面开始，爬取这个页面的内容，找到网页中的其他链接地址，然后从这个地址爬到下一个页面，这样一直不停的爬下去，进去批量的抓取信息。那么，我们可以看出网络爬虫就是一个不停爬取网页抓取信息的程序。 爬虫的基本流程1.发起请求：通过HTTP库向目标站点发起请求，即发送一个Request，请求可以包含额外的headers等信息，然后等待服务器响应。这个请求的过程就像我们打开浏览器，在浏览器地址栏输入网址：www.baidu.com，然后点击回车。这个过程其实就相当于浏览器作为一个浏览的客户端，向服务器端发送了 一次请求。 2.获取响应内容：如果服务器能正常响应，我们会得到一个Response，Response的内容便是所要获取的内容，类型可能有HTML、Json字符串，二进制数据(图片，视频等）等类型。这个过程就是服务器接收客户端的请求，进过解析发送给浏览器的网页HTML文件。 3.解析内容：得到的内容可能是HTML，可以使用正则表达式，网页解析库进行解析。也可能是Json，可以直接转为Json对象解析。可能是二进制数据，可以做保存或者进一步处理。这一步相当于浏览器把服务器端的文件获取到本地，再进行解释并且展现出来。 4.保存数据：保存的方式可以是把数据存为文本，也可以把数据保存到数据库，或者保存为特定的jpg，mp4等格式的文件。这就相当于我们在浏览网页时，下载了网页上的图片或者视频。 Request1.什么是Request？浏览器发送信息给该网址所在的服务器，这个过程就叫做HTTP Request。 2.Request中包含什么？ 请求方式：请求方式的主要类型是GET，OST两种，另外还有HEAD、PUT、DELETE等。GET请求的请求参数会显示在URL链接的后面，比如我们打开百度，搜索“图片”，我们会看到请求的URL链接为https://www.baidu.com/s?wd=图片。而POST请求的请求参数会存放在Request内，并不会出现在URL链接的后面，比如我们登录知乎，输入用户名和密码，我们会看到浏览器开发者工具的Network页，Request请求有Form Data的键值对信息，那里就存放了我们的登录信息，有利于保护我们的账户信息安全； 请求URL：URL全称是统一资源定位符，也就是我们说的网址。比如一张图片，一个音乐文件，一个网页文档等都可以用唯一URL来确定，它包含的信息指出文件的位置以及浏览器应该怎么去处理它； 请求头(Request Headers)：请求头包含请求时的头部信息，如User-Agent（指定浏览器的请求头），Host，Cookies等信息； 请求体：请求体是请求是额外携带的数据，比如登录表单提交的登录信息数据。 Response1.什么是Response？服务器收到浏览器发送的信息后，能够根据浏览器发送信息的内容，做出相应的处理，然后把消息回传给浏览器，这个过程就叫做HTTP Response。 2.Response中包含什么？ 响应状态：有多种响应状态，比如200代表成功，301跳转页面，404表示找不到页面，502表示服务器错误； 响应头(Response Headers)：比如内容类型，内容长度，服务器信息，设置Cookie等； 响应体：响应体最主要的部分，包含了请求资源的内容，比如网页HTML代码，图片二进制数据等。 简单演示1234567import requests # 导入requests库，需要安装# 模拟成浏览器访问的头headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'&#125;resp = requests.get('https://www.baidu.com',headers=headers)print(resp.text) # 打印出网页源代码print(resp.status_code) # 打印出状态码 运行成功后可以看到打印出来的html源代码和200状态码了。这就基本上实现了爬虫的Request和Response的过程。 能抓到什么样的数据？ 网页文本：如HTML文档，Ajax加载的Json格式文本等； 图片，视频等：获取到的是二进制文件，保存为图片或视频格式； 其他只要能请求到的，都能获取。 123456789import requestsheaders = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'&#125;resp = requests.get('http://www.baidu.com/img/baidu_jgylogo3.gif',headers=headers)print(resp.content) # 二进制文件使用content# 保存图片with open('logo.gif','wb') as f: f.write(resp.content) print('Ok') 成功运行就可以看到打印出来的图片的二进制数据，可保存成功后打印的OK，这个时候我们打开文件夹就可以看到下载下来的图片了。这几行代码就简单的演示了爬虫保存文件的过程。 解析方式有哪些？ 直接处理，比如简单的页面文档，只要去除一些空格的数据； Json解析，处理Ajax加载的页面； 正则表达式； BeautifulSoup库； PyQuery； XPath。 看到这里，大家是不是已经对爬虫的基本工作原理有了清晰的认识了呢。当然，罗马并不是一天建成的，只要积累了足够多的经验，大家肯定能成为爬虫大神的。 接下来，我会给大家继续分享一些我在学习爬虫时的经验总结。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五月的事]]></title>
    <url>%2F2017%2F09%2F15%2F%E4%BA%94%E6%9C%88%E7%9A%84%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[之前无聊的时候写过很多博客，但是那时候这个网站还没有上线，大多博客都写在知乎上。但是一些牢骚话自然不会发在知乎的，都是保存了草稿，然后自己删掉，发布的都是Python的分享。毕竟申请的是Python专栏，专栏地址。其实这种感觉其实特别难受，好比打了一半的嗝又被咽回去了。 转眼五月就要结束了，时间就像一匹欢快的小马，你永远抓不住他的尾巴。人家说天上一日，地上一年，大概就是说快乐地时光总是过得很快。上一次有时间的概念是网站备案的时候，阿里云说要等十几天才能备案成功，我才感觉时间漫长。一天一天过去，备案的十几天似乎还很遥远。可渐渐的忘记这件事的时候，时间的小马就匆匆跑过来告诉你，十几天又过去了，备案通过了，马上到六月了。 回想这个月，我也想不到自己到底干了些什么。处了这个网站成功上线，在知乎写了两篇博客，其他都没什么印象。每天重复的3点式生活占去了大部分记忆。不过也很好，毕竟这样也让我少去很多不必要的打扰。这个月窝在宿舍倒是把廖雪峰JavaScript的教程看了一遍，也算不枉这个月的时光。写到这我突然明白，如果一直这样与美丽的姑娘们毫无交集，可能就永远单身了=。=。 这个月开始天气开始热起来，越来越有夏天的感觉了。天气一热，心就开始浮躁，连看一部完整电影的耐心都没有。希望今后能静下心来，非宁静无以致远吗。毕竟专业课还有很多作业要补，下个月在不全部上交估计这门课要挂，欠下的总是要还的，这话说的一点没错。 今天是五月二十五日，在过五天就是端午节，虽然不会有多少人可以看到，我还是要提前衷心的祝大家端午快乐。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>

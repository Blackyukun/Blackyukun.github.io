<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python爬虫(8):分析Ajax请求爬取果壳网]]></title>
    <url>%2F2017%2F07%2F17%2FPython%E7%88%AC%E8%99%AB%E5%85%AB%E4%B9%8B%E5%88%86%E6%9E%90Ajax%E8%AF%B7%E6%B1%82%E7%88%AC%E5%8F%96%E6%9E%9C%E5%A3%B3%E7%BD%91%2F</url>
    <content type="text"><![CDATA[本篇文章我们来研究一下怎么分析网页的Ajax请求。 我们在平时爬取网页的时候，可能都遇到过有些网页直接请求得到的 HTML 代码里面，并没有我们需要的数据，也就是我们在浏览器中看到的内容。 这就是因为这些信息是通过Ajax加载的，并且通过js渲染生成的。这个时候我们就需要分析这个网页的请求了。 什么是Ajax AJAX即“Asynchronous Javascript And XML”（异步JavaScript和XML），是指一种创建交互式网页应用的网页开发技术。AJAX = 异步 JavaScript和XML（标准通用标记语言的子集）。AJAX 是一种用于创建快速动态网页的技术。AJAX 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。 简单的说就是网页加载，浏览器地址栏的网址并没有变，是javascript异步加载的网页，应该是ajax。AJAX一般是通过XMLHttpRequest 对象接口发送请求的，XMLHttpRequest 一般被缩写为 XHR。 分析果壳网站点我们目标网站就以果壳网来进行分析。地址 我们可以看到这个网页并没有翻页按钮，而当我们一直往下拉请求，网页会自动的给我们加载出更多内容。但是，当我们观察网页url时，发现它并没有随着网页的加载请求而变化。而当我们直接请求这个url时，显然我们只能获得到第一页的html内容。 那我们要怎么获得所有页的数据呢？我们在Chrome中打开开发者工具(F12)。我们点击Network，点击XHR标签。然后我们刷新网页，往下拉请求。这个时候我们就可以看到XHR标签，在网页每一次加载的时候就会跳出一个请求。 我们点击第一个请求，可以看到他的参数： retrieve_type:bysubjectlimit:20offset:18 :1500265766286 在点击第二个请求，参数如下： retrieve_type:bysubjectlimit:20offset:38 :1500265766287 limit参数是网页每一页限制加载的文章数，offset就是页数了。接着往下看，我们会发现每一个请求的offset参数都会加 20。 我们接着看每一个请求的响应内容，这是一个就是格式的数据。我们点开result键，可以看到一个 20 篇文章的数据信息。这样我们就成功找到我们需要的信息位置了，我们可以在请求头中看到存放json数据的url地址。http://www.guokr.com/apis/minisite/article.json?retrieve_type=by_subject&amp;amp;limit=20&amp;amp;offset=18 爬取流程 分析Ajax请求获得每一页的文章url信息； 解析每一篇文章，获得需要数据； 将获得的数据保存数据库； 开启多进程，大量抓取。 开始我们的工具仍然使用requests请求，BeautifulSoup解析。 首先我们要通过分析Ajax请求，获得所有页的信息，通过对上面对网页的分析，可以得到Ajax加载的json数据的URL地址为：http://www.guokr.com/apis/minisite/article.json?retrieve_type=by_subject&amp;amp;limit=20&amp;amp;offset=18 我们需要构造这个 URL。 123456789101112131415161718192021222324# 导入可能要用到的模块import requestsfrom urllib.parse import urlencodefrom requests.exceptions import ConnectionError# 获得索引页的信息def get_index(offset): base_url = 'http://www.guokr.com/apis/minisite/article.json?' data = &#123; 'retrieve_type': "by_subject", 'limit': "20", 'offset': offset &#125; params = urlencode(data) url = base_url + params try: resp = requests.get(url) if resp.status_code == 200: return resp.text return None except ConnectionError: print('Error.') return None 我们把上面分析页面得到的请求参数构造成一个字典data，然后我们可以手动的构造这个url，但是urllib库已经给我们提供了一个编码方法，我们直接使用，就可以构造出完整的url了。然后是标准的requests请求页面内容。 123456789101112import json# 解析json，获得文章urldef parse_json(text): try: result = json.loads(text) if result: for i in result.get('result'): # print(i.get('url')) yield i.get('url') except: pass 我们使用josn.loads方法解析json，将其转化成一个json对象。然后直接通过字典的操作，获得文章的url地址。这里使用yield，每次请求返回一个url，降低内存的消耗。由于我在后面抓取的时候出跳出一个json解析的错误，这里直接过滤就好。 这里我们可以试着打印看看，是不是成功运行。 既然获得了文章的url，那么对于获得文章的数据就显得很简单了。这里不在进行详细的叙述。我们的目标是获得文章的标题，作者和内容。由于有的文章里面包含一些图片，我们直接过滤掉文章内容里的图片就好了。 123456789101112131415161718192021from bs4 import BeautifulSoup# 解析文章页def parse_page(text): try: soup = BeautifulSoup(text, 'lxml') content = soup.find('div', class_="content") title = content.find('h1', id="articleTitle").get_text() author = content.find('div', class_="content-th-info").find('a').get_text() article_content = content.find('div', class_="document").find_all('p') all_p = [i.get_text() for i in article_content if not i.find('img') and not i.find('a')] article = '\n'.join(all_p) # print(title,'\n',author,'\n',article) data = &#123; 'title': title, 'author': author, 'article': article &#125; return data except: pass 这里在进行多进程抓取的时候，BeautifulSoup也会出现一个错误，依然直接过滤。我们把得到的数据保存为字典的形式，方便保存数据库。 接下来就是保存数据库的操作了，这里我们使用Mongodb进行数据的存储。具体的方法在上一篇文章里有说过。不在对他进行详细叙述。 1234567891011import pymongofrom config import *client = pymongo.MongoClient(MONGO_URL, 27017)db = client[MONGO_DB]def save_database(data): if db[MONGO_TABLE].insert(data): print('Save to Database successful', data) return True return False 我们把数据库的名字，和表名保存到config配置文件中，在把配置信息导入文件，这样会方便代码的管理。 最后呢，由于果壳网数据还是比较多的，如果想要大量的抓取，我们可以使用多进程。 123456789101112131415161718from multiprocessing import Pool# 定义一个主函数def main(offset): text = get_index(offset) all_url = parse_json(text) for url in all_url: resp = get_page(url) data = parse_page(resp) if data: save_database(data)if __name__ == '__main__': pool = Pool() offsets = ([0] + [i*20+18 for i in range(500)]) pool.map(main, offsets) pool.close() pool.join() 函数的参数offset就是页数了。经过我的观察，果壳网最后一页页码是 12758，有 637 页。这里我们就抓取 500 页。进程池的map方法和Python内置的map方法使用类似。 好了，对于一些使用Ajax加载的网页，我们就可以这么抓取了。 项目地址here 如果觉得有帮助，不妨star。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一切都会好的]]></title>
    <url>%2F2017%2F07%2F12%2F%E4%B8%80%E5%88%87%E9%83%BD%E4%BC%9A%E5%A5%BD%E7%9A%84%2F</url>
    <content type="text"><![CDATA[博客好久没更新了，但是最近真的有很多话想说，但是又不知如何表达才显得自然。虽然有时候会想反正也没有几个人看，但是话到键盘上，又不知该说什么。这种只可让人意会，而不可言传的感觉真是憋人。 我的大三在十天前结束，转眼就要大四。三年来一事无成，现在想想都觉得后背发凉。这个后背发凉是真实的感受，毕竟气温30多度，感觉到凉是我对自己迷雾般的未来的担忧。 今天早晨163muc给我推荐了一首歌，听着觉得音乐很是伤感，但是没人歌唱，想着是纯音乐就点进去看看什么名字。但是空空的歌词面板却有一句歌词。 一切都会好的，城南的花都开了。 当时窗外是烈日炎炎，远处那颗高高的树上知了无忧无虑的叫。不知道是被这个歌词触动了，还是被音乐的旋律触动了，我只感觉鼻子莫名的酸。饶有兴趣的点开评论区，才知道这音乐是作者写给一个骨癌晚期还有半年时间的听友的。城南花已开是他的网易云id。出于好奇，开始一条一条的翻看评论。作者3.26发布的音乐，城南花已开这个 id 从 3.26 开始一直更新他的动态，回复这评论区里的问候。4.10 日是我翻到的这个 id 最后一条评论，没有说身体状况，直说他很感谢他的父母。 现在7月份了，不知道他现在怎么样，评论去里每天都有人出来问君是否安好。伤感又充满希望的旋律，总会让人心生感伤，每个人都留着每个人的故事，但是再也没有他的回复。 音乐分享由于网易云版权问题，外链不支持。所以只能分享下他的链接。 城南花已开 Simpleblog最近，利用空余的时间写了个社交型网站。有兴趣的可以前往看看，由于没有着力去做移动端页面，所以建议不要用手机点开。 网站是模仿简书的风格，追求一种简单但是功能完备的优雅。适用于多人创作交流，界面清晰自然的社交型网站。 大致功能 管理员功能（包含管理文章，文章评论，添加网站公告等）； 用户设置个人头像、资料，相互关注； 发布文章（支持markdown），评论交流； 文章点赞； 评论，点赞，关注消息显示； 站内私信； … 项目地址here，喜欢不妨 star 谢谢阅读]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>音乐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[忙忙碌碌]]></title>
    <url>%2F2017%2F06%2F20%2F%E5%BF%99%E5%BF%99%E7%A2%8C%E7%A2%8C%2F</url>
    <content type="text"><![CDATA[临近学期末，各项考试也陆陆续续开始了。忙完这门课的作业，接着忙那门课的复习。可是忙不单行，科目二考试就定在6月19日，两边都很是考试，只忙一边，另一边肯定会生变故。如果两边一起忙，那是必然不可能的。 因为驾校离学校有40分钟的公交路程（最快最方便），如果骑ofo单车大约35分钟，如果步行的话，时间待实践。 18号去考场模拟考试，和室友坐地铁到考场附近，骑着单车，看着手机导航。可能天气太热，网络都很困乏。按着导航骑了半天，确认位置，发现我们早已偏离方向。室友悲愤不已，盯着手机屏幕，大有一种高举之，猛掷之的冲动。 索性我们发现的及时，找对方向骑了将近10分钟，又不知路在何方。室友见前方有加油站，便上前询问，师傅摇摇头，表示并不知道附近有什么车管所什么科目二考场。 由于时间紧迫，我们很是焦躁，加上天气炎热，路边卡车叫的响亮。一个室友暗叹一声掉头准备返回，没走多久便在一个路口喊我们过去。我们骑过去见他表情古怪，然后便看到路口一个大大的蓝色路牌，一个大大的箭头指向我们的目的地。 都说好事多磨，以为赶到考场能立即进去模拟，然后成绩不错，明天考试轻松通过。可现实总是很残酷，考场等待的人真是奇多。等到我们上车模拟时早已是傍晚时分，教练看了我一眼，问我吃饭了没，我说没有，他说那赶紧上车。我大为感动，以为遇上一个好教练，能给我一个好一点的模拟体验。 我上车，教练见我准备好后，匆匆扔掉手里没抽几口的烟，暗示我速度开始。我心领神会，开始考试项目。可能是不熟悉这辆车，也可能是考场外的大卡车喇叭声过于刺耳。刚准备我的表演，机器就报我左车轮压到黄线。由于是刚开始，犯错犯得太快，我很是惭愧望向教练，以为他会很愤怒，然后告诉我出错原因，教导我如何规避这样的错误。但我只看到他若有若无的笑，他似乎感觉到我在望他，脸色瞬间变得严肃，然后问我吃完饭没，催促我赶紧继续下一个项目。我甚是感动，这教练不仅不会骂我，还很关怀的一直问我吃没吃饭。 后面的模拟不出意外的犯了很多错，教练不出意外的多次提醒我吃晚饭。模拟结束，情况很是糟糕，感觉明天的考试败券在握。临走时教练坐在车内略带同情的对我说情况不怎么样建议我继续交钱模拟，顺带告诉我他去吃晚饭了。晚上难以入睡，如果考试不通过，那这几天的太阳就白晒了。一想到我这熊猫胳膊就很是伤感。和室友交流一会，心中暗示自己模拟不好只不过是当时心情糟糕，卡车的喇叭声太吵。明天只要按照平常的练习肯定能过。 19号正常考试，起床特意穿了平时练车穿的鞋子。可能是鞋子真的很舒服，也可能是早晨喝的脉动让我找回自己，考试异乎寻常的顺利。走出考场后长舒一口气，感觉很是轻松。觉得南京的天空异常的蓝，对着考生指指点点的考官特别的和蔼，一个个教练生气的对着自己学员说话，都在询问他们吃没吃午饭。 虽然科目二的考试通过，但是车技并不会有多厉害。不过有付出必然有回报，我现在的自行车技术倒是一流。也算是收获。 哈哈]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[返老还童]]></title>
    <url>%2F2017%2F06%2F15%2F%E8%BF%94%E8%80%81%E8%BF%98%E7%AB%A5%2F</url>
    <content type="text"><![CDATA[昨天晚上在知乎上看到 你听过最有力量的歌词有哪些？ 看了第一条回答，就很喜欢这句台词，于是找了这部电影，静静的看了一下午。影片名《本杰明.巴顿奇事》，又叫《返老还童》。 “一件事无论太晚或者对于我来说太早，都不会阻拦你成为你想成为的那个人，这个过程没有时间的期限，只要你想，随时都可以开始，要改变或者保留原状都无所谓，做事本不应该有所束缚，我们可以办好这件事却也可以把它搞砸，但我希望最终你能成为你想成为的人。我希望你有时能驻足于这个令你感到惊叹的世界，体会你从未有过的感觉。我希望你能见到其他与你观点不同的人们。我希望你能有一个值得自豪的人生。如果你想象的生活不一样，我希望你能有勇气重新启程。” 这是本杰明写给她女儿的话。这个时候他已经饱经人生变迁，可身型却已经是少年模样。他本可以在花甲之年和他的爱人和自己的女儿幸福生活，但是他明白女儿需要的是 一个父亲，而不是玩伴。此时的他外表是一个青年。站在窗外望着黛西和女儿，丝丝无奈，很是孤独。在生命的最后日子里，他已经老年痴呆，不记得任何人任何事。他不喜欢别人碰他，他不相信养老院的人，但对他的爱人黛西格外信任。他终于变成了婴儿，躺在黛西怀里安然睡去。 这本是两条不该相交的平行线，却在命运之神的手上，奇迹的相遇。孩童时期的本杰明，只能像养老院里普通的老人一样，坐在轮椅里，望着天空。享受一天一天的时光。十几岁的本杰明第一次遇到黛西，就在也忘不掉她蓝色的眼睛。本应该是青梅竹马的相遇，却让他们成了忘年之交。他们第一次在床下谈话，被戴西的奶奶拉走。本杰明第一次明白，有些人是不能够被人们理解的。 后来他遇到了改变它一生的迈克船长，他带他嫖妓，带他成为海军。他目睹了战争的残酷，1328条生命死去。迈克船长死去的时候看着自己被子弹打穿的身体，气愤的说敌人破坏了他的艺术纹身。他看着远远的大海，目光逐渐暗淡，说：“有时事与愿违，你大可以像疯狗一样咆哮，怨天尤人，诅咒命运，悔不当初……但走到最后一刻，还是不得不安静地放手归去…… ”。 而此时的黛西正值事业巅峰，没经历过挫折，挥霍青春。而中年的本杰明，从二战中回到家乡无法融入她青春的激流中，默默离去。“生命并不是如此复杂，要是你喜欢，大可以说你是在探索生命。” 而命运总是有很多意外，如果不是黛西朋友的鞋带没断，如果那辆货车早些开走，女士的包裹早早准备好，男孩和女孩没有分手，男人定了闹钟和平时一样起床，司机没有停车喝咖啡。。。黛西就不会出车祸，还在继续跳舞。康复后的黛西找到本杰明，过了幸福的生活。而女儿的出生打破了平静的生活。已是少年模样的本杰明选择离开。而之后的黛西也有了新的丈夫。 “时间是1985年4月4日 地点是新奥尔良.这是我最后的遗言，也是遗嘱。我没留下什么，没财产，没钱，真的。我孤独的来，孤独的去。生不带来，死不带去。我仅有的就是我的故事，我现在就写着，趁我还记得的时候。” 这是本杰明迟暮之年写下的日记开篇。 但是正如他自己说的一般。他的出生不寻常，在一战结束的那晚，他的母亲难产去世。他的父亲觉得他是怪物将他抛弃。好在被善良的黑人女子奎尼收养。刚出生的本杰明相当于80岁老人，身体蹉跎干枯，医生说他没有多久时日。 影片并不是本杰明返老还童的无聊历程，而是通过战争，亲情，爱情，这些普通的方面，以回忆的形式讲述出来。1918年火车站建成，请来了最好的钟表匠盖图先生。因为战争失去儿子的盖图先生造出了一个倒着走的大钟，他说他希望所有的不幸全都消散，战场上死了的儿子可以回到家乡耕种，安度余生。但是这并不可能实现，倒着走的大钟继续走着，而该老去的依旧老去，该死去的依旧要死去。 影片中道斯先生曾被雷电劈七次，他说：“当我瞎了一个眼睛，几乎听不见晨间喧嚣，动不动就抽搐，总是丢三落四，上帝却依然扔下闪电来提醒我，能够活着已经是桩幸运的事了”。 或许你在生活中不善交流与别人格格不入，但是我们在空余的时间里一个人读读书，听听音乐。可怕的不是孤独，而是惧怕孤独。其实孤独没什么不好，至少你会有大把时间去满足自己的好奇心，去丰富自己的生活。世间没有什么是永恒的，遇到在困难的事都是会过去的，我们要知道生活本就是奇迹，没人知道明天会发生什么。 打开门窗，外面一片阳光。 -Goodnight, Daisy.-Goodnight, Benjamin.]]></content>
      <categories>
        <category>电影</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>电影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(7):多进程抓取拉钩网十万数据]]></title>
    <url>%2F2017%2F06%2F12%2FPython%E7%88%AC%E8%99%AB%E4%B8%83%E4%B9%8B%E5%A4%9A%E8%BF%9B%E7%A8%8B%E6%8A%93%E5%8F%96%E6%8B%89%E9%92%A9%E7%BD%91%E5%8D%81%E4%B8%87%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[由于拉钩网页面做了一些更新，之前的程序无法正常工作，本篇文章做一次更新。只更新一些程序和一些程序的实现方法。由于没有仔细修改，可能前后语言不通顺，大家谅解。 大家好，几天没有更新了。相信大家经过前两篇的练手爬虫，大家已经知道如何抓取一个简单网站。 这篇文章我们来抓取 拉钩网 的招聘信息。全部抓取大概十几万条全国招聘信息，并且保存数据库。 准备安装Mongodb数据库其实不是一定要使用MongoDB，大家完全可以使用MySQL或者Redis，全看大家喜好。这篇文章我们的例子是Mongodb，所以大家需要 下载 它。 在Windows中。由于MongoDB默认的数据目录为C:\data\db，建议大家直接在安装的时候更改默认路径为C:\MongoDB. 然后创建如下目录文件： 12C:\data\log\mongod.log //用于存储数据库的日志C:\data\db //用于存储数据库数据 然后在C:\MongoDB文件夹下（安装 Mongodb 路径）创建配置文件mongod.cfg。并且在配置文件里写入以下配置： 12345systemLog: destination: file path: C:\data\log\mongod.logstorage: dbPath: C:\data\db 大家记住要打开文件后缀名，不然我们可能创建了一个mongod.cfg.txt文件。 最后我们需要打开管理员权限的 CMD 窗口，执行如下命令，安装数据库成服务： 1&quot;C:\mongodb\bin\mongod.exe&quot; --config &quot;C:\mongodb\mongod.cfg&quot; --install 设置为服务后，需要在管理员权限打开的windows cmd窗口用服务的方式启动或停止MongoDB。 12net start mongodb //启动mongodb服务net stop mongodb //关闭mongodb服务 好了，安装好Mongodb数据库后，我们需要安装PyMongo，它是MongoDB的Python接口开发包。 1pip install pymongo 开始准备完成后，我们就开始浏览拉勾网。我们可以发现拉勾网所有的招聘职位都在左侧分类里。如图： 我们先获取首页HTML文件: 1234567891011121314151617import requestsfrom bs4 import BeautifulSoupfrom requests.exceptions import RequestExceptionurl = 'https://www.lagou.com/'# 获取页面源码函数def get_page_resp(url): headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'&#125; try: resp = requests.get(url, headers=headers) if resp.status_code == 200: return resp.text return None except RequestException: return Nonesoup = BeautifulSoup(get_page_resp(url), 'lxml') 然后我们打开开发者工具，找到招聘职业的位置。 大家还记得BeautifulSoup的CSS选择器吧，我们直接使用.select()方法获取标签信息。 123all_positions = soup.select('div.menu_sub.dn &gt; dl &gt; dd &gt; a')print(all_positions)print(len(all_positions)) 输出结果： 12[&lt;a class="curr" href="https://www.lagou.com/zhaopin/Java/" data-lg-tj-cid="idnull" data-lg-tj-id="4O00" data-lg-tj-no="0101"&gt;Java&lt;/a&gt;, &lt;a class="curr" href="https://www.lagou.com/zhaopin/C%2B%2B/" data-lg-tj-cid="idnull" data-lg-tj-id="4O00" data-lg-tj-no="0102"&gt;C++&lt;/a&gt;, # ... 省略部分 &lt;a class="" href="https://www.lagou.com/zhaopin/fengxiankongzhizongjian/" data-lg-tj-cid="idnull" data-lg-tj-id="4U00" data-lg-tj-no="0404"&gt;风控总监&lt;/a&gt;, &lt;a class="" href="https://www.lagou.com/zhaopin/zongcaifuzongcai/" data-lg-tj-cid="idnull" data-lg-tj-id="4U00" data-lg-tj-no="0405"&gt;副总裁&lt;/a&gt;] 260 获取到所有职位标签的a标签后，我们只需要提取标签的href属性和标签内内容，就可以获得到职位的招聘链接和招聘职位的名称了。我们准备信息生成一个字典。方便我们后续程序的调用。 123456789101112131415# 解析首页获得所有职位信息的函数def parse_index(): url = 'https://www.lagou.com/' soup = BeautifulSoup(get_html(url), 'lxml') all_positions = soup.select('div.menu_sub.dn &gt; dl &gt; dd &gt; a') joburls = [i['href'] for i in all_positions] jobnames = [i.get_text() for i in all_positions] for joburl, jobname in zip(joburls, jobnames): data = &#123; 'url' : joburl, 'name' : jobname &#125; # 这里使用yield语法糖，不熟悉的同学自己查看资料哦 yield data 这里我们用zip函数，同时迭代两个list。生成一个键值对。 接下来我们可以随意点击一个职位分类，分析招聘页面的信息。 分页我们首先来分析下网站页数信息。经过我的观察，每个职位的招聘信息最多不超过 30 页。也就是说，我们只要从第 1 页循环到第 30 页，就可以得到所有招聘信息了。但是也可以看到有的职位招聘信息，页数并不到 30 页。以下图为例： 如果我们访问页面：https://www.lagou.com/zhaopin/Java/31/ 也就是第 31 页。我们会得到 404 页面。所以我们需要在访问到404页面时进行过滤。 12if resp.status_code == 404: pass 这样我们就可以放心的 30 页循环获得每一页招聘信息了。 我们的每一页url使用format拼接出来： 1link = '&#123;&#125;&#123;&#125;/'.format(url, str(pages)) 获取信息 我们可以看到上面划线的信息。这就是我们要抓取的信息了。 当然。抓取的方法千篇一律，我们可以使用find()或find_all() 选择器，当然也可以使用 CSS选择器。但是 CSS选择器相对于前两者代码量稍微少一些。这里大家自己动手抓取，我们直接上代码供大家借鉴。 12345678910111213141516171819# 职位信息 positions = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_top &gt; a &gt; h3') # 工作地址 adds = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_top &gt; a &gt; span &gt; em') # 发布时间 publishs = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_top &gt; span') # 薪资信息 moneys = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_bot &gt; div &gt; span') # 工作需求 needs = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_bot &gt; div') # 发布公司 companys = soup.select('ul &gt; li &gt; div.list_item_top &gt; div.company &gt; div.company_name &gt; a') tags = [] # 由于我发现有的招聘信息没有标签信息，if判断防止没有标签报错 if soup.find('div', class_='li_b_l'): # 招聘信息标签 tags = soup.select('ul &gt; li &gt; div.list_item_bot &gt; div.li_b_l') # 公司福利 fulis = soup.select('ul &gt; li &gt; div.list_item_bot &gt; div.li_b_r') 获取到全部信息后，我们同样的把他们组成键值对字典。 123456789101112for position,add,publish,money,need,company,tag,fuli in \ zip(positions,adds,publishs,moneys,needs,companys,tags,fulis): data = &#123; 'position' : position.get_text(), 'add' : add.get_text(), 'publish' : publish.get_text(), 'money' : money.get_text(), 'need' : need.get_text().split('\n')[2], 'company' : company.get_text(), 'tag' : tag.get_text().replace('\n','-'), 'fuli' : fuli.get_text() &#125; 组成字典的目的是方便我们将全部信息保存到数据库。 保存数据库保存数据库前我们需要配置数据库信息： 12345import pymongoclient = pymongo.MongoClient('localhost', 27017)lagou = client['lagou']url_list = lagou['url_list'] 这里我们导入了pymongo库，并且与MongoDB建立连接，这里是默认连接本地的MongoDB数据。创建并选择一个数据库lagou，并在这个数据库中，创建一个table，即url_list。然后，我们进行数据的保存： 12if url_list.insert_one(data): print('保存数据库成功', data) 如果保存成功，打印出成功信息。 多进程抓取十万多条数据是不是抓取的有点慢，有办法，我们使用多进程同时抓取。由于Python的历史遗留问题，多线程在Python中始终是个美丽的梦。 12345678910111213from multiprocessing import Pooldef main(pages): datas = parse_index() for i in datas: url = i['url'] print(url) mongo_table = i['name'] # 因为有的职位是以'.'开头的，比如.Net，数据库表名不能以.开头 if mongo_table[0] == '.': mongo_table = mongo_table[1:] # 我们把之前抓取职位所有招聘信息的程序整理为parse_link()函数 # 这个函数接收职位url，页码，和数据库表名为参数 parse_link(url, pages, mongo_table) 我们把之前提取职位招聘信息的代码，写成一个函数，方便我们调用。这里的parse_link()就是这个函数，他就收职位的 url 和所有页数为参数。我们get_alllink_data()函数里面使用for循环 30 页的数据。然后这个作为主函数传给多进程内部调用。 123456if __name__ == '__main__': pool = Pool(processes=6) pages = ([p for p in range(1, 31)]) pool.map(main,pages) pool.close() pool.join() 这里是一个pool进程池，我们调用进程池的map方法. map(func, iterable[,chunksize=None]) 多进程Pool类中的map方法，与Python内置的map函数用法行为基本一致。它会使进程阻塞，直到返回结果。需要注意，虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。join() 方法等待子进程结束后再继续往下运行，通常用于进程间的同步. 针对反爬如果大家就这样整理完代码，直接就开始抓取的话。相信在抓取的不久后就会出现程序中止不走了。我刚刚第一次中止后，我以为是网站限制了我的 ip。于是我做了如下改动。 12345678910111213141516171819202122import randomimport timeuser_agent_list = [ 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36', ' Mozilla/5.0 (Windows; U; Windows NT 5.2) Gecko/2008070208 Firefox/3.0.1', 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.2)']user_agent = random.choice(user_agent_list)headers = &#123; 'User-Agent':user_agent, 'Connection':'keep-alive'&#125;proxy_list = [ 'http://140.224.76.21:808', 'http://60.178.14.90:8081', 'http://121.232.146.13:9000', ]proxy_ip = random.choice(proxy_list)proxies = &#123;'http': proxy_ip&#125;resp = requests.get(url, headers=headers, proxies=proxies)time.sleep(1) 这里我是直接在在网上找了一些免费的ip，还自己找了几个浏览器的user-agent。利用Python内置的random库，开始随机选择列表里的user-agent和ip。并且将参数传入requests内继续抓取。为了防止请求频率过快，我们设置每次请求结束停留一秒。然后我以为问题这么结束了。就开始继续抓取，但是在之前中断的位置突然有中断了。 于是，我在代码抓取信息位置添加了一句打印url的代码。我得到了中断爬取的url，然后我手动多点进去，发现了这个网页。当然出现这个情况并不是更换ip不可以，而是我们的ip太少了，一个ip可能仍然出现多次抓取的情况，后续我将会和大家一起写一个自己的ip代理池。 模拟登录没错这是一个登录界面，不知道是不是这里的浏览器头，或者ip请求过多。如果继续增多ip和user_agent就不会出现这种情况。 但是如何面对这种需要登录才能持续爬取的网站呢，很简单，我们只要打开浏览器的开发者工具。登录自己的账号，在Network标签找一个请求，查看你的请求头部信息，找到自己的cookies。这个cookies就是你的登录信息了，我们需要将他和你的user-agent一样，添加到请求头就可以了。如果大家不明白这个cookies是什么，没关系，后面会有专门的讲解。 1234headers = &#123; 'Cookie':'user_trace_token=20170603115043-d0c257a054ee44f99177a3540d44dda1; LGUID=20170603115044-d1e2b4d1-480f-11e7-96cf-525400f775ce; JSESSIONID=ABAAABAAAGHAABHAA8050BE2E1D33E6C2A80E370FE9167B; _gat=1; PRE_UTM=; PRE_HOST=; PRE_SITE=; PRE_LAND=https%3A%2F%2Fwww.lagou.com%2F; index_location_city=%E5%85%A8%E5%9B%BD; login=false; unick=""; _putrc=""; _ga=GA1.2.922290439.1496461627; X_HTTP_TOKEN=3876430f68ebc0ae0b8fac6c9f163d45; _ga=GA1.3.922290439.1496461627; LGSID=20170720174323-df1d6e50-6d2f-11e7-ac93-5254005c3644; LGRID=20170720174450-12fc5214-6d30-11e7-b32f-525400f775ce; Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1500541369; Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1500543655', 'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',&#125; 好了我们继续抓取，发现这次程序并没有中止。~但是由于博主网速过慢，抓取太慢了，还没有全部抓取下来，不知道后面会不会出现问题。~ 这次更新，我是完全抓取玩全站才过来的，没错我已经抓取了全部的信息。用时1412 .9008133411407 秒。 好了，说了这么多，基本也都说完了。 项目地址对整理代码有疑惑的伙伴可以点击 这里。 最后这是我已经抓取的一部分。这里如果大家希望看到Mongodb数据库里的保存内容。 我们需要安装一个Mongodb可视化应用 robomango。 安装没什么要说的，大家链接到我们的lagou数据库，就可以看到里面的数据了。 如果博主后面把全部数据抓下来后，可以和大家一起进行数据分析，分析Python招聘的一些信息啊什么的。大家加油。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(6):煎蛋网全站妹子图爬虫]]></title>
    <url>%2F2017%2F06%2F04%2FPython%E7%88%AC%E8%99%AB%E5%85%AD%E4%B9%8B%E7%85%8E%E8%9B%8B%E7%BD%91%E5%85%A8%E7%AB%99%E5%A6%B9%E5%AD%90%E5%9B%BE%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[上一篇文章中我们抓取了豆瓣图书的数据，如果大家运行成功，并且看到文件夹下的 txt 文件了。是不是有一种刚接触编程，第一次输出Hello world!时的欣喜。和上一篇实践不同，我们这一次来爬取 煎蛋网 全站妹子图，并且保存到指定文件夹下。 爬取流程 从煎蛋网妹子图第一页开始抓取； 爬取分页标签获得最后一页数字； 根据最后一页页数，获得所有页URL； 迭代所有页，对页面所有妹子图片url进行抓取；访问图片URL并且保存图片到文件夹。 开始通过上一篇文章的爬取过程，我们基本上理解了抓取一个网站的大致流程。因为一个网站虽然有很多页，但是大部分网站每一页的HTML标签内容都是相同的。我们只要获取到一页的内容，就可以获得所有页的内容了。那么开始之前，我们来分析一下煎蛋网妹子图页面的URL。 第一页的 url：http://jandan.net/ooxx/page-1 第二页：http://jandan.net/ooxx/page-2 最后一页：http://jandan.net/ooxx/page-93 不难发现，煎蛋网的 url 的规律是比较简单的，每一页后面 page 的数字就是几。那么我们可以通过一个循环就可以获得所有的页面 URL 了。但是大家应该想到，这个网站每天都会更新，今天是 93 页，明天就会增加到94页了。如果每一都要爬一次的话，那么每次都要改一下代码页数信息了。这样实现起来虽然可以，但是不免有些愚蠢。 所以我们需要通过页面的标签信息让程序自己获得页数，我们访问http://jandan.net/ooxx/这个页面时，就相当于我们直接访问了最后一页。大家可以自己试试看。 图中我们可以明确的看到最后一页的数字是94.只要通过这个页面的URL进行抓取就可以得到。我们先获得源码： 12345678import requestsfrom bs4 import BeautifulSoupurl = 'http://jandan.net/ooxx/'headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'&#125;resp = requests.get(url, headers=headers)soup = BeautifulSoup(resp.text, 'lxml') 我们按下f12，从页面源码中找到最后一页 94 所在的标签： 原来 94 就在这个span标签啊。接下来是不是很简单了： 12# 获得最高页码数allpage = soup.find('span', class_="current-comment-page").get_text()[1:-1] 由于标签内的 94 边包含一个[]，大家不要以为只是一个list，只要[0]就可以获得的。我们完全可以使用type(),看一下他的属性，就知道它是一个字符串，我们利用切片去掉第一个和最后一个字符，就得到了页数了。得到页数后，我们利用循环就可以得到所有页的url了： 12345urllist = []# for循环迭代出所有页面，得到urlfor page in range(1, int(allpage)+1): allurl = base_url + 'page-' + str(page) urllist.append(allurl) 我们把它保存到一个list中。那么现在，我们得到所有页面的 url，就可以来获取每一页的内容了。我们以最后一页为例来进行抓取。 我们仍然使用审查元素，找到图片 url 所在的标签。仍然是老方法，获取到页面所有包含图片的img标签： 12# css选择器allimgs = soup.select('div.text &amp;gt; p &amp;gt; img') 只要一行代码，我们就成功获得所有标签了。这里使用了 CSS选择器，大家是否还记得这个方法呢。可以查看之前的文章或者BeautifulSoup的官方文档了解哦。如果大家对 CSS 不是很熟悉，或者根本不知道。也没关系，反正find_all()和find()方法也是可以实现的。不过这里我教大家一个简单的CSS选择器方法。 我们只要按f12打开浏览器的开发者工具，找到标签的位置，右击标签。就可以看到这个情况： 没错，我们直接Copy selector的内容，粘贴出来就是这样的字符串：#comment-3468457 &gt; div &gt; div &gt; div.text &gt; p &gt; img 我们稍微去掉前面的一些标签，大多数情况下保留到父标签后面的内容就可以了。就是这样：div.text &gt; p &gt; img 我们放到代码中，运行一下就知道是不是成功了。 结果只一个列表： 1[&lt;img onload="add_img_loading_mask(this, load_sina_gif);" org_src="//wx4.sinaimg.cn/mw690/6adc108fly1fg90v6vzipg20ak05xqv5.gif" src="//wx4.sinaimg.cn/thumb180/6adc108fly1fg90v6vzipg20ak05xqv5.gif"/&gt;, &lt;img onload="add_img_loading_mask(this, load_sina_gif);" org_src="//wx4.sinaimg.cn/mw690/6adc108fly1fg90qymd8pg20dc0dcnph.gif" src="//wx4.sinaimg.cn/thumb180/6adc108fly1fg90qymd8pg20dc0dcnph.gif"/&gt;, &lt;img onload="add_img_loading_mask(this, load_sina_gif);" org_src="//wx4.sinaimg.cn/mw690/6adc108fly1fg90s98qsbg207e08mu10.gif" src="//wx4.sinaimg.cn/thumb180/6adc108fly1fg90s98qsbg207e08mu10.gif"/&gt;, &lt;img onload="add_img_loading_mask(this, load_sina_gif);" org_src="//wx3.sinaimg.cn/mw690/6adc108fly1fg90v51p5eg20a00dchdv.gif" src="//wx3.sinaimg.cn/thumb180/6adc108fly1fg90v51p5eg20a00dchdv.gif"/&gt;, &lt;img src="//wx4.sinaimg.cn/mw600/a1b56627gy1fdb7851js0j20ku4xcx6q.jpg"/&gt;, &lt;img src="//wx4.sinaimg.cn/mw600/a1b56627gy1fdb78fw14yj20dc46pb2a.jpg"/&gt;, &lt;img src="//wx3.sinaimg.cn/mw600/a1b56627gy1fdb78hqr1kj20dc1i0x02.jpg"/&gt;, &lt;img src="//wx4.sinaimg.cn/mw600/a1b56627gy1fdb78ldnwbj20ku3ei4qq.jpg"/&gt;, &lt;img src="//wx4.sinaimg.cn/mw600/a82b014bly1fg8gkj95toj21kw1kwh3s.jpg"/&gt;, &lt;img src="//wx4.sinaimg.cn/mw600/a82b014bly1fg8gkhmfraj21kw11xgth.jpg"/&gt;, &lt;img src="//wx2.sinaimg.cn/mw600/a82b014bly1fg8gklclivj21kw2dc1dg.jpg"/&gt;, &lt;img src="//wx2.sinaimg.cn/mw600/a82b014bly1fg8eohjekwj21421jkah5.jpg"/&gt;] 很显然，这一页的图片URL都在这里。接下来就是提取img标签的src属性了。1234567for img in list: urls = img['src'] # 判断url是否完整 if urls[0:5] == 'http:': img_url = urls else: img_url = 'http:' + urls 由于有的标签内url并不完整，所以这里我们进行一次判断。如果不完整就给他补全。 好了，图片的 url 都获得了，接下来就是保存图片了。大家还记得之前我们介绍Requests模块的时候，有过保存图片的演示。因为万维网中每个图片，每个视频都有唯一的 url 指向它们。所以我们只要访问这个 url，并且获得图片的二进制数据，保存到本地就可以了。 12345678910imgs = requests.get(img_url，headers=headers)filename = img_url.split('/')[-1]# 保存图片with open(filename, 'wb') as f: # 直接过滤掉保存失败的图片，不终止程序 try: f.write(imgs.content) print('Sucessful image:',filename) except: print('Failed:',filename) 大家注意，获取图片二进制数据是.content方法，而不是.text。这里我们有一个错误过滤，因为保存的过程中会有一些文件保存错误的情况，我们直接过滤掉，不终止程序的运行。 好了，爬虫程序到这里基本上已经全部实现了。但是我们如果把所有的图片存放在一个文件夹中，而且还是代码所在文件夹，不免有些难看。我们可以自己指定他们存放的位置。这里需要用的Python内置的os库了，不清楚的伙伴可以自己查看资料哈。 1234567891011121314151617# 创建文件夹的函数，保存到D盘def mkdir(path): # os.path.exists(name)判断是否存在路径 # os.path.join(path, name)连接目录与文件名 isExists = os.path.exists(os.path.join("D:\jiandan", path)) # 如果不存在 if not isExists: print('makedir', path) # 创建文件夹 os.makedirs(os.path.join("D:\jiandan", path)) # 切换到创建的文件夹 os.chdir(os.path.join("D:\jiandan", path)) return True # 如果存在了就返回False else: print(path, 'already exists') return False 我们只要传给这个函数一个路径参数就可以创建文件夹了。全部的功能都已经实现了，如果不出现以外的话，大家就可以在 d 盘看到这个文件夹了。 如果程序出现错误，可能是我们的程序访问过于频繁，网站封禁了我们的ip。这时，我们就要使用一个代理了。网上免费的代理很多，大家可以自己找，这里简单做一个使用代理的演示。由于是免费ip，不会存活太久就会不能使用，大家不要直接使用代码中的ip。后续可以带着大家一起代建一个自己的代理池。 1234567proxies = &#123;'http': '111.23.10.27:8080'&#125;try: # Requests库的get请求 resp = requests.get(url, headers=headers)except: # 如果请求被阻，就使用代理 resp = requests.get(url, headers=headers, proxies=proxies) 代码好了，最后上一下完整代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# -*- coding:utf-8 -*-# author: yukunimport requestsimport osimport timefrom bs4 import BeautifulSoup# 发出请求获得HTML源码def get_html(url): # 指定一个浏览器头 headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'&#125; # 代理，免费的代理只能维持一会可能就没用了，自行更换 proxies = &#123;'http': '111.23.10.27:8080'&#125; try: # Requests库的get请求 resp = requests.get(url, headers=headers) except: # 如果请求被阻，就使用代理 resp = requests.get(url, headers=headers, proxies=proxies) return resp# 创建文件夹的函数，保存到D盘def mkdir(path): # os.path.exists(name)判断是否存在路径 # os.path.join(path, name)连接目录与文件名 isExists = os.path.exists(os.path.join("D:\jiandan", path)) # 如果不存在 if not isExists: print('makedir', path) # 创建文件夹 os.makedirs(os.path.join("D:\jiandan", path)) # 切换到创建的文件夹 os.chdir(os.path.join("D:\jiandan", path)) return True # 如果存在了就返回False else: print(path, 'already exists') return False# 获得图片地址调用download函数进行下载def get_imgs(): # 调用函数获得所有页面 for url in all_page(): path = url.split('-')[-1] # 创建文件夹的函数 mkdir(path) # 调用请求函数获得HTML源码 html = get_html(url).text # 使用lxml解析器，也可以使用html.parser soup = BeautifulSoup(html, 'lxml') # css选择器 allimgs = soup.select('div.text &amp;gt; p &amp;gt; img') # 调用download函数下载保存 download(allimgs) # 执行完毕打出ok print('ok')# 获得所有页面def all_page(): base_url = 'http://jandan.net/ooxx/' # BeautifulSoup解析页面得到最高页码数 soup = BeautifulSoup(get_html(base_url).text, 'lxml') # 获得最高页码数 allpage = soup.find('span', class_="current-comment-page").get_text()[1:-1] urllist = [] # for循环迭代出所有页面，得到url for page in range(1, int(allpage)+1): allurl = base_url + 'page-' + str(page) urllist.append(allurl) return urllist# 保存图片函数，传入的参数是一页所有图片url集合def download(list): for img in list: urls = img['src'] # 判断url是否完整 if urls[0:5] == 'http:': img_url = urls else: img_url = 'http:' + urls filename = img_url.split('/')[-1] # 保存图片 with open(filename, 'wb') as f: # 直接过滤掉保存失败的图片，不终止程序 try: f.write(get_html(img_url).content) print('Sucessful image:',filename) except: print('Failed:',filename)if __name__ == '__main__': # 计时 t1 = time.time() # 调用函数 get_imgs() print(time.time() - t1) 经过计时，我们只需要146秒就爬取了全站的妹子图片了，大家注意身体。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(5):豆瓣读书练手爬虫]]></title>
    <url>%2F2017%2F06%2F03%2FPython%E7%88%AC%E8%99%AB%E4%BA%94%E4%B9%8B%E8%B1%86%E7%93%A3%E8%AF%BB%E4%B9%A6%E7%BB%83%E6%89%8B%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[我们在之前的文章中基本上掌握了Python爬虫的原理和方法，不知道大家有没有练习呢。今天我就来找一个简单的网页进行爬取，就当是给之前的兵书做一个实践。不然不就是纸上谈兵的赵括了吗。 好了，我们这次的目标是豆瓣图书Top250，地址是：https://book.douban.com/top250?start=0 准备爬一个网页我们至少要知道自己需要什么信息，大家看截图： 红色箭头标记的地方就是我们要获取的信息了，包括书的名字，作者和出版社信息，豆瓣评分和一句话简介。我们有了目标信息，就需要找到信息所在的页面源码，然后通过解析源码来获取到信息数据。那么，我们怎样获得页面 HTML 源代码呢？翻阅兵书，我们知道可以使用requests之计。代码实现如下： 1234import requestsresp = requests.get('https://book.douban.com/top250?start=0')print(resp.text) 运行程序，我们就轻松的获得了敌军的 HTML 信息了。但是问题又来了，我们得到 HTML 信息后，怎样得到我们的目标数据呢？ 深夜了，一轮弯月躲在云朵后面，窗外下着雨，我们坐在烛火前，翻阅兵书，顿时茅塞顿开，BeautifulSoup大法好。 我们打开浏览器，按f12到开发者工具，我们从网页源码里查找到数据位置，截图如下： 可以看到书名信息包含在class=&#39;pl2&#39; div里面的a标签内，是a标签的title属性。发现目标位置后，就简单多了。我们利用BeautifulSoup来获得一个对象，按找标准的缩进显示的html代码： 123from bs4 import BeautifulSoupsoup = BeautifulSoup(resp.text, 'lxml') 推荐大家使用lxml解析器，因为他快。如果安装lxml遇到问题的可以参考 上一篇文章 的方法。当然，如果大家怕麻烦，也完全可以使用Python的内置标准库html.parser.对我们获得结果并没有影响。 开始工作现在我们要用到BeautifulSoup的find_all()选择器，因为我们这一页有很多书，而每一本书的信息都包含在class=pl2的div标签内，我们使用find_all()就可以直接得到本页所有书的书名了。我们用find()方法和find_all()方法来做一个比较： 12345678910# find_all()方法，# 注意class是Python关键词，后面要加下划线_：alldiv = soup.find_all('div', class_='pl2')for a in alldiv: names = a.find('a')['title'] print('find_all():', names)# find()方法：alldiv2 = soup.find('div', class_='pl2')names2 = alldiv2.find('a')['title']print('find():', names2 ) 运行结果： 123456789find_all(): 追风筝的人find_all(): 小王子# ...# ...省略部分# ...find_all(): 三体Ⅲfind(): 追风筝的人Process finished with exit code 0 我们通过结果就可以看到两者之间的差距了，前者输出了一页的数据，而后者只输出了第一条数据。所以包括后面的信息，由于每一天数据所在标签是一样的，我们都是用find_all()方法。 上面的代码写的优雅点，就是这样实现,注意结果是一个 list： 1234# 书名, 注意是L小写，不是阿拉伯数字1alldiv = soup.find_all('div', class_='pl2')names = [a.find('a')['title'] for a in alldiv]print(names) 这样书名数据我们就得到了，接下来是作者信息。方法和获取书名方法一样： 1234# 作者,由于信息在一个p标签内部，# 我们获取到标签直接get_text()方法获得文本内容allp = soup.find_all('p', class_='pl')authors = [p.get_text() for p in allp] 运行结果： 123456789[&apos;[美] 卡勒德·胡赛尼 / 李继宏 / 上海人民出版社 / 2006-5 / 29.00元&apos;, &apos;[法] 圣埃克苏佩里 / 马振聘 / 人民文学出版社 / 2003-8 / 22.00元&apos;, &apos;钱锺书 / 人民文学出版社 / 1991-2 / 19.00&apos;, &apos;余华 / 南海出版公司 / 1998-5 / 12.00元&apos;, # ...# ...省略部分结果# ...&apos;高铭 / 武汉大学出版社 / 2010-2 / 29.80元&apos;, &apos;刘慈欣 / 重庆出版社 / 2010-11 / 38.00元&apos;] 后面的评分内容和简介内容也是一样获得，只是标签不同，但是方法一样，具体也不需要多余赘述。直接看实现代码： 123456# 评分starspan = soup.find_all('span', class_='rating_nums')scores = [s.get_text() for s in starspan]# 简介sumspan = soup.find_all('span', class_='inq')sums = [i.get_text() for i in sumspan] 程序运行成功，我们就获得了4个list，分别是书名，作者，评分和简介内容。我们要把他们放在一起，打印出来，就是一页的数据信息了。这里我们使用zip()函数，zip()函数在运算时，会以一个或多个序列做为参数，返回一个元组的列表。同时将这些序列中并排的元素配对。 123456for name, author, score, sum in zip(names, authors, scores, sums): name = '书名：' + str(name) + '\n' author = '作者：' + str(author) + '\n' score = '评分：' + str(score) + '\n' sum = '简介：' + str(sum) + '\n' data = name + author + score + sum 我们使用换行符’\n‘给数据信息一点整齐的样式。我们可以查看到打印的结果，并没有所有数据黏在一起，显得丑陋。获得信息后，就是保存数据了。保存数据也很简单，Python的文件读写操作就可以实现。代码如下： 1234567# 文件名filename = '豆瓣图书Top250.txt'# 保存文件操作with open(filename, 'w', encoding='utf-8') as f: # 保存数据 f.writelines(data + '=======================' + '\n')print('保存成功') 运行成功，我们就可以看到项目文件下面的 txt 文件了，上面保存了我们上面打印出来的内容。 123456789101112书名：追风筝的人作者：[美] 卡勒德·胡赛尼 / 李继宏 / 上海人民出版社 / 2006-5 / 29.00元评分：8.8简介：为你，千千万万遍==================# ...# ...书名：活着作者：余华 / 南海出版公司 / 1998-5 / 12.00元评分：9.1简介：活着本身就是人生最大的意义================== 但是，我们要的是 250 条数据，而不是一页的十几条数据，那么要怎么获得到所有的数据呢。我们可以检查页面的信息，可以看到页面一共 10 页，第一页的URL是https://book.douban.com/top250?start=0。而最后一页的 URL 是https://book.douban.com/top250?start=225我们接着多看几页，第二页是https://book.douban.com/top250?start=25，第三页是https://book.douban.com/top250?start=50。 规律已经很清晰了，我们的页面的页数信息是最后的start=后面的数字。而且数字从0开始到225，每一页数字加 25.这就很简单了，我们以https://book.douban.com/top250?start=为基层URL，每一页在后面加页面的页数数字。就可以得到所有的页面 url 了。再以for循环迭代每一个 url，使用上面获取数据的方法，获得所有的数据信息。 获取所有页面URL的代码如下： 123456base_url = 'https://book.douban.com/top250?start='urllist = []# 从0到225，间隔25的数组for page in range(0, 250, 25): allurl = base_url + str(page) urllist.append(allurl) 我们把他保存在 list 里面，好用循环迭代。 代码那么，所有的功能都实现了。现在，我们只要将所有的代码组合起来，就可以实现我们需要的所有功能了。上代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# -*- coding:utf-8 -*-# author: yukunimport requestsfrom bs4 import BeautifulSoup# 发出请求获得HTML源码的函数def get_html(url): # 伪装成浏览器访问 headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'&#125; resp = requests.get(url, headers=headers).text return resp# 解析页面，获得数据信息def html_parse(): # 调用函数，for循环迭代出所有页面 for url in all_page(): # BeautifulSoup的解析 soup = BeautifulSoup(get_html(url), 'lxml') # 书名 alldiv = soup.find_all('div', class_='pl2') names = [a.find('a')['title'] for a in alldiv] # 作者 allp = soup.find_all('p', class_='pl') authors = [p.get_text() for p in allp] # 评分 starspan = soup.find_all('span', class_='rating_nums') scores = [s.get_text() for s in starspan] # 简介 sumspan = soup.find_all('span', class_='inq') sums = [i.get_text() for i in sumspan] for name, author, score, sum in zip(names, authors, scores, sums): name = '书名：' + str(name) + '\n' author = '作者：' + str(author) + '\n' score = '评分：' + str(score) + '\n' sum = '简介：' + str(sum) + '\n' data = name + author + score + sum # 保存数据 f.writelines(data + '=======================' + '\n')# 获得所有页面的函数def all_page(): base_url = 'https://book.douban.com/top250?start=' urllist = [] # 从0到225，间隔25的数组 for page in range(0, 250, 25): allurl = base_url + str(page) urllist.append(allurl) return urllist# 文件名filename = '豆瓣图书Top250.txt'# 保存文件操作f = open(filename, 'w', encoding='utf-8')# 调用函数html_parse()f.close()print('保存成功。') 我们只用了36行的代码（去掉空行和注释）就实现了抓取豆瓣图书的数据了。大家是不是觉得很简单了，不要兴奋，这只是一个小白最基础的练手项目，大家快去找更有挑战性的项目实现吧。大家加油。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行走世间都是妖怪]]></title>
    <url>%2F2017%2F06%2F02%2F%E8%A1%8C%E8%B5%B0%E4%B8%96%E9%97%B4%E9%83%BD%E6%98%AF%E5%A6%96%E6%80%AA%2F</url>
    <content type="text"><![CDATA[热的时候喊热是无济于事的。顶着火红的太阳，看着姗姗来迟的 138 路公交，无奈的翻翻白眼。 南京的天气一如既往的热，由于科目二考试将至，连安静呆在宿舍的想法都无法满足。不过年轻人就是应该多出来走走，不然就无法看到外面美丽的姑娘们了。钱钟书说暴露的姑娘是局部的真理，我觉得这再合适不过了，真理总是赤裸的。正值躁动青春期的室友，掩饰不住自己对真理渴望的心，时不时露出莫名其妙的笑。 倒车，转弯，刹车，踩离合。时间就在这些机械的步骤中流过，真让人兴奋，终于可以回到凉爽的宿舍了。室友说他要在中途有个事，说不坐公交回去了。我说开什么玩笑，不坐公交怎么回去，难道请我打车。然后在我惊讶的目光下，他推来辆自行车。 其实我一直搞不清楚共享单车的盈利方式，扫个码就可以免费骑，而且还可以扫两辆。于是我们骑着自行车，行驶在烈日之下，行驶在公交车行驶30分钟的路程上，挥汗如雨。路边不断的有单身的或者不是单身的姑娘走过，为了自己的形象，即使在疲惫也要摆出轻松的姿势，面露沉稳的微笑，匆匆驶过，流下一阵燥热的风。 我今年二十多岁了，一直单身。我仍然不明白有些人究竟是有多么的空虚，多么的寂寞。分个手吵个架，遇到了不舒服的事就发个朋友圈，发个微博，感时运之不济，叹人性之不真。然而我在这条通往目的地的道路上，面对着周围美丽的真理，我竟没有一点想法。 我想那些天天矫情的分享人生感悟的人，并不是他们在人生的道路上遇到了磨难，而是他们都太闲了。走出房门，扫个码骑个自行车，行驶个40分钟，自然会明白还是屋里舒服。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(4):Beautiful Soup的常用方法]]></title>
    <url>%2F2017%2F06%2F01%2FPython%E7%88%AC%E8%99%AB%E5%9B%9B%E4%B9%8BBeautifulSoup%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Requests库的用法大家肯定已经熟练掌握了，但是当我们使用Requests获取到网页的 HTML 代码信息后，我们要怎样才能抓取到我们想要的信息呢？我相信大家肯定尝试过很多办法，比如字符串的 find 方法，还有高级点的正则表达式。虽然正则可以匹配到我们需要的信息，但是我相信大家在匹配某个字符串一次一次尝试着正则匹配的规则时，一定很郁闷。 那么，我们就会想有没有方便点的工具呢。答案是肯定的，我们还有一个强大的工具，叫BeautifulSoup。有了它我们可以很方便地提取出HTML或XML标签中的内容，这篇文章就让我们了解下BeautifulSoup的常用方法吧。 什么是BeautifulSoup？Python的网页解析可以用正则表达式去完成，那么我们在写的时候，要挨个的去把代码拿出来匹配，而且还要写匹配的规则，整体实现起来就很复杂。BeautifulSoup呢，它是一个方便的网页解析库，处理高效，支持多种解析器。大部分情况下，利用它我们不在需要编写正则表达式就可以方便的实现网页信息的提取。 官方文档 安装：$ pip install beautifulsoup4 BeautifulSoup是一个网页解析库，它支持很多解析器，不过最主流的有两个。一个是Python标准库，一个是lxml HTML 解析器。两者的使用方法相似： 1234567from bs4 import BeautifulSoup# Python的标准库BeautifulSoup(html, 'html.parser')# lxmlBeautifulSoup(html, 'lxml') Python内置标准库的执行速度一般，但是低版本的Python中，中文的容错能力比较差。lxmlHTML 解析器的执行速度快，但是需要安装 C语言的依赖库。 lxml的安装由于lxml安装需要依赖C语言库，所以当lxml在Windows上安装时，我们会发现各种奇怪的报错，当然脸好的使用pip install lxml 安装也是可以成功的。不过大部分人都是会倒在这里。 这里推荐大家使用lxml的.whl文件来安装。首先我们需要安装一下wheel库，有了这个库我们才可以正常安装.whl文件。pip install wheel 从官方网站下载与系统，Python版本匹配的lxml文件：地址。 另外，不知道自己系统和python版本信息的伙伴。需要进入系统管理员工具（CMD）或者python的 IDLE，输入以下代码： 123import pipprint(pip.pep425tags.get_supported()) 这时我们就可以看到打印出来的Python版本信息了。下载好lxml的文件后，我们需要找到文件的位置，然后进入管理员工具，使用pip安装：pip install whl文件的全名 安装完成后，可以进入Python，import一下，如果没有报错，那么恭喜你安装成功。如果有的伙伴觉得麻烦，那我推荐大家安装anaconda 下载地址（如果安装速度慢，可以找国内镜像），不知道是什么的小伙伴可以谷歌一下，有了他，那些在windows上pip安装出错的问题将不再存在。 BeautifulSoup的基本标签选择方法虽然Python内置的标准库解析器还不错，但是我还是推荐大家使用lxml，因为它够快。那么后面的代码我们都是用lxml解析器来进行演示。我们先导入官方文档的例子: 12345678910111213html_doc = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;""" HTML 代码,我们能够得到一个BeautifulSoup的对象,并能按照标准的缩进格式的结构输出: 12from bs4 import BeautifulSoupsoup = BeautifulSoup(html_doc, 'lxml') 我们可以看到上面的 HTML 代码并不完整，接下来我们使用prettify()方法来进行自动补全，注释部分就是运行的输出： 123456789101112131415161718192021222324252627282930313233print(soup.prettify())# &lt;html&gt;# &lt;head&gt;# &lt;title&gt;# The Dormouse's story# &lt;/title&gt;# &lt;/head&gt;# &lt;body&gt;# &lt;p class="title"&gt;# &lt;b&gt;# The Dormouse's story# &lt;/b&gt;# &lt;/p&gt;# &lt;p class="story"&gt;# Once upon a time there were three little sisters; and their names were# &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;# Elsie# &lt;/a&gt;# ,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;# Lacie# &lt;/a&gt;# and# &lt;a class="sister" href="http://example.com/tillie" id="link2"&gt;# Tillie# &lt;/a&gt;# ; and they lived at the bottom of a well.# &lt;/p&gt;# &lt;p class="story"&gt;# ...# &lt;/p&gt;# &lt;/body&gt;# &lt;/html&gt; 获取标签12print(soup.title)# &lt;title&gt;The Dormouse's story&lt;/title&gt; 通过输出结果，我们可以看到获取内容的属性，实际上就是 HTML 代码里的一个title标签。 获取名称12print(soup.title.name)# 'title' 实际上就是标签的名称。 获取属性12345print(soup.p.attrs['class'])# 'title'print(soup.p['class'])# 'title' 获取标签的属性我们可以使用attrs方法，传给他属性名，就可以得到标签的属性。通过结果我们可以看到，直接传给p标签属性名，一样可以获取到标签属性。 获取内容12print(soup.title.string)# 'The Dormouse's story' 我们还可以使用嵌套的选择，比如我们获得body标签里面p标签的内容： 12print(soup.body.p.string)# 'The Dormouse's story' 常见用法标准选择器虽然BeautifulSoup的基本用法，标签获取，内容获取，可以解析一些 html代码。但是在遇到很多复杂的页面时，上面的方法是完全不足的，或者是很繁琐的，因为有时候有的标签会有几个属性（class、id等）。 索性BeautifulSoup给我们提供了很方便的标准选择器，也就是 API 方法，这里着重介绍2个: find() 和 find_all() 。其它方法的参数和用法类似,大家举一反三吧。 find_all()find_all(name, attrs, recursive, text, **kwargs)可以根据标签，属性，内容查找文档。find_all()其实和正则表达式的原理很相似，他能找出所有能满足匹配模式的结果，在把结果以列表的形式返回。仍然是文档的例子： 12345678910111213141516html_doc = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html_doc, 'lxml') 过滤器文档参考介绍 find_all() 方法前,大家可以参考一下过滤器的类型。过滤器只能作为搜索文档的参数,或者说应该叫参数类型更为贴切。这些过滤器贯穿整个搜索的API。过滤器可以被用在 tag 的name中,节点的属性中,字符串中或他们的混合中。 find_all() 方法搜索当前 tag 的所有 tag 子节点,并判断是否符合过滤器的条件。这里有几个例子: 12345678910111213soup.find_all("title")# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]soup.find_all("p", "title")# [&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;]soup.find_all("a")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]soup.find_all(id="link2")# [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;] 有几个方法很相似,还有几个方法是新的,参数中的 string 和id是什么含义? 为什么 find_all(&quot;p&quot;, &quot;title&quot;) 返回的是CSS Class为”title”的标签? 我们来仔细看一下find_all()的参数: name参数name 参数可以查找所有名字为 name 的 tag,字符串对象会被自动忽略掉。 12soup.find_all("title")# [The Dormouse's story] 搜索 name 参数的值可以使任一类型的过滤器,字符窜,正则表达式,列表,方法或是True 。我们常用的 name 参数是搜索文档的标签名。 keyword参数如果我们的 HTML代码中有几个div标签，但是我们只想获取到class属性为top的div标签，我们怎么出来呢。 12soup.find_all('div', class_='top')# 这里注意下，class是Python的内部关键词，我们需要在css属性class后面加一个下划线'_'，不然会报错。 仍然以上面的代码实例： 12soup.find_all('a', id='link2')# [&lt;a id="link2" href="http://example.com/lacie"&gt;Lacie&lt;/a&gt;] 这样我们就只获取到id为link2的a标签。 limit参数find_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢。如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量。效果与 SQL 中的limit关键字类似,当搜索到的结果数量达到limit的限制时,就停止搜索返回结果。 比如我们要搜索出a标签，但是满足的有3个，我们只想要得到2个： 123soup.find_all("a", limit=2)# [&lt;a id="link1" class="sister" href="http://example.com/elsie"&gt;Elsie&lt;/a&gt;,# &lt;a id="link2" class="sister" href="http://example.com/lacie"&gt;Lacie&lt;/a&gt;] 其他的参数，不是经常用到，大家如需了解可以参考官方文档。 find()find_all()返回的是所有元素列表，find()返回单个元素。 find( name , attrs , recursive , string , **kwargs ) find_all()方法将返回文档中符合条件的所有 tag,尽管有时候我们只想得到一个结果。比如文档中只有一个标签,那么使用find_all() 方法来查找标签就不太合适, 使用find_all方法并设置limit=1参数不如直接使用find()方法。下面两行代码是等价的: 12345soup.find_all('title', limit=1)# [The Dormouse's story]soup.find('title')#The Dormouse's story 唯一的区别是find_all()方法的返回结果是值包含一个元素的列表,而find()方法直接返回结果。find_all()方法没有找到目标是返回空列表, find()方法找不到目标时,返回None。 CSS选择器Beautiful Soup支持大部分的 CSS选择器。在Tag或BeautifulSoup对象的.select()方法中传入字符串参数, 即可使用 CSS选择器的语法找到 tag。我们在写 css 时，标签 class类名加”.“，id属性加”#“。 12soup.select("title")# [The Dormouse's story] 通过 tag标签逐层查找: 1234567soup.select("body a")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]soup.select("html head title")# [&lt;title&gt;The Dormouse's story&lt;/title&gt;] 找到某个 tag标签下的直接子标签: 12345678910111213soup.select("head &gt; title")# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]soup.select("p &gt; a")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]soup.select("p &gt; #link1")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]soup.select("body &gt; a")# [] 通过 CSS 的 class类名查找: 1234soup.select(".sister")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;] 通过 tag 的 id 查找: 12345soup.select("#link1")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]soup.select("a#link2")# [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;] 同时用多种 CSS选择器查询元素，使用逗号隔开: 123soup.select("#link1,#link2")# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,# &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;] 提取标签内容如果我们得到了几个标签： 12345list = [&lt;a href="http://www.baidu.com/"&gt;百度&lt;/a&gt;,&lt;a href="http://www.163.com/"&gt;网易&lt;/a&gt;,&lt;a href="http://www.sina.com/"新浪&lt;/a&gt;] 我们要怎样提取他里面的内容呢。我们开始的时候有提及。 1234for i in list: print(i.get_text()) # 我们使用get_text()方法获得标签内容 print(i.get['href'] # get['attrs']方法获得标签属性 print(i['href']) # 简写结果一样 结果： 123456789百度网易新浪http://www.baidu.com/http://www.163.com/http://www.sina.com/http://www.baidu.com/http://www.163.com/http://www.sina.com/ 总结 BeautifulSoup的解析库，推荐使用lxml，如果出现乱码的情况下，可以使用html.parser； BeautifulSoup的标签选择筛选方法，虽然弱但是速度快； 推荐使用find_all(),find()方法搜索标签，当然如果对css选择器熟悉，推荐使用.select()方法； get_text()方法获取标签文本内容，get[attrs]方法获取标签属性值。 本篇我们就基本上整理了BeautifulSoup的常用方法。如果大家希望了解更高级的用法，可以查看BeautifulSoup的官方文档。 最后，大家可以结合Requests库写出自己的爬虫吧。 谢谢阅读]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(3):Requests的高级用法]]></title>
    <url>%2F2017%2F05%2F29%2FPython%E7%88%AC%E8%99%AB%E4%B8%89%E4%B9%8BRequests%E7%9A%84%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[上一篇文章我们整理了Requests库的基本用法，相信大家已经经过爬取一些简单网页的练习，已经很熟练了。 这一篇文章我们来 看一下Requests库的高级操作。 高级操作1.文件上传12345import requestsfiles = &#123;'file' : open('logo.gif','rb')&#125;resp = requests.post('http://httpbin.org/post', files=files)print(resp.text) 文件上传的操作只要我们从文件夹中把文件读取出来，并且赋值给 files 参数，就可以了，打印出源代码我们就可以看待上传文件的字节流了。 2.获取Cookie12345678&gt;&gt;&gt;import requests&gt;&gt;&gt;resp = requests.get('http://www.baidu.com')&gt;&gt;&gt;print(resp.cookies)&lt;RequestsCookieJar[]&gt;&gt;&gt;&gt;for key, value in resp.cookies.items():... print(key + '=' + value)BDORZ=27315 我们可以通过获取字典的键值对来查看cookie. 3.会话维持我们获得到了cookie就可以做一个会话维持，可以维持一个登录的状态，也就是做模拟登录。我们来看实现方式： 123456import requestss = requests.Session()s.get('http://httpbin.org/cookies/set/number/123456789') # 设置了一个cookieresp = s.get('http://httpbin.org/cookies')print(resp.text) 这就相当于模拟了一个会话，比如做登陆验证，可以用session，POST 一下，登陆一下，然后保持会话信息，在访问登录过页面的话，就可以正常获取登录后的页面了。如果你要模拟登录，可以通过申明Session对象，再用Session对象发起两次get请求，那么这两次请求相当于在一个浏览器里面，先访问set cookie页面，在访问get cookie页面。当然，cookie是自动处理的，不需要担心写一些处理cookies的方法。 建议模拟登录用requests的Session对象。 4.SSL证书验证Requests可以为 HTTPS 请求验证 SSL 证书，就像 web浏览器一样。要想检查某个主机的 SSL证书，你可以使用 verify参数: 1234&gt;&gt;&gt;import requests&gt;&gt;&gt;requests.get('https://kennethreitz.com', verify=True) # verify参数默认值为Truerequests.exceptions.SSLError: hostname 'kennethreitz.com' doesn't match either of '*.herokuapp.com', 'herokuapp.com' 如果不想他报这个错误，我们可以把参数verify的值设为False.运行后发现程序没有报错，但是会出现警告信息，警告我们要验证 SSL证书。如果要消除这个警告，我们需要调用原生包： 12345&gt;&gt;&gt;import requests&gt;&gt;&gt;from requests.packages import urllib3urllib3.disable_warnings()&gt;&gt;&gt;requests.get('https://kennethreitz.com', verify=False) 我们还可以自己指定一个证书： 12345&gt;&gt;&gt;import requests&gt;&gt;&gt;resp = requests.get('https://kennethreitz.com', cert=('/path/server.crt', '/path/key'))&gt;&gt;&gt;print(resp.status_code)200 5.代理设置有些网站会限制 IP 访问频率，超过频率就断开连接。这个时候我们就需要使用到代理，我们可以通过为任意请求方式提供proxies参数来配置单个请求。 12345678import requestsproxies = &#123; "http": "http://10.10.1.10:3128", "https": "http://10.10.1.10:1080",&#125;resp = requests.get('http://www.baidu.com', proxies=proxies)print(resp.status_code) 也可以通过环境变量 HTTP_PROXY 和 HTTPS_PROXY 来配置代理。有些代理需要加上用户名和密码的，代理可以使用http://user:password@host/语法，比如： 123proxies = &#123; "http": "http://user:pass@10.10.1.10:3128/",&#125; 除了基本的 HTTP代理，Requests还支持SOCKS协议的代理，如果需要用的，可以安装带三方库： 1$ pip install requests[socks] 安装好依赖以后，使用 SOCKS 代理和使用 HTTP 代理一样简单： 1234proxies = &#123; "http": 'socks5://user:pass@host:port', "https": 'socks5://user:pass@host:port'&#125; 6.超时设置超时设置就是设置请求的时间，如果在规定的时间内没有返回应答，就抛出异常. 1234import requestsresp = requests.get('http://www.baidu.com', timeout=0.5)print(resp.status_code) 如果在0.5秒内没有返回，就会报出ReadTimeout的异常。如果远端服务器很慢，你可以让Request永远等待，传入一个None作为timeout值，然后就冲咖啡去吧。 7.认证设置有一些网站在访问的时候需要我们输入用户名和密码，那么这种网站我们要怎样处理呢。 12345import requestsfrom requests.auth import HTTPBasicAuthresp = requests.get(url, auth=HTTPBasicAuth('username','password'))print(resp.status_code) 调用HTTPBasicAuth类，直接传入用户名和密码就可以了。 8.异常处理如果你遇到无法访问的网站，或者是你的网速不够快，你的访问超时，就会导致程序的中断。显然我们在实际的抓取中不愿意看到爬取到一半的程序突然中断的情况，那么我们能够避免这种程序中断的情况吗，答案是肯定的： 123456789101112import requestsfrom requests.exceptions import ReadTimeout, ConnectionError, RequestExceptiontry: resp = requests.get('http://httpbin.org/get', timeout=0.5) print(resp.status_code)except ReadTimeout： # 访问超时的错误 print('Timeout')except ConnectionError: # 网络中断连接错误 print('Connect error')except RequestException: # 父类错误 print('Error') 这样我们就可以把requests抓取过程中常见的异常都处理捕获了，捕获错误应该先捕获子类异常在捕获父类异常，这样做能够更加直观清楚的应对程序中出现的错误了。 如果我们能够自己捕获了这些异常，就可以保证我们的爬虫一直运行了。 好了，Requests的大部分用法已经全部说完了，大家是否已经学会了这门屠龙之术了呢。快找个网页练练手吧。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(2):Requests的基本用法]]></title>
    <url>%2F2017%2F05%2F29%2FPython%E7%88%AC%E8%99%AB%E4%BA%8C%E4%B9%8BRequests%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[虽然Python有内置的urllib库，可以实现网络的请求，但是我并不推荐。因为urllib在很多时候使用起来不方便，比如加一个代理，处理Cookie时API都很繁琐，再比如发送一个POST请求也很麻烦。 而Requests就相当于urllib的升级版本，简化了urllib的使用方法。有了Requests，我们可以用几句代码实现代理的设置，Cookie的设置，非常方便。下面我就给大家整理了Requests库的使用方法和细节。详细可以参考Requests官方文档。 什么是Requests？Requests是Python语言编写，基于urllib3，采用Apache2 Licensed开源协议的HTTP库。 它比urllib更加方便，可以节约我们大量的工作，完全满足HTTP测试需求。是Python实现的简单易用的HTTP库。 安装也很简单：pip install requests Requests的语法操作1.实例引入1234567import requestsresponse = requests.get('http://www.baidu.com/')print(response.status_code)print(type(response.text))print(response.text)print(response.cookies) 运行结果：12345200&lt;class &apos;str&apos;&gt; # ...HTML网页源码..&lt;RequestsCookieJar[]&gt; 可以看到，我们非常方便的就获取到了Cookies. 2.各种请求方式123456import requestsrequests.get('http://httpbin.org/get') # 发送get请求requests.post('http://httpbin.org/post') # 发送post请求，只要调用post方法，传入一个url参数requests.put('http://httpbin.org/put')requests.delete('http://httpbin.org/delete') 官方文档里提供的这个网址足够我们测试这些请求方式了。 请求1.基本GET请求1234import requestsresp = requests.get('http://httpbin.org/get')print(resp.text) 这个我们前面有使用过，也是最常用的方法。运行成功就可以看到网页的源码了。 2.带参数的GET请求12345678import requestsdata = &#123; 'name' : 'jack', 'age' : 20&#125;resp = requests.get('http://httpbin.org/get', params=data)print(resp.text) 传入参数只需要我们把数据生成一个字典，然后调用params参数，赋值给他就可以，是不是很方便。 3.解析json12345678import requestsimport jsonresp = requests.get('http://httpbin.org/get')print(resp.text)print(resp.json())print(json.loads(resp.text))print(type(resp.json())) 运行结果： 可以看出Requests的jaon解析和json的loads方法解析出来的结果是完全一样的。所以Requests可以很方便的解析json数据。 4.获取二进制数据12345import requestsresp = requests.get('http://www.baidu.com/img/baidu_jgylogo3.gif')print(resp.content)print(resp.text) 运行成功我们可以看到content方法获取的图片页面源码是二进制数据，而text获取的则是字符串代码。显然获取图片这种二进制数据需要使用content方法。 12with open('logo.gif','wb') as f: f.write(resp.content) 这样我们就保存了图片，我们可以在文件夹下看到这张图片。 5.添加headers12345import requestsheaders = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'&#125;resp = requests.get('http://www.baidu.com', headers=headers)print(resp.text) 有些网页如果我们直接去请求的话，他会查看请求的对象是不是浏览器，如果没有浏览器信息就会禁止我们爬虫的访问，这个时候我们就要给爬虫加一个headers，加一个浏览器的user-agent信息。这样我们就可以正常访问了。如果有的伙伴不知道怎么得到User-Agent，可以打开浏览器的审查元素，找到network，随便点击一个链接就可以看到User-Agent的信息了。 6.基本POST请求12345678import requestsdata = &#123; 'name' : 'jack', 'age' : 20&#125;resp = requests.post('http://httpbin.org/post', data=data)print(resp.text) 一个POST必然是要有一个Form Data的表单提交的，我们只要把信息传给data参数就可以了。一个POST请求只需要调用post方法，是不是特别方便呢。如果不觉得方便的话，可以去参考urllib的使用方法。 响应1.response属性123456789import requestsresponse = requests.get('http://www.baidu.com/')print(type(response.status_code)) # 状态码print(type(response.text)) # 网页源码print(type(response.headers)) # 头部信息print(type(response.cookies)) # Cookieprint(type(response.url)) # 请求的urlprint(type(response.history)) # 访问的历史记录 获取这些信息只需要简单的调用就可以实现了。 2.状态码判断12345&gt;&gt;&gt;import requests &gt;&gt;&gt;response = requests.get('http://www.baidu.com/')&gt;&gt;&gt;exit() if not resp.status_code == 200 else print('Sucessful')Sucessful 如果发送了一个错误请求(一个4XX客户端错误，或者5XX服务器错误响应)，我们可以通过 Response.raise_for_status() 来抛出异常： 12345678&gt;&gt;&gt;bad_r = requests.get('http://httpbin.org/status/404')&gt;&gt;&gt;bad_r.status_code404&gt;&gt;&gt;bad_r.raise_for_status()Traceback (most recent call last): File "requests/models.py", line 832, in raise_for_status raise http_errorrequests.exceptions.HTTPError: 404 Client Error 好了，这篇文章我们了解了Requests库的基本语法操作，相信大家对Requests库的请求和响应已经很清楚了，大家完全可以抓取一些网页了。 纸上得来终觉浅，绝知此事要躬行，大家加油！]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(1):基本原理]]></title>
    <url>%2F2017%2F05%2F28%2FPython%E7%88%AC%E8%99%AB%E4%B8%80%E4%B9%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[大家好，今天周末，希望大家周末愉快。 这篇文章我来梳理一下爬虫的基本原理。用过Python的伙伴都知道Python用来写爬虫是件很简单很爽的事情。但是有些伙伴不了解爬虫到底是什么，会纳闷为什么爬虫要设置什么请求头、请求体啊，到底要怎么样去发送这个请求啊。当然有的伙伴使用过很多爬虫的请求库，解析库，写过很多爬虫，但是可能他们的思路脉络也不是太清晰。那么，这篇文章就来帮大家理清爬虫原理的脉络。 什么是爬虫？ 爬虫即网络爬虫，英文是Web Spider。翻译过来就是网络上爬行的蜘蛛，如果把互联网看作一张大网，那么爬虫就是在大网上爬来爬去的蜘蛛，碰到想要的食物，就把他抓取出来。 我们在浏览器中输入一个网址，敲击回车，看到网站的页面信息。这就是浏览器请求了网站的服务器，获取到网络资源。那么，爬虫也相当于模拟浏览器发送请求，获得到HTML代码。HTML代码里通常包含了标签和文字信息，我们就从中提取到我们想要的信息。 通常爬虫是从某个网站的某个页面开始，爬取这个页面的内容，找到网页中的其他链接地址，然后从这个地址爬到下一个页面，这样一直不停的爬下去，进去批量的抓取信息。那么，我们可以看出网络爬虫就是一个不停爬取网页抓取信息的程序。 爬虫的基本流程1.发起请求：通过HTTP库向目标站点发起请求，即发送一个Request，请求可以包含额外的headers等信息，然后等待服务器响应。这个请求的过程就像我们打开浏览器，在浏览器地址栏输入网址：www.baidu.com，然后点击回车。这个过程其实就相当于浏览器作为一个浏览的客户端，向服务器端发送了 一次请求。 2.获取响应内容：如果服务器能正常响应，我们会得到一个Response，Response的内容便是所要获取的内容，类型可能有HTML、Json字符串，二进制数据(图片，视频等）等类型。这个过程就是服务器接收客户端的请求，进过解析发送给浏览器的网页HTML文件。 3.解析内容：得到的内容可能是HTML，可以使用正则表达式，网页解析库进行解析。也可能是Json，可以直接转为Json对象解析。可能是二进制数据，可以做保存或者进一步处理。这一步相当于浏览器把服务器端的文件获取到本地，再进行解释并且展现出来。 4.保存数据：保存的方式可以是把数据存为文本，也可以把数据保存到数据库，或者保存为特定的jpg，mp4 等格式的文件。这就相当于我们在浏览网页时，下载了网页上的图片或者视频。 Request1.什么是Request？浏览器发送信息给该网址所在的服务器，这个过程就叫做HTTP Request。 2.Request中包含什么？ 请求方式：请求方式的主要类型是GET，OST两种，另外还有HEAD、PUT、DELETE等。GET 请求的请求参数会显示在URL链接的后面，比如我们打开百度，搜索“图片”，我们会看到请求的URL链接为https://www.baidu.com/s?wd=图片。而 POST 请求的请求参数会存放在Request内，并不会出现在 URL 链接的后面，比如我们登录知乎，输入用户名和密码，我们会看到浏览器开发者工具的Network页，Request请求有Form Data的键值对信息，那里就存放了我们的登录信息，有利于保护我们的账户信息安全； 请求 URL：URL 全称是统一资源定位符，也就是我们说的网址。比如一张图片，一个音乐文件，一个网页文档等都可以用唯一URL来确定，它包含的信息指出文件的位置以及浏览器应该怎么去处理它； 请求头(Request Headers)：请求头包含请求时的头部信息，如User-Agent（指定浏览器的请求头），Host，Cookies等信息； 请求体：请求体是请求是额外携带的数据，比如登录表单提交的登录信息数据。 Response1.什么是Response？服务器收到浏览器发送的信息后，能够根据浏览器发送信息的内容，做出相应的处理，然后把消息回传给浏览器，这个过程就叫做HTTP Response。 2.Response中包含什么？ 响应状态：有多种响应状态，比如200代表成功，301 跳转页面，404 表示找不到页面，502 表示服务器错误； 响应头(Response Headers)：比如内容类型，内容长度，服务器信息，设置Cookie等； 响应体：响应体最主要的部分，包含了请求资源的内容，比如网页 HTML 代码，图片二进制数据等。 简单演示1234567import requests # 导入requests库，需要安装# 模拟成浏览器访问的头headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'&#125;resp = requests.get('https://www.baidu.com',headers=headers)print(resp.text) # 打印出网页源代码print(resp.status_code) # 打印出状态码 运行成功后可以看到打印出来的 html 源代码和 200 状态码了。这就基本上实现了爬虫的Request和Response的过程。 能抓到什么样的数据？ 网页文本：如 HTML 文档，Ajax加载的Json格式文本等； 图片，视频等：获取到的是二进制文件，保存为图片或视频格式； 其他只要能请求到的，都能获取。 123456789import requestsheaders = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'&#125;resp = requests.get('http://www.baidu.com/img/baidu_jgylogo3.gif',headers=headers)print(resp.content) # 二进制文件使用content# 保存图片with open('logo.gif','wb') as f: f.write(resp.content) print('Ok') 成功运行就可以看到打印出来的图片的二进制数据，可保存成功后打印的 OK，这个时候我们打开文件夹就可以看到下载下来的图片了。这几行代码就简单的演示了爬虫保存文件的过程。 解析方式有哪些？ 直接处理，比如简单的页面文档，只要去除一些空格的数据； Json解析，处理Ajax加载的页面； 正则表达式； BeautifulSoup库； PyQuery； XPath。 看到这里，大家是不是已经对爬虫的基本工作原理有了清晰的认识了呢。当然，罗马并不是一天建成的，只要积累了足够多的经验，大家肯定能成为爬虫大神的。 接下来，我会给大家继续分享一些我在学习爬虫时的经验总结。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五月的事]]></title>
    <url>%2F2017%2F05%2F25%2F%E4%BA%94%E6%9C%88%E7%9A%84%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[之前无聊的时候写过很多博客，但是那时候这个网站还没有上线，大多博客都写在知乎上。但是一些牢骚话自然不会发在知乎的，都是保存了草稿，然后自己删掉，发布的都是 Python 的分享。毕竟申请的是 Python 专栏，专栏地址。其实这种感觉其实特别难受，好比打了一半的嗝又被咽回去了。 转眼五月就要结束了，时间就像一匹欢快的小马，你永远抓不住他的尾巴。人家说天上一日，地上一年，大概就是说快乐地时光总是过得很快。上一次有时间的概念是网站备案的时候，阿里云说要等十几天才能备案成功，我才感觉时间漫长。一天一天过去，备案的十几天似乎还很遥远。可渐渐的忘记这件事的时候，时间的小马就匆匆跑过来告诉你，十几天又过去了，备案通过了，马上到六月了。 回想这个月，我也想不到自己到底干了些什么。处了这个网站成功上线，在知乎写了两篇博客，其他都没什么印象。每天重复的3点式生活占去了大部分记忆。不过也很好，毕竟这样也让我少去很多不必要的打扰。这个月窝在宿舍倒是把廖雪峰 JavaScript 的教程看了一遍，也算不枉这个月的时光。写到这我突然明白，如果一直这样与美丽的姑娘们毫无交集，可能就永远单身了=。=。 这个月开始天气开始热起来，越来越有夏天的感觉了。天气一热，心就开始浮躁，连看一部完整电影的耐心都没有。希望今后能静下心来，非宁静无以致远吗。毕竟专业课还有很多作业要补，下个月在不全部上交估计这门课要挂，欠下的总是要还的，这话说的一点没错。 今天是五月二十五日，在过五天就是端午节，虽然不会有多少人可以看到，我还是要提前衷心的祝大家端午快乐。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Always</title>
  
  <subtitle>俞坤的博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wyukun.coding.me/"/>
  <updated>2017-09-15T15:27:32.075Z</updated>
  <id>https://wyukun.coding.me/</id>
  
  <author>
    <name>俞坤</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python爬虫(2):Requests的基本用法</title>
    <link href="https://wyukun.coding.me/2017/09/15/Python%E7%88%AC%E8%99%AB%E4%BA%8C%E4%B9%8BRequests%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/"/>
    <id>https://wyukun.coding.me/2017/09/15/Python爬虫二之Requests的基本用法/</id>
    <published>2017-09-15T15:08:56.000Z</published>
    <updated>2017-09-15T15:27:32.075Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>虽然Python有内置的<code>urllib</code>库，可以实现网络的请求，但是我并不推荐。因为<code>urllib</code>在很多时候使用起来不方便，比如加一个代理，处理<code>Cookie</code>时API都很繁琐，再比如发送一个POST请求也很麻烦。</p><p>而<code>Requests</code>就相当于<code>urllib</code>的升级版本，简化了<code>urllib</code>的使用方法。有了<code>Requests</code>，我们可以用几句代码实现代理的设置，<code>Cookie</code>的设置，非常方便。下面我就给大家整理了<code>Requests</code>库的使用方法和细节。详细可以参考<code>Requests</code><a href="http://docs.python-requests.org/zh_CN/latest/" target="_blank" rel="external">官方文档</a>。<br><a id="more"></a></p><h2 id="什么是Requests？"><a href="#什么是Requests？" class="headerlink" title="什么是Requests？"></a>什么是Requests？</h2><p><code>Requests</code>是<code>Python</code>语言编写，基于<code>urllib3</code>，采用<code>Apache2 Licensed</code>开源协议的HTTP库。</p><p>它比<code>urllib</code>更加方便，可以节约我们大量的工作，完全满足<code>HTTP</code>测试需求。是<code>Python</code>实现的简单易用的<code>HTTP</code>库。</p><p>安装也很简单：<code>pip install requests</code></p><h2 id="Requests的语法操作"><a href="#Requests的语法操作" class="headerlink" title="Requests的语法操作"></a>Requests的语法操作</h2><h3 id="1-实例引入"><a href="#1-实例引入" class="headerlink" title="1.实例引入"></a>1.实例引入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">response = requests.get(<span class="string">'http://www.baidu.com/'</span>)</div><div class="line">print(response.status_code)</div><div class="line">print(type(response.text))</div><div class="line">print(response.text)</div><div class="line">print(response.cookies)</div></pre></td></tr></table></figure><p>运行结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">200</div><div class="line">&lt;class &apos;str&apos;&gt;</div><div class="line"> </div><div class="line"># ...HTML网页源码..</div><div class="line">&lt;RequestsCookieJar[]&gt;</div></pre></td></tr></table></figure></p><p>可以看到，我们非常方便的就获取到了<code>Cookies</code>.</p><h3 id="2-各种请求方式"><a href="#2-各种请求方式" class="headerlink" title="2.各种请求方式"></a>2.各种请求方式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">requests.get(<span class="string">'http://httpbin.org/get'</span>) <span class="comment"># 发送get请求</span></div><div class="line">requests.post(<span class="string">'http://httpbin.org/post'</span>) <span class="comment"># 发送post请求，只要调用post方法，传入一个url参数</span></div><div class="line">requests.put(<span class="string">'http://httpbin.org/put'</span>)</div><div class="line">requests.delete(<span class="string">'http://httpbin.org/delete'</span>)</div></pre></td></tr></table></figure><p>官方文档里提供的这个网址足够我们测试这些请求方式了。</p><h2 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h2><h3 id="1-基本GET请求"><a href="#1-基本GET请求" class="headerlink" title="1.基本GET请求"></a>1.基本GET请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'http://httpbin.org/get'</span>)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>这个我们前面有使用过，也是最常用的方法。运行成功就可以看到网页的源码了。</p><h3 id="2-带参数的GET请求"><a href="#2-带参数的GET请求" class="headerlink" title="2.带参数的GET请求"></a>2.带参数的GET请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">data = &#123;</div><div class="line">    <span class="string">'name'</span> : <span class="string">'jack'</span>,</div><div class="line">    <span class="string">'age'</span> : <span class="number">20</span></div><div class="line">&#125;</div><div class="line">resp = requests.get(<span class="string">'http://httpbin.org/get'</span>, params=data)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>传入参数只需要我们把数据生成一个字典，然后调用<code>params</code>参数，赋值给他就可以，是不是很方便。</p><h3 id="3-解析json"><a href="#3-解析json" class="headerlink" title="3.解析json"></a>3.解析json</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> json</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'http://httpbin.org/get'</span>)</div><div class="line">print(resp.text)</div><div class="line">print(resp.json())</div><div class="line">print(json.loads(resp.text))</div><div class="line">print(type(resp.json()))</div></pre></td></tr></table></figure><p>运行结果：</p><p><img src="http://imgout.ph.126.net/55901018/QQCDBCC6AC20170529120909.jpg" alt="image"></p><p>可以看出<code>Requests</code>的<code>jaon</code>解析和<code>json</code>的<code>loads</code>方法解析出来的结果是完全一样的。所以<code>Requests</code>可以很方便的解析<code>json</code>数据。</p><h3 id="4-获取二进制数据"><a href="#4-获取二进制数据" class="headerlink" title="4.获取二进制数据"></a>4.获取二进制数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com/img/baidu_jgylogo3.gif'</span>)</div><div class="line">print(resp.content)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>运行成功我们可以看到<code>content</code>方法获取的图片页面源码是二进制数据，而<code>text</code>获取的则是字符串代码。显然获取图片这种二进制数据需要使用<code>content</code>方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> open(<span class="string">'logo.gif'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(resp.content)</div></pre></td></tr></table></figure><p>这样我们就保存了图片，我们可以在文件夹下看到这张图片。</p><h3 id="5-添加headers"><a href="#5-添加headers" class="headerlink" title="5.添加headers"></a>5.添加headers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com'</span>, headers=headers)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>有些网页如果我们直接去请求的话，他会查看请求的对象是不是浏览器，如果没有浏览器信息就会禁止我们爬虫的访问，这个时候我们就要给爬虫加一个<code>headers</code>，加一个浏览器的<code>user-agent</code>信息。这样我们就可以正常访问了。如果有的伙伴不知道怎么得到<code>User-Agent</code>，可以打开浏览器的审查元素，找到<code>network</code>，随便点击一个链接就可以看到<code>User-Agent</code>的信息了。</p><p><img src="http://imgout.ph.126.net/55898004/QQCDBCC6AC20170529131930.jpg" alt="image"></p><h3 id="6-基本POST请求"><a href="#6-基本POST请求" class="headerlink" title="6.基本POST请求"></a>6.基本POST请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">data = &#123;</div><div class="line">    <span class="string">'name'</span> : <span class="string">'jack'</span>,</div><div class="line">    <span class="string">'age'</span> : <span class="number">20</span></div><div class="line">&#125;</div><div class="line">resp = requests.post(<span class="string">'http://httpbin.org/post'</span>, data=data)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>一个<code>POST</code>必然是要有一个<code>Form Data</code>的表单提交的，我们只要把信息传给<code>data</code>参数就可以了。一个<code>POST</code>请求只需要调用<code>post</code>方法，是不是特别方便呢。如果不觉得方便的话，可以去参考<code>urllib</code>的使用方法。</p><h2 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h2><h3 id="1-response属性"><a href="#1-response属性" class="headerlink" title="1.response属性"></a>1.response属性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">response = requests.get(<span class="string">'http://www.baidu.com/'</span>)</div><div class="line">print(type(response.status_code)) <span class="comment"># 状态码</span></div><div class="line">print(type(response.text)) <span class="comment"># 网页源码</span></div><div class="line">print(type(response.headers)) <span class="comment"># 头部信息</span></div><div class="line">print(type(response.cookies)) <span class="comment"># Cookie</span></div><div class="line">print(type(response.url)) <span class="comment"># 请求的url</span></div><div class="line">print(type(response.history)) <span class="comment"># 访问的历史记录</span></div></pre></td></tr></table></figure><p>获取这些信息只需要简单的调用就可以实现了。</p><h3 id="2-状态码判断"><a href="#2-状态码判断" class="headerlink" title="2.状态码判断"></a>2.状态码判断</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">import</span> requests</div><div class="line"> </div><div class="line">&gt;&gt;&gt;response = requests.get(<span class="string">'http://www.baidu.com/'</span>)</div><div class="line">&gt;&gt;&gt;exit() <span class="keyword">if</span> <span class="keyword">not</span> resp.status_code == <span class="number">200</span> <span class="keyword">else</span> print(<span class="string">'Sucessful'</span>)</div><div class="line">Sucessful</div></pre></td></tr></table></figure><p>如果发送了一个错误请求(一个4XX客户端错误，或者5XX服务器错误响应)，我们可以通过 <code>Response.raise_for_status()</code> 来抛出异常：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;bad_r = requests.get(<span class="string">'http://httpbin.org/status/404'</span>)</div><div class="line">&gt;&gt;&gt;bad_r.status_code</div><div class="line"><span class="number">404</span></div><div class="line">&gt;&gt;&gt;bad_r.raise_for_status()</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"requests/models.py"</span>, line <span class="number">832</span>, <span class="keyword">in</span> raise_for_status</div><div class="line">    <span class="keyword">raise</span> http_error</div><div class="line">requests.exceptions.HTTPError: <span class="number">404</span> Client Error</div></pre></td></tr></table></figure><p>好了，这篇文章我们了解了<code>Requests</code>库的基本语法操作，相信大家对<code>Requests</code>库的请求和响应已经很清楚了，大家完全可以抓取一些网页了。</p><p>纸上得来终觉浅，绝知此事要躬行，大家加油！</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;虽然Python有内置的&lt;code&gt;urllib&lt;/code&gt;库，可以实现网络的请求，但是我并不推荐。因为&lt;code&gt;urllib&lt;/code&gt;在很多时候使用起来不方便，比如加一个代理，处理&lt;code&gt;Cookie&lt;/code&gt;时API都很繁琐，再比如发送一个POST请求也很麻烦。&lt;/p&gt;
&lt;p&gt;而&lt;code&gt;Requests&lt;/code&gt;就相当于&lt;code&gt;urllib&lt;/code&gt;的升级版本，简化了&lt;code&gt;urllib&lt;/code&gt;的使用方法。有了&lt;code&gt;Requests&lt;/code&gt;，我们可以用几句代码实现代理的设置，&lt;code&gt;Cookie&lt;/code&gt;的设置，非常方便。下面我就给大家整理了&lt;code&gt;Requests&lt;/code&gt;库的使用方法和细节。详细可以参考&lt;code&gt;Requests&lt;/code&gt;&lt;a href=&quot;http://docs.python-requests.org/zh_CN/latest/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方文档&lt;/a&gt;。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(1):基本原理</title>
    <link href="https://wyukun.coding.me/2017/09/15/Python%E7%88%AC%E8%99%AB%E4%B8%80%E4%B9%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/"/>
    <id>https://wyukun.coding.me/2017/09/15/Python爬虫一之基本原理/</id>
    <published>2017-09-15T14:44:37.000Z</published>
    <updated>2017-09-15T15:06:22.323Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>大家好，今天周末，希望大家周末愉快。</p><p>这篇文章我来梳理一下爬虫的基本原理。用过Python的伙伴都知道Python用来写爬虫是件很简单很爽的事情。但是有些伙伴不了解爬虫到底是什么，会纳闷为什么爬虫要设置什么请求头、请求体啊，到底要怎么样去发送这个请求啊。当然有的伙伴使用过很多爬虫的请求库，解析库，写过很多爬虫，但是可能他们的思路脉络也不是太清晰。那么，这篇文章就来帮大家理清爬虫原理的脉络。<br><a id="more"></a></p><h2 id="什么是爬虫？"><a href="#什么是爬虫？" class="headerlink" title="什么是爬虫？"></a>什么是爬虫？</h2><p><img src="http://imgout.ph.126.net/55876004/spider-1673428_960_720.jpg" alt="spider"></p><p>爬虫即网络爬虫，英文是<code>Web Spider</code>。翻译过来就是网络上爬行的蜘蛛，如果把互联网看作一张大网，那么爬虫就是在大网上爬来爬去的蜘蛛，碰到想要的食物，就把他抓取出来。</p><p>我们在浏览器中输入一个网址，敲击回车，看到网站的页面信息。这就是浏览器请求了网站的服务器，获取到网络资源。那么，爬虫也相当于模拟浏览器发送请求，获得到HTML代码。HTML代码里通常包含了标签和文字信息，我们就从中提取到我们想要的信息。</p><p>通常爬虫是从某个网站的某个页面开始，爬取这个页面的内容，找到网页中的其他链接地址，然后从这个地址爬到下一个页面，这样一直不停的爬下去，进去批量的抓取信息。那么，我们可以看出网络爬虫就是一个不停爬取网页抓取信息的程序。</p><h2 id="爬虫的基本流程"><a href="#爬虫的基本流程" class="headerlink" title="爬虫的基本流程"></a>爬虫的基本流程</h2><h3 id="1-发起请求："><a href="#1-发起请求：" class="headerlink" title="1.发起请求："></a>1.发起请求：</h3><p>通过HTTP库向目标站点发起请求，即发送一个<code>Request</code>，请求可以包含额外的<code>headers</code>等信息，然后等待服务器响应。这个请求的过程就像我们打开浏览器，在浏览器地址栏输入网址：<code>www.baidu.com</code>，然后点击回车。这个过程其实就相当于浏览器作为一个浏览的客户端，向服务器端发送了 一次请求。</p><h3 id="2-获取响应内容："><a href="#2-获取响应内容：" class="headerlink" title="2.获取响应内容："></a>2.获取响应内容：</h3><p>如果服务器能正常响应，我们会得到一个<code>Response</code>，<code>Response</code>的内容便是所要获取的内容，类型可能有HTML、Json字符串，二进制数据(图片，视频等）等类型。这个过程就是服务器接收客户端的请求，进过解析发送给浏览器的网页HTML文件。</p><h3 id="3-解析内容："><a href="#3-解析内容：" class="headerlink" title="3.解析内容："></a>3.解析内容：</h3><p>得到的内容可能是HTML，可以使用正则表达式，网页解析库进行解析。也可能是Json，可以直接转为Json对象解析。可能是二进制数据，可以做保存或者进一步处理。这一步相当于浏览器把服务器端的文件获取到本地，再进行解释并且展现出来。</p><h3 id="4-保存数据："><a href="#4-保存数据：" class="headerlink" title="4.保存数据："></a>4.保存数据：</h3><p>保存的方式可以是把数据存为文本，也可以把数据保存到数据库，或者保存为特定的jpg，mp4等格式的文件。这就相当于我们在浏览网页时，下载了网页上的图片或者视频。</p><h2 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h2><h3 id="1-什么是Request？"><a href="#1-什么是Request？" class="headerlink" title="1.什么是Request？"></a>1.什么是Request？</h3><p>浏览器发送信息给该网址所在的服务器，这个过程就叫做<code>HTTP Request</code>。</p><h3 id="2-Request中包含什么？"><a href="#2-Request中包含什么？" class="headerlink" title="2.Request中包含什么？"></a>2.Request中包含什么？</h3><ul><li>请求方式：请求方式的主要类型是<code>GET</code>，<code>OST</code>两种，另外还有<code>HEAD</code>、<code>PUT</code>、<code>DELETE</code>等。GET请求的请求参数会显示在URL链接的后面，比如我们打开百度，搜索“图片”，我们会看到请求的URL链接为<code>https://www.baidu.com/s?wd=图片</code>。而POST请求的请求参数会存放在Request内，并不会出现在URL链接的后面，比如我们登录知乎，输入用户名和密码，我们会看到浏览器开发者工具的<code>Network</code>页，Request请求有<code>Form Data</code>的键值对信息，那里就存放了我们的登录信息，有利于保护我们的账户信息安全；</li><li>请求URL：URL全称是统一资源定位符，也就是我们说的网址。比如一张图片，一个音乐文件，一个网页文档等都可以用唯一URL来确定，它包含的信息指出文件的位置以及浏览器应该怎么去处理它；</li><li>请求头<code>(Request Headers)</code>：请求头包含请求时的头部信息，如<code>User-Agent</code>（指定浏览器的请求头），<code>Host</code>，<code>Cookies</code>等信息；</li><li>请求体：请求体是请求是额外携带的数据，比如登录表单提交的登录信息数据。</li></ul><h2 id="Response"><a href="#Response" class="headerlink" title="Response"></a>Response</h2><h3 id="1-什么是Response？"><a href="#1-什么是Response？" class="headerlink" title="1.什么是Response？"></a>1.什么是Response？</h3><p>服务器收到浏览器发送的信息后，能够根据浏览器发送信息的内容，做出相应的处理，然后把消息回传给浏览器，这个过程就叫做<code>HTTP Response</code>。</p><h3 id="2-Response中包含什么？"><a href="#2-Response中包含什么？" class="headerlink" title="2.Response中包含什么？"></a>2.Response中包含什么？</h3><ul><li>响应状态：有多种响应状态，比如200代表成功，301跳转页面，404表示找不到页面，502表示服务器错误；</li><li>响应头<code>(Response Headers)</code>：比如内容类型，内容长度，服务器信息，设置<code>Cookie</code>等；</li><li>响应体：响应体最主要的部分，包含了请求资源的内容，比如网页HTML代码，图片二进制数据等。</li></ul><h2 id="简单演示"><a href="#简单演示" class="headerlink" title="简单演示"></a>简单演示</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests <span class="comment"># 导入requests库，需要安装</span></div><div class="line"></div><div class="line"><span class="comment"># 模拟成浏览器访问的头</span></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(<span class="string">'https://www.baidu.com'</span>,headers=headers)</div><div class="line">print(resp.text) <span class="comment"># 打印出网页源代码</span></div><div class="line">print(resp.status_code) <span class="comment"># 打印出状态码</span></div></pre></td></tr></table></figure><p>运行成功后可以看到打印出来的html源代码和200状态码了。这就基本上实现了爬虫的<code>Request</code>和<code>Response</code>的过程。</p><h2 id="能抓到什么样的数据？"><a href="#能抓到什么样的数据？" class="headerlink" title="能抓到什么样的数据？"></a>能抓到什么样的数据？</h2><ul><li>网页文本：如HTML文档，Ajax加载的Json格式文本等；</li><li>图片，视频等：获取到的是二进制文件，保存为图片或视频格式；</li><li>其他只要能请求到的，都能获取。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com/img/baidu_jgylogo3.gif'</span>,headers=headers)</div><div class="line">print(resp.content) <span class="comment"># 二进制文件使用content</span></div><div class="line"><span class="comment"># 保存图片</span></div><div class="line"><span class="keyword">with</span> open(<span class="string">'logo.gif'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(resp.content)</div><div class="line">    print(<span class="string">'Ok'</span>)</div></pre></td></tr></table></figure><p>成功运行就可以看到打印出来的图片的二进制数据，可保存成功后打印的OK，这个时候我们打开文件夹就可以看到下载下来的图片了。这几行代码就简单的演示了爬虫保存文件的过程。</p><h2 id="解析方式有哪些？"><a href="#解析方式有哪些？" class="headerlink" title="解析方式有哪些？"></a>解析方式有哪些？</h2><ul><li>直接处理，比如简单的页面文档，只要去除一些空格的数据；</li><li>Json解析，处理Ajax加载的页面；</li><li>正则表达式；</li><li>BeautifulSoup库；</li><li>PyQuery；</li><li>XPath。</li></ul><p>看到这里，大家是不是已经对爬虫的基本工作原理有了清晰的认识了呢。当然，罗马并不是一天建成的，只要积累了足够多的经验，大家肯定能成为爬虫大神的。</p><p>接下来，我会给大家继续分享一些我在学习爬虫时的经验总结。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;大家好，今天周末，希望大家周末愉快。&lt;/p&gt;
&lt;p&gt;这篇文章我来梳理一下爬虫的基本原理。用过Python的伙伴都知道Python用来写爬虫是件很简单很爽的事情。但是有些伙伴不了解爬虫到底是什么，会纳闷为什么爬虫要设置什么请求头、请求体啊，到底要怎么样去发送这个请求啊。当然有的伙伴使用过很多爬虫的请求库，解析库，写过很多爬虫，但是可能他们的思路脉络也不是太清晰。那么，这篇文章就来帮大家理清爬虫原理的脉络。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>五月的事</title>
    <link href="https://wyukun.coding.me/2017/09/15/%E4%BA%94%E6%9C%88%E7%9A%84%E4%BA%8B/"/>
    <id>https://wyukun.coding.me/2017/09/15/五月的事/</id>
    <published>2017-09-15T11:43:14.000Z</published>
    <updated>2017-09-15T11:59:01.507Z</updated>
    
    <content type="html"><![CDATA[<p>之前无聊的时候写过很多博客，但是那时候这个网站还没有上线，大多博客都写在知乎上。但是一些牢骚话自然不会发在知乎的，都是保存了草稿，然后自己删掉，发布的都是Python的分享。毕竟申请的是Python专栏，<a href="https://zhuanlan.zhihu.com/yukun" target="_blank" rel="external">专栏地址</a>。其实这种感觉其实特别难受，好比打了一半的嗝又被咽回去了。<br><a id="more"></a></p><p><img src="http://imgout.ph.126.net/55814031/55814031.jpg" alt="image"></p><iframe src="//music.163.com/outchain/player?type=2&amp;id=31445772&amp;auto=0&amp;height=66" width="330" height="86" frameborder="no" marginwidth="0" marginheight="0"></iframe><p>转眼五月就要结束了，时间就像一匹欢快的小马，你永远抓不住他的尾巴。人家说天上一日，地上一年，大概就是说快乐地时光总是过得很快。上一次有时间的概念是网站备案的时候，阿里云说要等十几天才能备案成功，我才感觉时间漫长。一天一天过去，备案的十几天似乎还很遥远。可渐渐的忘记这件事的时候，时间的小马就匆匆跑过来告诉你，十几天又过去了，备案通过了，马上到六月了。</p><p>回想这个月，我也想不到自己到底干了些什么。处了这个网站成功上线，在知乎写了两篇博客，其他都没什么印象。每天重复的3点式生活占去了大部分记忆。不过也很好，毕竟这样也让我少去很多不必要的打扰。这个月窝在宿舍倒是把廖雪峰JavaScript的教程看了一遍，也算不枉这个月的时光。写到这我突然明白，如果一直这样与美丽的姑娘们毫无交集，可能就永远单身了=。=。</p><p>这个月开始天气开始热起来，越来越有夏天的感觉了。天气一热，心就开始浮躁，连看一部完整电影的耐心都没有。希望今后能静下心来，非宁静无以致远吗。毕竟专业课还有很多作业要补，下个月在不全部上交估计这门课要挂，欠下的总是要还的，这话说的一点没错。</p><p>今天是五月二十五日，在过五天就是端午节，虽然不会有多少人可以看到，我还是要提前衷心的祝大家端午快乐。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前无聊的时候写过很多博客，但是那时候这个网站还没有上线，大多博客都写在知乎上。但是一些牢骚话自然不会发在知乎的，都是保存了草稿，然后自己删掉，发布的都是Python的分享。毕竟申请的是Python专栏，&lt;a href=&quot;https://zhuanlan.zhihu.com/yukun&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;专栏地址&lt;/a&gt;。其实这种感觉其实特别难受，好比打了一半的嗝又被咽回去了。&lt;br&gt;
    
    </summary>
    
      <category term="生活" scheme="https://wyukun.coding.me/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="随笔" scheme="https://wyukun.coding.me/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>意外</title>
  
  <subtitle>俞坤的博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wyukun.coding.me/"/>
  <updated>2017-09-19T10:25:35.152Z</updated>
  <id>https://wyukun.coding.me/</id>
  
  <author>
    <name>俞坤</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>八月</title>
    <link href="https://wyukun.coding.me/2017/08/27/%E5%85%AB%E6%9C%88/"/>
    <id>https://wyukun.coding.me/2017/08/27/八月/</id>
    <published>2017-08-27T09:42:31.000Z</published>
    <updated>2017-09-19T10:25:35.152Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>今天是八月二十七号，还有四天这个月就结束了。当然八月的离去，就是九月的到来。</p><p>这个月发生了很多事，吴京的《战狼2》火了，炉石传说出冰冠堡垒了，著名的电影公司暴雪又出新CG了，郭敬明也开始蠢蠢欲动了…<br><a id="more"></a></p><p><img src="http://imgout.ph.126.net/57363048/pexels-photo-416208.jpg" alt="image"></p><p>对于我来说，这个月我想通了一件事，决定考研。虽然我知道可能太迟了，但是我还是要去做，如果失败了就接着来。所以博客会在后面很长时间内，更新很慢。其实这也没什么，毕竟过来浏览的也没几个人。看了看考研的科目，和需要的知识范围，心里还算比较清楚自己的状态，因为基本上所有的科目都需要重新去学。这个时候一想到大学几年的荒废就感到头痛，这世界还是很公平的，你快乐过了，就该痛苦了。</p><p>当然这种事是不能以事后人的身份去对待的，就比如之前一个高中同学发给我一张高中时的照片。</p><p><img src="http://imgout.ph.126.net/57350046/me.jpg" alt="me"></p><p>照片中的我，嘴角微微上扬，闭着双目，很是安详。课座上放着还在背的英语 3500 词，快要高考了，气愤很是压抑，大家都在认真学习。同学接着发了一条信息说，如果你没有在课桌前睡着，是不是现在是在清华了。听后，我颇感有理，心中顿时想到如果十年前，在往前的那个夏天我能够认真呆在家里写暑假作业，是不是现在我已经迎娶白富美，出任 CEO，走上人生巅峰了，想到这丝丝心痛。</p><p>其实，这就是典型的事后诸葛亮。生活没有如果当初，活在过去只会止步不前。一代宗师中章子怡说，宁在一思进，莫在一思停。与其悔恨当初不如做好现在，著名的文人鲁迅还是哪个谁说，种一颗树最好的时候是十年前，其次是现在。</p><p>下个月就大四了，希望在大四能找到一个实习工作。</p><p>明天就是虐狗哦不对，明天就是七夕节了，当然对于我来说是没有什么意义的。其实所有的节日对于我来说都不会有什么意义，毕竟所有的节日，其实都是购物节和秀恩爱节。</p><p>最后给大家分享一首歌。</p><div id="aplayer0" class="aplayer" style="margin-bottom: 20px;"></div><script>new APlayer({element: document.getElementById("aplayer0"),narrow: false,autoplay: false,showlrc: 0,music: {title: "逆鳞",author: "周杰伦",url: "http://opxib6gmc.bkt.clouddn.com/nilin.mp3",pic: "http://opxib6gmc.bkt.clouddn.com/nilin.jpg",}});</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天是八月二十七号，还有四天这个月就结束了。当然八月的离去，就是九月的到来。&lt;/p&gt;
&lt;p&gt;这个月发生了很多事，吴京的《战狼2》火了，炉石传说出冰冠堡垒了，著名的电影公司暴雪又出新CG了，郭敬明也开始蠢蠢欲动了…&lt;br&gt;
    
    </summary>
    
      <category term="生活" scheme="https://wyukun.coding.me/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="随笔" scheme="https://wyukun.coding.me/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>我看小说的时候我在看什么</title>
    <link href="https://wyukun.coding.me/2017/08/02/%E6%88%91%E7%9C%8B%E5%B0%8F%E8%AF%B4%E7%9A%84%E6%97%B6%E5%80%99%E6%88%91%E5%9C%A8%E7%9C%8B%E4%BB%80%E4%B9%88/"/>
    <id>https://wyukun.coding.me/2017/08/02/我看小说的时候我在看什么/</id>
    <published>2017-08-02T09:25:21.000Z</published>
    <updated>2017-09-19T09:40:39.959Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>最近无聊的时候逛知乎收到几个伙伴的私信，是我之前在专栏写的一个小说网站的代码不能用了。由于我之前说过不在更新那个项目了，所以我也没想去改，我知道大部分原因是爬虫抓取的小说网站更新了，爬虫代码用不了了。然后我看了一下之前的代码，我靠，怎么跟屎一样。这样的代码真的对不起认真阅读的伙伴，于是我将<code>github</code>上的代码全部删了，就重新实现了一下。<br><a id="more"></a></p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>实现的大部分方式都写在知乎上，如果大家感兴趣不妨看一下：<a href="https://zhuanlan.zhihu.com/p/28216335" target="_blank" rel="external">zhihu</a></p><p>实现后的效果如下：</p><p>首页：</p><p><img src="http://imgout.ph.126.net/56952195/mn.jpg" alt="index"></p><p>搜索结果页：</p><p><img src="http://imgout.ph.126.net/56955190/mn2.jpg" alt="search"></p><p>章节页：</p><p><img src="http://imgout.ph.126.net/56952197/mn3.jpg" alt="chapter"></p><p>文章页：</p><p><img src="http://imgout.ph.126.net/56955191/mn4.jpg" alt="article"></p><p>主要使用<code>xpath</code>加<code>requests</code>抓取顶点网的小说数据，利用<code>flask</code>写<code>web</code>端，使用<code>bootstrap</code>做前端页面。</p><h2 id="项目地址："><a href="#项目地址：" class="headerlink" title="项目地址："></a>项目地址：</h2><p><div class="github-card" data-github="Blackyukun/dingdian" data-width="400" data-height="" data-theme="medium"></div></p><script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script><p>小说网站：<a href="http://dingdian.herokuapp.com/" target="_blank" rel="external">MyNovels</a></p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近无聊的时候逛知乎收到几个伙伴的私信，是我之前在专栏写的一个小说网站的代码不能用了。由于我之前说过不在更新那个项目了，所以我也没想去改，我知道大部分原因是爬虫抓取的小说网站更新了，爬虫代码用不了了。然后我看了一下之前的代码，我靠，怎么跟屎一样。这样的代码真的对不起认真阅读的伙伴，于是我将&lt;code&gt;github&lt;/code&gt;上的代码全部删了，就重新实现了一下。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="Flask" scheme="https://wyukun.coding.me/tags/Flask/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(13):Scrapy实战抓取网易云音乐</title>
    <link href="https://wyukun.coding.me/2017/07/29/Python%E7%88%AC%E8%99%AB%E5%8D%81%E4%B8%89%E4%B9%8BScrapy%E5%AE%9E%E6%88%98%E6%8A%93%E5%8F%96%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90/"/>
    <id>https://wyukun.coding.me/2017/07/29/Python爬虫十三之Scrapy实战抓取网易云音乐/</id>
    <published>2017-07-29T07:56:38.000Z</published>
    <updated>2017-09-19T08:54:33.711Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>前两篇文章我们了解了Scrapy的理论知识，那么我们不能做纸上谈兵的赵括。实践才是检验真理的唯一标准。本篇文章我们来抓取网易云音乐的所有音乐及音乐的热评。<br><a id="more"></a></p><h2 id="分析站点"><a href="#分析站点" class="headerlink" title="分析站点"></a>分析站点</h2><p>我们打开浏览器，访问网易云音乐的网页端。如果我们想要抓取到所有的音乐，就得有一个切入口，能够获得到所有的音乐数据。</p><p>那么通过观察页面的导航，我们只能通过获取全部的歌手。但是由于歌手详情页并没有全部音乐这个链接，我们只能获取全部的专辑。在通过全部的专辑获得全部的音乐。</p><h2 id="爬虫流程"><a href="#爬虫流程" class="headerlink" title="爬虫流程"></a>爬虫流程</h2><p>以歌手页为索引页，抓取所有的歌手；<br>通过所有的歌手抓取全部专辑；<br>通过全部专辑抓取所有的音乐；<br>分析所有音乐的Ajax，获得所有热评；<br>将音乐名，歌手，专辑，热评，热评作者，热评赞数保存数据库。</p><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><h3 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h3><p><code>scrapy startproject 163music</code></p><h3 id="创建爬虫文件（可以通过命令行创建）："><a href="#创建爬虫文件（可以通过命令行创建）：" class="headerlink" title="创建爬虫文件（可以通过命令行创建）："></a>创建爬虫文件（可以通过命令行创建）：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># spiders/spider.py</span></div><div class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Spider</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MusicSpider</span><span class="params">(Spider)</span>:</span></div><div class="line">    name = <span class="string">"music"</span></div><div class="line">    allowed_domains = [<span class="string">"163.com"</span>]</div><div class="line">    base_url = <span class="string">'https://music.163.com'</span></div></pre></td></tr></table></figure><h3 id="确定数据名称"><a href="#确定数据名称" class="headerlink" title="确定数据名称"></a>确定数据名称</h3><p>我们先将要保存下来得到数据写到item文件中，虽然这一步不是必须先写，但是我们按照流程来不会错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#items.py</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MusicItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    <span class="comment"># 我们保存歌曲的id</span></div><div class="line">    id = scrapy.Field()</div><div class="line">    artist = scrapy.Field()</div><div class="line">    album = scrapy.Field()</div><div class="line">    music = scrapy.Field()</div><div class="line">    comments = scrapy.Field()</div></pre></td></tr></table></figure><h3 id="分析索引页"><a href="#分析索引页" class="headerlink" title="分析索引页"></a>分析索引页</h3><p>我们的索引页为歌手页，地址：<code>https://music.163.com/#/discover/artist/cat?id=1001&amp;amp;initial=65</code></p><p><img src="http://imgout.ph.126.net/56902217/music1.jpg" alt="image"></p><p>通过图片结合我们对索引页的观察，我们可以看到左侧比如华语男歌手，欧美男歌手是分类，而歌手下的ABCDE也是一个按姓名的分类。</p><p>通过观察链接可以发现，<code>id</code>就是左侧分类的值，<code>initial</code>是 ABCDE 链接的值。</p><p>我们可以发现 ABCDE 每一个链接是从 65 开始，一直到 90，再加上’其他’链接为 0。这样的规则我们是可以用代码很简单实现的。而左侧的歌手分类的数字相对是不好用代码实现他的规则的。索性他的数目不多，我们一个一个写出来保存集合就可以了。我们将这两个参数写到爬虫类中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MusicSpider</span><span class="params">(Spider)</span>:</span></div><div class="line">    name = <span class="string">"music"</span></div><div class="line">    allowed_domains = [<span class="string">"163.com"</span>]</div><div class="line">    base_url = <span class="string">'https://music.163.com'</span></div><div class="line">    ids = [<span class="string">'1001'</span>,<span class="string">'1002'</span>,<span class="string">'1003'</span>,<span class="string">'2001'</span>,<span class="string">'2002'</span>,<span class="string">'2003'</span>,<span class="string">'6001'</span>,<span class="string">'6002'</span>,<span class="string">'6003'</span>,<span class="string">'7001'</span>,<span class="string">'7002'</span>,<span class="string">'7003'</span>,<span class="string">'4001'</span>,<span class="string">'4002'</span>,<span class="string">'4003'</span>]</div><div class="line">    initials = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">65</span>, <span class="number">91</span>)]+[<span class="number">0</span>]</div></pre></td></tr></table></figure><h3 id="起始url"><a href="#起始url" class="headerlink" title="起始url"></a>起始url</h3><p>很显然歌手页有不同的分类，所有起始页不可能是单独的一个<code>url</code>，所以我们要重写<code>start_requests</code>。也就是构建所有的歌手分类页。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">for</span> id <span class="keyword">in</span> self.ids:</div><div class="line"><span class="keyword">for</span> initial <span class="keyword">in</span> self.initials:</div><div class="line">    url = <span class="string">'&#123;url&#125;/discover/artist/cat?id=&#123;id&#125;&amp;amp;initial=&#123;initial&#125;'</span>.format(url=self.base_url,id=id,initial=initial)</div><div class="line"><span class="keyword">yield</span> Request(url, callback=self.parse_index)</div></pre></td></tr></table></figure><p>这一步实现起来逻辑还是很清晰的，循环每一个<code>id</code>，在循环每一个<code>initial</code>，将他们通过<code>.format</code>方法组成<code>url</code>。然后使用<code>yield</code>语法糖，将<code>url</code>回调给索引页解析函数。相信大家在前两篇理论的梳理下，对于这步操作没有什么问题。</p><p>那么我们在<code>parse_index()</code>函数中打印一下<code>Response</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">(self, response)</span>:</span></div><div class="line">    print(response.text)</div></pre></td></tr></table></figure><p>控制台运行爬虫：<code>scrapy crawl music</code></p><p>由于<code>scrapy</code>不支持<code>lde</code>运行，所以如果我们非要想在比如<code>pycharm</code>中运行的话，我们需要编写一个运行程序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 163music/entrypoint.py</span></div><div class="line"><span class="comment"># 注意这个文件在项目的根目录，也就是scrapy.cfg文件所在</span></div><div class="line"><span class="comment"># 这里的music就是爬虫的名字</span></div><div class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</div><div class="line">execute([<span class="string">'scrapy'</span>, <span class="string">'crawl'</span>, <span class="string">'music'</span>])</div></pre></td></tr></table></figure><p>现在我们在<code>pycharm</code>做运行这个文件就相当于运行爬虫了。</p><p>运行成功，但是我们好像并没有获得到我们想要的数据。这是怎么回事呢？<br>大家如果回忆使用<code>Requests</code>库请求的时候，我们在请求中有时候会添加一些请求头，那么<code>scrapy</code>中我们要在哪里添加呢。</p><p>答案很简单，就是在<code>settings.py</code>文件中。</p><h3 id="添加请求头设置"><a href="#添加请求头设置" class="headerlink" title="添加请求头设置"></a>添加请求头设置</h3><p>我们需要在<code>settings</code>文件中先取消掉<code>DEFAULT_REQUEST_HEADERS</code>的注释，因为<code>scrapy</code>默认我们不需要请求头。我们在里面添加网易云的头部请求，就是我们开发者工具里的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">DEFAULT_REQUEST_HEADERS = &#123;</div><div class="line">    <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'</span>,</div><div class="line">    <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate, sdch'</span>,</div><div class="line">    <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.8,en;q=0.6'</span>,</div><div class="line">    <span class="string">'Cache-Control'</span>: <span class="string">'no-cache'</span>,</div><div class="line">    <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</div><div class="line">    <span class="string">'Cookie'</span>:<span class="string">'_ntes_nuid=5e2135ea19041c08d61bddbb9009de63; _ntes_nnid=a387121ca9ed891dca82492f6c088c57,1483420952257; __utma=187553192.690483437.1489583101.1489583101.1489583101.1; __utmz=187553192.1489583101.1.1.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided); __oc_uuid=ff821060-097f-11e7-8c2a-73421a9a1bc4; mail_psc_fingerprint=032ad52396a72877e07f21386dee35a2; NTES_CMT_USER_INFO=106964635%7C%E6%9C%89%E6%80%81%E5%BA%A6%E7%BD%91%E5%8F%8B06o2qr%7Chttps%3A%2F%2Fsimg.ws.126.net%2Fe%2Fimg5.cache.netease.com%2Ftie%2Fimages%2Fyun%2Fphoto_default_62.png.39x39.100.jpg%7Cfalse%7CbTE1MTUyMzQ3Mjc3QDE2My5jb20%3D; usertrack=c+5+hlkgTIMgjwa+EDUGAg==; _ga=GA1.2.690483437.1489583101; Province=025; City=05278; NTES_PASSPORT=aXWcpL4bYTLQnXY4eO888VlwXt.v922HPG1pBkj.vkeDwsISwc4gjpib7gtylUsoCy.yIGuJPZg7Uq2lTWqIo3A5ddE7eIf5DP_mjdHrg7ky2KFIZHP60ge8g; P_INFO=m15152347277@163.com|1500267468|1|blog|11&amp;amp;10|jis&amp;amp;1499527300&amp;amp;mail163#jis&amp;amp;320800#10#0#0|151277&amp;amp;1|study&amp;amp;blog&amp;amp;photo|15152347277@163.com; UM_distinctid=15d4ee58fc9483-032aae6568b355-333f5902-100200-15d4ee58fca912; NTES_SESS=35juNvuVAClEtPfwjy5rP5GVXVpRFMmwg2ItfudhfLmyGTk4G2l_fIFHi_xsOJTWQrUJvW3JwsMFyepEs0SR6z1_QnKjbQFaesBY9ABy0TVFP_KIiXNgb89wCGe.3_hmKR90f2ybdvNPWqPX8_YesVlIQrWdw5Nfg6KF0EcoVXO3DgV09cJHAeiE_; S_INFO=1500623480|1|0&amp;amp;80##|m15152347277; ANTICSRF=dd45f2a4489d303de869d820a0dadf05; playerid=64643457; JSESSIONID-WYYY=oR0Q0Ce%2Bhldid%2FFtfsiobsg%5Cecyra1qnHBuFFPNBUW%2BbZ3%5C2uq5%2Fqz4VrhRll0%5CaVCfY%2Fg0%2BC47vS%5Cv6rsyuD76tlqWN%2BUryVxph9fZeCmVIDtu5so7vdcdp%2B92hI3A0R5Zm%2Besa5l3ND%5Cz59WOYTY%2FCUjG%2B8gFSGVyzTpMquPQIxyIM%3A1500647790286; _iuqxldmzr_=32; MUSIC_U=f5333454d16d0f0ca5e59b3a82afaabcb107f5e73a4504bae87278f38158d65dbef309e3badc0bfac257abd5a88c5d62dc7e2cf554b1b3fc233a987fb3c42671e386323209b86ec1bf122d59fa1ed6a2; __remember_me=true; __csrf=5cd5b19efc6ea479e298487216162acf; __utma=94650624.776578804.1489210725.1500604214.1500644866.50; __utmb=94650624.28.10.1500644866; __utmc=94650624; __utmz=94650624.1499960824.48.42.utmcsr=yukunweb.com|utmccn=(referral)|utmcmd=referral|utmcct=/412.html'</span>,</div><div class="line">    <span class="string">'DNT'</span>: <span class="string">'1'</span>,</div><div class="line">    <span class="string">'Host'</span>: <span class="string">'music.163.com'</span>,</div><div class="line">    <span class="string">'Pragma'</span>: <span class="string">'no-cache'</span>,</div><div class="line">    <span class="string">'Referer'</span>: <span class="string">'http://music.163.com/'</span>,</div><div class="line">    <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</div><div class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>注意，网易云音乐的歌曲提取是要我们登录才可以获得数据的，我们直接加上登录后的<code>cookies</code>就可以了。</p><p>现在运行爬虫，如果运行成功，那么大家应该就能看到打印出来的数据了。这说明我们的程序是正确的。</p><h3 id="编写起始页解析函数"><a href="#编写起始页解析函数" class="headerlink" title="编写起始页解析函数"></a>编写起始页解析函数</h3><p>这一步就要使用我们的选择器提取信息了，我们打开开发者工具，我们需要的就是歌手<code>a</code>标签中的<code>href</code>信息。对于还不会使用<code>xpath</code>和<code>css</code>选择器的可以使用<code>Chrome</code>开发者工具，右击该标签，如下图的操作：</p><p><img src="http://imgout.ph.126.net/56888108/music2.jpg" alt="image"></p><p>直接上解析起始页代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 获得所有歌手的url</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">(self, response)</span>:</span></div><div class="line">    artists = response.xpath(<span class="string">'//*[@id="m-artist-box"]/li/div/a/@href'</span>).extract()</div><div class="line">    <span class="keyword">for</span> artist <span class="keyword">in</span> artists:</div><div class="line">artist_url = self.base_url + <span class="string">'/artist'</span> + <span class="string">'/album?'</span> + artist[<span class="number">8</span>:]</div><div class="line"><span class="keyword">yield</span> Request(artist_url, callback=self.parse_artist)</div></pre></td></tr></table></figure><p>歌手专辑详情页<code>url</code>实例如：<code>https://music.163.com/#/artist/album?id=6452</code></p><p>我们解析得到<code>href</code>值后，在将他组合成完整的歌手专辑详情页<code>url</code>。然后回调给下一个解析函数。</p><h3 id="提取所有专辑url"><a href="#提取所有专辑url" class="headerlink" title="提取所有专辑url"></a>提取所有专辑url</h3><p>这一步和上一步一样，由于这一步也没什么难点，不过多赘述。上代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 获得所有歌手专辑的url</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_artist</span><span class="params">(self, response)</span>:</span></div><div class="line">    albums = response.xpath(<span class="string">'//*[@id="m-song-module"]/li/div/a[@class="msk"]/@href'</span>).extract()</div><div class="line">    <span class="keyword">for</span> album <span class="keyword">in</span> albums:</div><div class="line">album_url = self.base_url + album</div><div class="line"><span class="keyword">yield</span> Request(album_url, callback=self.parse_album)</div></pre></td></tr></table></figure><h3 id="提取所有歌曲"><a href="#提取所有歌曲" class="headerlink" title="提取所有歌曲"></a>提取所有歌曲</h3><p>这一步就有一点不同了，因为如果我们提取了音乐<code>url</code>，那么我们需要的音乐<code>id</code>就在<code>url</code>中。如果我们直接将<code>URL</code>回调给解析音乐页的函数后，我们在后面是获取不到这个<code>id</code>的。大家可以自己观察页面，确定这一步。</p><p>所以我们不仅要把<code>url</code>回调给下一个解析函数，还要把音乐<code>id</code>传给下一个函数。那么大家应该会有疑问，为什么不把<code>id</code>直接保存到<code>item</code>呢。</p><p>这是因为我们需要的数据结构会是这样：</p><p><code>{&#39;id&#39;:123456,&#39;music&#39;:&#39;晴天&#39;,&#39;artist&#39;:&#39;周杰伦&#39;,&#39;album&#39;:&#39;叶美惠&#39;,&#39;comments&#39;:[{&#39;comment_author&#39;:&#39;小明&#39;,&#39;comment_content&#39;:&#39;我爱你&#39;,&#39;comment_like&#39;:&#39;123456&#39;},{...},{}...]}</code></p><p>如果我们现在保存了音乐<code>id</code>，那么后面的信息能否对应我们也不确定。那么怎样才能将数据传给下一个函数呢？</p><p><code>scrapy</code>给我提供了<code>meta</code>参数用来保存我们的数据传给函数，我们来看代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 获得所有专辑音乐的url</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_album</span><span class="params">(self, response)</span>:</span></div><div class="line">    musics = response.xpath(<span class="string">'//ul[@class="f-hide"]/li/a/@href'</span>).extract()</div><div class="line">    <span class="keyword">for</span> music <span class="keyword">in</span> musics:</div><div class="line">music_id = music[<span class="number">9</span>:]</div><div class="line">music_url = self.base_url + music</div><div class="line"></div><div class="line"><span class="keyword">yield</span> Request(music_url, meta=&#123;<span class="string">'id'</span>: music_id&#125;, callback=self.parse_music)</div></pre></td></tr></table></figure><p>像这样我们把组合的<code>URL</code>传给解析函数，也将音乐<code>id</code>传给下一个函数。</p><h3 id="提取音乐信息，分析评论Ajax"><a href="#提取音乐信息，分析评论Ajax" class="headerlink" title="提取音乐信息，分析评论Ajax"></a>提取音乐信息，分析评论Ajax</h3><p>对于提取页面的音乐信息，使用选择器提取就可以了，难的是评论区并不在我们获取的源码中。如果大家有疑惑，可以打印一些音乐详情页的源码。那么评论的信息究竟在哪呢，这是相信大家心里都开始怀疑这是不是<code>Ajax</code>加载的呢。</p><p>为了验证这个疑惑，我们点击评论区的翻页，可以看到到了第二页浏览器的<code>url</code>并没有变化。这个时候基本上可以知道这是<code>ajax</code>加载的页面了。</p><p>我们之前有一篇说过<code>Ajax</code>请求的处理方法，我们这里不多余赘述。打开<code>Chrome</code>开发者工具，点开<code>Network</code>标签的<code>XHR</code>刷新页面，这时候会有几个请求出来。我们一个一个点开看他们的响应内容，发现<code>R_SO_4_186016?csrf_token=</code>请求中包含了评论的信息。数一下热评数在对比页面中的热评信息，完全一致。我们看下面的图片：</p><p><img src="http://imgout.ph.126.net/56890168/music3.jpg" alt="image"></p><p><img src="http://imgout.ph.126.net/56897110/music4.jpg" alt="image"></p><p>通过上面一张图片，红框里框出的<code>Form Data</code>数据，没错，这是一个<code>Post</code>请求信息。接着我们就要将他们构造成字典通过<code>post</code>请求。我们在看图中的<code>referer</code>的url，没错url后面的id就是歌曲的id。上一个函数我们将歌曲id也传过来是不是很方便这一步的处理呢。</p><p>我们需要在之前的请求头中加入每个音乐请求的<code>referer</code>参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">DEFAULT_REQUEST_HEADERS[<span class="string">'Referer'</span>] = self.base_url + <span class="string">'/playlist?id='</span> + str(music_id)</div></pre></td></tr></table></figure><p>将<code>Form Data</code>构造字典大家都没问题，构造<code>Ajax</code>请求<code>url</code>就是<code>R_SO_4_</code>后面接上音乐的<code>id</code>。也没问题，那么在<code>scrapy</code>中怎么使用<code>Post</code>请求呢。</p><p>答案就是scrapy的<code>FormRequest</code>方法，我们需要导入他，然后用法和<code>Request</code>一样，我们还需要将这个函数提取的所有音乐信息传给下一个提取热评的函数，然后将所有数据一起传给<code>item</code>。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 获得音乐信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_music</span><span class="params">(self, response)</span>:</span></div><div class="line">    music_id = response.meta[<span class="string">'id'</span>]</div><div class="line">    music = response.xpath(<span class="string">'//div[@class="tit"]/em[@class="f-ff2"]/text()'</span>).extract_first()</div><div class="line">    artist = response.xpath(<span class="string">'//div[@class="cnt"]/p[1]/span/a/text()'</span>).extract_first()</div><div class="line">    album = response.xpath(<span class="string">'//div[@class="cnt"]/p[2]/a/text()'</span>).extract_first()</div><div class="line"></div><div class="line">    data = &#123;</div><div class="line"><span class="string">'csrf_token'</span>: <span class="string">''</span>,</div><div class="line"><span class="string">'params'</span>: <span class="string">'Ak2s0LoP1GRJYqE3XxJUZVYK9uPEXSTttmAS+8uVLnYRoUt/Xgqdrt/13nr6OYhi75QSTlQ9FcZaWElIwE+oz9qXAu87t2DHj6Auu+2yBJDr+arG+irBbjIvKJGfjgBac+kSm2ePwf4rfuHSKVgQu1cYMdqFVnB+ojBsWopHcexbvLylDIMPulPljAWK6MR8'</span>,</div><div class="line"><span class="string">'encSecKey'</span>: <span class="string">'8c85d1b6f53bfebaf5258d171f3526c06980cbcaf490d759eac82145ee27198297c152dd95e7ea0f08cfb7281588cdab305946e01b9d84f0b49700f9c2eb6eeced8624b16ce378bccd24341b1b5ad3d84ebd707dbbd18a4f01c2a007cd47de32f28ca395c9715afa134ed9ee321caa7f28ec82b94307d75144f6b5b134a9ce1a'</span></div><div class="line">    &#125;</div><div class="line">    DEFAULT_REQUEST_HEADERS[<span class="string">'Referer'</span>] = self.base_url + <span class="string">'/playlist?id='</span> + str(music_id)</div><div class="line">    music_comment = <span class="string">'http://music.163.com/weapi/v1/resource/comments/R_SO_4_'</span> + str(music_id)</div><div class="line"></div><div class="line">    <span class="keyword">yield</span> FormRequest(music_comment, meta=&#123;<span class="string">'id'</span>:music_id,<span class="string">'music'</span>:music,<span class="string">'artist'</span>:artist,<span class="string">'album'</span>:album&#125;, \</div><div class="line">  callback=self.parse_comment, formdata=data)</div></pre></td></tr></table></figure><h3 id="提取热评信息传给item"><a href="#提取热评信息传给item" class="headerlink" title="提取热评信息传给item"></a>提取热评信息传给item</h3><p>这是爬虫部分的最后一步了，这一步从<code>Ajax</code>请求的<code>json</code>数据了提取信息，相信大家都会，就不去多说。我们提取到所有的数据后，就是传给<code>item</code>了。</p><p><code>item</code>的操作和字典是一样的，我们就像保存字典数据一样保存他们就可以了。但是那么多数据写字典那样一步一步的是不是很蠢呢。那有没有方便一点的方法了。这个时候内置的<code>eval</code>方法派上用场，这里不做方法的讲解，用起来很简单，他会动态的获取我们字典的每一个键，然后帮我们保存。我们看代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 获得所有音乐的热评数据</span></div><div class="line"><span class="keyword">import</span> json</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_comment</span><span class="params">(self, response)</span>:</span></div><div class="line">    id = response.meta[<span class="string">'id'</span>]</div><div class="line">    music = response.meta[<span class="string">'music'</span>]</div><div class="line">    artist = response.meta[<span class="string">'artist'</span>]</div><div class="line">    album = response.meta[<span class="string">'album'</span>]</div><div class="line">    result = json.loads(response.text)</div><div class="line">    comments = []</div><div class="line">    <span class="keyword">if</span> <span class="string">'hotComments'</span> <span class="keyword">in</span> result.keys():</div><div class="line"><span class="keyword">for</span> comment <span class="keyword">in</span> result.get(<span class="string">'hotComments'</span>):</div><div class="line">    hotcomment_author = comment[<span class="string">'user'</span>][<span class="string">'nickname'</span>]</div><div class="line">    hotcomment = comment[<span class="string">'content'</span>]</div><div class="line">    hotcomment_like = comment[<span class="string">'likedCount'</span>]</div><div class="line">    <span class="comment"># 这里我们将评论的作者头像也保存，如果大家喜欢这个项目，我后面可以做个web端的展现</span></div><div class="line">    hotcomment_avatar = comment[<span class="string">'user'</span>][<span class="string">'avatarUrl'</span>]</div><div class="line">    data = &#123;</div><div class="line"><span class="string">'nickname'</span>: hotcomment_author,</div><div class="line"><span class="string">'content'</span>: hotcomment,</div><div class="line"><span class="string">'likedcount'</span>: hotcomment_like,</div><div class="line"><span class="string">'avatarurl'</span>: hotcomment_avatar</div><div class="line">    &#125;</div><div class="line">comments.append(data)</div><div class="line"></div><div class="line">    item = MusicItem()</div><div class="line">    <span class="comment"># 由于eval方法不稳定，具体的可以自己搜索，我们过滤一下错误</span></div><div class="line">    <span class="keyword">for</span> field <span class="keyword">in</span> item.fields:</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    item[field] = eval(field)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    print(<span class="string">'Field is not defined'</span>, field)</div><div class="line">    <span class="keyword">yield</span> item</div></pre></td></tr></table></figure><p>最后我们将数据传给<code>Item</code>。</p><h3 id="Pipeline中处理数据"><a href="#Pipeline中处理数据" class="headerlink" title="Pipeline中处理数据"></a>Pipeline中处理数据</h3><p>在<code>Pipeline</code>中处理数据，其实我们这里没什么好对数据做什么改动的，这里我们要对数据做数据库的保存。</p><p>我们需要创建一个<code>mongodb</code>类。然后在<code>settings</code>中将<code>ITEM_PIPELINES</code>的键改为我们创建的<code>mongdb</code>类，由于我们不需要对数据进行改动，所以直接覆盖就好了。为了方便管理和整体架构的清晰，我们也需要在<code>settings</code>中设置我们的数据库信息。具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">   <span class="string">'music163.pipelines.MongoPipeline'</span>: <span class="number">300</span>,</div><div class="line">&#125;</div><div class="line"><span class="comment"># 添加数据库信息</span></div><div class="line">MONGO_URI = <span class="string">'localhost'</span></div><div class="line">MONGO_DB = <span class="string">'music163'</span></div></pre></td></tr></table></figure><p>接下来就是写我们的<code>Mongodb</code>类了。首先我们需要给这个类传入两个参数，也就是我们前面在<code>settings</code>文件定义的数据库<code>uri</code>和数据库名，我们对它们进行一个赋值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></div><div class="line">        self.mongo_uri = mongo_uri</div><div class="line">        self.mongo_db = mongo_db&lt;/pre&gt;</div><div class="line">接下来我们定义一个from_crawler类方法，这个方法就相当于将这个类的两个参数通过crawler对象从settings中拿到这两个参数（数据库uri和名称）。</div><div class="line">&lt;pre class="lang:python decode:true "&gt;class MongoPipeline(object):</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></div><div class="line">        self.mongo_uri = mongo_uri</div><div class="line">        self.mongo_db = mongo_db</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></div><div class="line">        <span class="keyword">return</span> cls(</div><div class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</div><div class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DB'</span>)</div><div class="line">        )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</div><div class="line">        self.db = self.client[self.mongo_db]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.client.close()</div></pre></td></tr></table></figure><p>这里的<code>@classmethod</code>装饰器是<code>python</code>中比较常用的一个方法，具体操作大家可以参阅相关资料了解他。</p><p>后面的<code>open_spider()</code>和<code>close_spider()</code>方法其实是重定义的一个类方法，意思就是我们在启动爬虫的开始，调用<code>open_spider()</code>方法，在关闭爬虫是调用<code>close_spider()</code>方法。我们给他们添加启动数据库和关闭数据库的操作。</p><p>后面是最重要的方法，<code>process_item()</code>方法就是用来对<code>item</code>进行操作的。我们这里主要就是对数据库进行一个插入操作。</p><p>首先我们需要在<code>items.py</code>文件中加入一个 <code>table_name = &#39;music&#39;</code>的属性，也就是相当于一个数据库表名。这样做方便我们将这个属性传到<code>process_item()</code>方法，我们需要调用数据库的<code>update</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">    self.db[item.table_name].update(&#123;<span class="string">'id'</span>: item.get(<span class="string">'id'</span>)&#125;, &#123;<span class="string">'$set'</span>: dict(item)&#125;, <span class="keyword">True</span>)</div><div class="line">    <span class="keyword">return</span> item</div></pre></td></tr></table></figure><p>这个方法有三个参数，第一个参数传入数据库查询的字段，我们使用音乐的<code>id</code>来进行查询。</p><p>第二个参数就是我们的<code>item</code>数据，我们将他转化为字典形式。</p><p>第三个参数至关重要，我们传入<code>True</code>。意思是如果我们查询到相同的数据，我们就做更新操作，如果没有查询到相同的数据就做插入操作。这就相当于我们己做了插入数据库同时有做了去重的操作。</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>好了，这样我们的爬虫就完成了，整理完代码运行起来吧。</p><h2 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h2><p><a href="https://github.com/Blackyukun/163Music" target="_blank" rel="external">github</a></p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前两篇文章我们了解了Scrapy的理论知识，那么我们不能做纸上谈兵的赵括。实践才是检验真理的唯一标准。本篇文章我们来抓取网易云音乐的所有音乐及音乐的热评。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(12):Scrapy组件的用法</title>
    <link href="https://wyukun.coding.me/2017/07/29/Python%E7%88%AC%E8%99%AB%E5%8D%81%E4%BA%8C%E4%B9%8BScrapy%E7%BB%84%E4%BB%B6%E7%9A%84%E7%94%A8%E6%B3%95/"/>
    <id>https://wyukun.coding.me/2017/07/29/Python爬虫十二之Scrapy组件的用法/</id>
    <published>2017-07-29T07:46:06.000Z</published>
    <updated>2017-09-19T07:55:54.368Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>大家好，点开文章大家应该可以听到一曲大气恢弘的音乐，仿佛置身于江湖中，而自己是一名行侠仗义的侠客。见多了江湖的纷扰，你早已经累了，功名利禄对你来说不如一壶好酒。你骑马田间，而这时已是傍晚时分，起风了，你看这远方的夕阳，心生感慨。如果当初能够多一点勇气，那么这数十年来就不会是孤身一人了。你叹了口气，猛地抽了自己一巴掌，乱想什么呢，赶紧学习了。</p><p>通过上一篇文章，我相信大家对于<code>Scrapy</code>的使用也有了初步的了解。如果说要使用<code>Scrapy</code>写一些爬虫，也并非什么难事了。那么为了能写出更加复杂的爬虫，我们在本篇文章就来了解一下<code>Scrapy</code>的组件及他们的用法。</p><p>由于我在浏览<code>Scrapy</code>文档的时候，发现文档说的很是详细，如果我来写这篇文章的话，无疑是班门弄斧。</p><p>所以，本篇文章主要分享一下我在学习<code>Scrapy</code>中对于组件整体运作的理解。<br><a id="more"></a></p><h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>Scrapy作为一个框架，整体的组件协作我们还是要做到流程清晰的。我们来看一下经典的Scrapy架构图：</p><p><img src="http://imgout.ph.126.net/56888086/scrapy_architecture.jpg" alt="image"></p><p>我们上一篇文章写的<code>Scrapy</code>项目大致流程就是：</p><ul><li>确定爬虫名称和要抓取的初始网站；</li><li>请求网站获得响应数据<code>Response</code>；</li><li>解析<code>Response</code>提取数据；</li><li>有需要跟进的<code>url</code>继续循环执行第二步；</li><li>将需要的数据通过管道<code>Pipeline</code>的处理；</li><li>处理完的数据传给<code>Item</code>保存。</li></ul><p><strong>图中的Scrapy Engine就是Scrapy的引擎，他处于中间位置，主要负责各个组件之间的通信，数据传递。</strong></p><p><strong>那上图的Spiders就是来完成爬虫的逻辑，还有网页数据的提取。他将要请求的url通过Scrapy Engine引擎，传给Scheduler（调度器）。</strong></p><p><strong>调度器接收了引擎发送来的Requests请求，按顺序排列整理。</strong></p><p><strong>由于所有的数据交流都是要通过引擎的，所以调度器是通过把处理好的Requests有发送给引擎，由引擎通过Downloader Middlewares（下载中间件）发送给Downloader（下载器）。</strong></p><p><strong>下载器通过下载中间件的设置来下载引擎传过来的Requests的url，并且将获取到的Response又发送给引擎（如果有下载失败的Requests，那么下载器会在后面接着下载）。</strong></p><p><strong>引擎收到Response后，再将它发送给Spiders，Spiders对Response解析提取出需要的数据（或者还有需要继续请求的url，会继续执行上面的循环）。</strong></p><p><strong>Spiders将这些数据发送给引擎，最后引擎将这些数据发送给Item Pipeline进行数据处理和存储。</strong></p><p>那么这个流程就结束了。下面我给大家把相应组件还有<code>Scrapy</code>提供的命令行工具的文档地址整理给大家。</p><h2 id="Spiders"><a href="#Spiders" class="headerlink" title="Spiders"></a>Spiders</h2><p>我们上篇文章有用过<code>Spider</code>，就是我们爬虫类的基类。<code>Spider</code>他主要就是来完成爬虫的逻辑，还有网页数据的提取。他将要请求的<code>url</code>通过<code>Scrapy Engine</code>引擎，传给<code>Scheduler</code>（调度器），</p><p>对spider来说，爬取的循环类似下文:</p><p>以初始的<code>URL</code>初始化<code>Request</code>，并设置回调函数。 当该<code>request</code>下载完毕并返回时，将生成<code>response</code>，并作为参数传给该回调函数。</p><p><code>spider</code>中初始的<code>request</code>是通过调用 start_requests() 来获取的。 <code>start_requests()</code>读取 <code>start_urls</code> 中的<code>URL</code>， 并以 <code>parse</code>为回调函数生成<code>Request</code> 。</p><p>在回调函数内分析返回的(网页)内容，返回 <code>Item</code> 对象或者 <code>Request</code> 或者一个包括二者的可迭代容器。 返回的<code>Request</code>对象之后会经过<code>Scrapy</code>处理，下载相应的内容，并调用设置的<code>callback</code>函数(函数可相同)。</p><p>在回调函数内，您可以使用 选择器(<code>Selectors</code>) (您也可以使用BeautifulSoup, lxml 或者您想用的任何解析器) 来分析网页内容，并根据分析的数据生成item。</p><p>最后，由<code>spider</code>返回的<code>item</code>将被存到数据库(由某些 Item Pipeline 处理)或使用 <code>Feed exports</code> 存入到文件中。</p><p>文档地址：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html#id2" target="_blank" rel="external">here</a></p><h2 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h2><p>文档地址：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/item-pipeline.html#id1" target="_blank" rel="external">here</a></p><h2 id="Downloader-Middleware"><a href="#Downloader-Middleware" class="headerlink" title="Downloader Middleware"></a>Downloader Middleware</h2><p>文档地址：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/downloader-middleware.html#downloader-middleware" target="_blank" rel="external">here</a></p><h2 id="Scrapy命令行工具"><a href="#Scrapy命令行工具" class="headerlink" title="Scrapy命令行工具"></a>Scrapy命令行工具</h2><p>文档地址：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/commands.html#command-line-tools" target="_blank" rel="external">here</a></p><h2 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h2><p>文档地址：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/selectors.html#selectors" target="_blank" rel="external">here</a></p><h2 id="Scrapy源码"><a href="#Scrapy源码" class="headerlink" title="Scrapy源码"></a>Scrapy源码</h2><p>源码地址：<a href="https://github.com/scrapy/scrapy" target="_blank" rel="external">here</a></p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>学一个框架最好的方法就是看他的文档和阅读他的源码，如大家认真看完上面的文档，那么对于<code>Scrapy</code>使用基本上是了然于胸了。那么下面的文章我们就来使用<code>Scrapy</code>写一些爬虫，知行合一才是最好的学习方法。</p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=28786809&auto=1&height=66"></iframe><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大家好，点开文章大家应该可以听到一曲大气恢弘的音乐，仿佛置身于江湖中，而自己是一名行侠仗义的侠客。见多了江湖的纷扰，你早已经累了，功名利禄对你来说不如一壶好酒。你骑马田间，而这时已是傍晚时分，起风了，你看这远方的夕阳，心生感慨。如果当初能够多一点勇气，那么这数十年来就不会是孤身一人了。你叹了口气，猛地抽了自己一巴掌，乱想什么呢，赶紧学习了。&lt;/p&gt;
&lt;p&gt;通过上一篇文章，我相信大家对于&lt;code&gt;Scrapy&lt;/code&gt;的使用也有了初步的了解。如果说要使用&lt;code&gt;Scrapy&lt;/code&gt;写一些爬虫，也并非什么难事了。那么为了能写出更加复杂的爬虫，我们在本篇文章就来了解一下&lt;code&gt;Scrapy&lt;/code&gt;的组件及他们的用法。&lt;/p&gt;
&lt;p&gt;由于我在浏览&lt;code&gt;Scrapy&lt;/code&gt;文档的时候，发现文档说的很是详细，如果我来写这篇文章的话，无疑是班门弄斧。&lt;/p&gt;
&lt;p&gt;所以，本篇文章主要分享一下我在学习&lt;code&gt;Scrapy&lt;/code&gt;中对于组件整体运作的理解。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(11):Scrapy框架的安装和基本使用</title>
    <link href="https://wyukun.coding.me/2017/07/27/Python%E7%88%AC%E8%99%AB%E5%8D%81%E4%B8%80%E4%B9%8BScrapy%E6%A1%86%E6%9E%B6%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <id>https://wyukun.coding.me/2017/07/27/Python爬虫十一之Scrapy框架的安装和基本使用/</id>
    <published>2017-07-27T07:22:09.000Z</published>
    <updated>2017-09-19T07:44:21.735Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>大家好，本篇文章我们来看一下强大的<code>Python</code>爬虫框架<code>Scrapy</code>。<code>Scrapy</code>是一个使用简单，功能强大的异步爬虫框架，我们先来看看他的安装。<br><a id="more"></a></p><h2 id="Scrapy的安装"><a href="#Scrapy的安装" class="headerlink" title="Scrapy的安装"></a>Scrapy的安装</h2><p><code>Scrapy</code>的安装是很麻烦的，对于一些想使用<code>Scrapy</code>的人来说，它的安装常常就让很多人死在半路。在此我将我的安装过程和网络上整理的安装方法，分享给大家，希望大家能够安装顺利。</p><h3 id="Windows安装"><a href="#Windows安装" class="headerlink" title="Windows安装"></a>Windows安装</h3><p>开始之前，我们要确定自己安装了<code>Python</code>，本篇文章我们以<code>Python3.5</code>为例。<code>Scrapy</code>有很多依赖的包，我们来一一安装。</p><ul><li>首先，使用<code>pip -v</code>，查看<code>pip</code>是否安装正常，如果正常，那么我们进行下一步；</li><li><code>pip install wheel</code>这个包我们之前的文章介绍过，安装好他我们就可以安装一些<code>wheel</code>件；</li><li><code>lxml</code>安装，之前的文章说过他的安装，那么我们这里在重新整理一下。whl文件地址：<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml" target="_blank" rel="external">here</a>。找到自己对应版本的文件，下载好后，找到文件位置，右键点击文件属性，点击安全标签，复制他的所在路径。打开管理员工具(cmd)，<code>pip install &lt;粘贴whl路径&gt;</code>；</li><li><code>PyOpenssl</code> 的<code>whl</code>文件地址：<a href="http://www.python.org/pypi/pyOpenSSL#downloads" target="_blank" rel="external">here</a>。点击下载，<code>whl</code>文件安装方式同上；</li><li><code>Twisted</code>框架这个框架是一个异步网络库，是<code>Scrapy</code>的核心。<code>whl</code>文件地址：<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted" target="_blank" rel="external">here</a>；</li><li><code>Pywin32</code>这是一个<code>Pywin32</code>兼容的库，下载地址：<a href="https://sourceforge.net/projects/pywin32/files/pywon32/Build%20220" target="_blank" rel="external">here</a>，选好版本进行下载；</li><li>如果上面的库全都安装好了，那么我们就可以安装我们的<code>Scrapy</code>了，<code>pip install scrapy</code></li></ul><p>是不是很麻烦呢，如果大家不喜欢折腾，那么在<code>Windows</code>下也可以很方便的安装。那就要使用我们之前提到的<code>Anaconda</code>了。具体安装大家自己找找，或者在之前的文章中找。那么他的安装<code>Scrapy</code>只需要一行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda install scrapy</div></pre></td></tr></table></figure><h3 id="Linux安装"><a href="#Linux安装" class="headerlink" title="Linux安装"></a>Linux安装</h3><p><code>Linux</code>系统安装起来就要简单一点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install build-essential python3-dev libssl-dev libffi-dev libxml2 libxml2-dev libxslt1-dev zlib1g-dev</div></pre></td></tr></table></figure><h3 id="Mac-OS安装"><a href="#Mac-OS安装" class="headerlink" title="Mac OS安装"></a>Mac OS安装</h3><p>我们需要先安装一些<code>C++</code>的依赖库，<code>xcode-select --install</code></p><p>需要安装命令行开发工具，我们点击安装。安装完成，那么依赖库也就安装完成了。</p><p>然后我们直接使用<code>pip</code>安装<code>pip install scrapy</code></p><p>以上，我们的<code>Scrapy</code>库的安装基本上就解决了。</p><h2 id="Scrapy的基本使用"><a href="#Scrapy的基本使用" class="headerlink" title="Scrapy的基本使用"></a>Scrapy的基本使用</h2><p><code>Scrapy</code>的中文文档地址：<a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/overview.html" target="_blank" rel="external">here</a></p><blockquote><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p></blockquote><p>他的基本项目流程为：</p><ul><li>创建一个<code>Scrapy</code>项目</li><li>定义提取的<code>Item</code></li><li>编写爬取网站的<code>spider</code>并提取<code>Item</code></li><li>编写<code>Item Pipeline</code>来存储提取到的<code>Item</code>(即数据)</li></ul><p>而一般我们的爬虫流程为：</p><ul><li>抓取索引页：请求索引页的<code>URL</code>并得到源代码，进行下一步分析；</li><li>获取内容和下一页链接：分析源代码，提取索引页数据，并且获取下一页链接，进行下一步抓取；</li><li>翻页爬取：请求下一页信息，分析内容并请求在下一页链接；</li><li>保存爬取结果：将爬取结果保存为特定格式和文本，或者保存数据库。</li></ul><p>我们一步一步来看看如何使用。</p><h3 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h3><p>在开始爬取之前，您必须创建一个新的<code>Scrapy</code>项目。 进入您打算存储代码的目录中，运行下列命令（以知乎日报为例）:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy startproject zhihurb</div></pre></td></tr></table></figure><p>该命令将会创建包含下列内容的 zhihu 目录:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">zhihurb/</div><div class="line">    scrapy.cfg</div><div class="line">    zhihurb/</div><div class="line">        __init__.py</div><div class="line">        items.py</div><div class="line">        pipelines.py</div><div class="line">        settings.py</div><div class="line">        spiders/</div><div class="line">            __init__.py</div><div class="line">            ...</div></pre></td></tr></table></figure><p>这些文件分别是:</p><blockquote><p>scrapy.cfg: 项目的配置文件<br>zhihurb/: 该项目的python模块。之后您将在此加入代码。<br>zhihurb/items.py: 项目中的item文件.<br>zhihurb/pipelines.py: 项目中的pipelines文件.<br>zhihurb/settings.py: 项目的设置文件.<br>zhihurb/spiders/: 放置spider代码的目录.</p></blockquote><h3 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h3><p>这一步是定义我们需要获取到的数据信息，比如我们需要获得网站里的一些<code>url</code>，网站文章的内容，文章的作者等。这一步定义的地方就在我们的<code>items.py</code>文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    name = scrapy.Field()</div><div class="line">    article = scrapy.Field()</div></pre></td></tr></table></figure><h3 id="编写Spider"><a href="#编写Spider" class="headerlink" title="编写Spider"></a>编写Spider</h3><p>这一步就是写我们最熟悉的爬虫了，而我们的<code>Scrapy</code>框架可以让我们不需要去考虑实现的方法，只需要写出爬取的逻辑就可以了。</p><p>首先我们需要在 spiders/ 文件夹下创建我们的爬虫文件，比如就叫<code>spider.py</code>。写爬虫前，我们需要先定义一些内容。我们以知乎日报为例：<code>https://daily.zhihu.com/</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Spider</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSpider</span><span class="params">(Spider)</span>:</span></div><div class="line">    name = <span class="string">"zhihu"</span></div><div class="line">    allowed_domains = [<span class="string">"zhihu.com"</span>]</div><div class="line">start_urls = [<span class="string">'https://daily.zhihu.com/'</span>]</div></pre></td></tr></table></figure><p>这里我们定义了什么呢?首先我们导入了<code>Scrapy</code>的<code>Spider</code>组件。然后创建一个爬虫类，在类里我们定义了我们的爬虫名称：zhihu（注意：爬虫名称独一无二的，是不可以和别的爬虫重复的）。还定义了一个网址范围，和一个起始 url 列表，说明起始 url 可以是多个。</p><p>然后我们定义一个解析函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">    print(response.text)</div></pre></td></tr></table></figure><p>我们直接打印一下，看看这个解析函数是什么。</p><h3 id="运行爬虫"><a href="#运行爬虫" class="headerlink" title="运行爬虫"></a>运行爬虫</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl zhihu</div></pre></td></tr></table></figure><p>由于<code>Scrapy</code>是不支持在<code>IDE</code>中执行，所以我们必须在命令行里执行命令，我们要确定是不是<code>cd</code>到爬虫目录下。然后执行，这里的命令顾名思义，<code>crawl</code>是蜘蛛的意思，<code>zhihu</code>就是我们定义的爬虫名称了。</p><p>查看输出，我们先看到的是一些爬虫类的输出，可以看到输出的<code>log</code>中包含定义在 <code>start_urls</code> 的初始URL，并且与<code>spider</code>中是一一对应的。我们接着可以看到打印出了网页源代码。可是我们似乎并没有做什么，就得到了网页的源码，这是<code>Scrapy</code>比较方便的一点。</p><h3 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h3><p>接着就可以使用解析工具解析源码，拿到数据了。</p><p>由于<code>Scrapy</code>内置了<code>CSS</code>和<code>xpath</code>选择器，而我们虽然可以使用<code>Beautifulsoup</code>，但是<code>BeautifulSoup</code>的缺点就是慢，这不符合我们<code>Scrapy</code>的风格，所有我还是建议大家使用<code>CSS</code>或者<code>Xpath</code>。</p><p>由于之前我并没有写过关于<code>Xpath</code>或者<code>CSS</code>选择器的用法，那么首先这个并不难，而且熟悉浏览器的用法，可以很简单的掌握他们。</p><p>我们以提取知乎日报里的文章<code>url</code>为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">    urls = response.xpath(<span class="string">'//div[@class="box"]/a/@href'</span>).extract()</div><div class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urls:</div><div class="line">    <span class="keyword">yield</span> Request(url, callback=self.parse_url)</div></pre></td></tr></table></figure><p>这里我们使用<code>xpath</code>解析出所有的<code>url</code>（extract()是获得所有URL集合，extract_first()是获得第一个）。然后将<code>url</code>利用<code>yield</code>语法糖，回调函数给下一个解析<code>url</code>的函数。</p><h3 id="使用item"><a href="#使用item" class="headerlink" title="使用item"></a>使用item</h3><p>后面详细的组件使用留在下一章讲解，这里假如我们解析出了文章内容和标题，我们要将提取的数据保存到<code>item</code>容器。</p><p><code>Item</code>对象相当于是自定义的<code>python</code>字典。 您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 假如我们下一个解析函数解析出了数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_url</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="comment"># name = xxxx</span></div><div class="line"><span class="comment"># article = xxxx</span></div><div class="line"><span class="comment"># 保存</span></div><div class="line">item = DmozItem()</div><div class="line">item[<span class="string">'name'</span>] = name</div><div class="line">item[<span class="string">'article'</span>] = article</div><div class="line"></div><div class="line"><span class="comment"># 返回item</span></div><div class="line"><span class="keyword">yield</span> item</div></pre></td></tr></table></figure><h3 id="保存爬取到的数据"><a href="#保存爬取到的数据" class="headerlink" title="保存爬取到的数据"></a>保存爬取到的数据</h3><p>这里我们需要在管道文件<code>pipelines.py</code>里去操作数据，比如我们要将这些数据的文章标题只保留 5 个字，然后保存在文本里。或者我们要将数据保存到数据库里，这些都是在管道文件里面操作。我们后面在详细讲解。</p><p>那么最简单的存储方法是使用命令行命令：</p><p><code>scrapy crawl zhihu -o items.json</code></p><p>这条命令就会完成我们的数据保存在根目录的<code>json</code>文件里，我们还可以将他格式保存为<code>msv</code>,<code>pickle</code>等。改变命令后面的格式就可以了。</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>本篇教程仅介绍了<code>Scrapy</code>的基础，还有很多特性没有涉及到，那么我会在下一篇文章分享一下我对于<code>Scrapy</code>组件的学习理解。</p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大家好，本篇文章我们来看一下强大的&lt;code&gt;Python&lt;/code&gt;爬虫框架&lt;code&gt;Scrapy&lt;/code&gt;。&lt;code&gt;Scrapy&lt;/code&gt;是一个使用简单，功能强大的异步爬虫框架，我们先来看看他的安装。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(10):Selenium+PhantomJS基本操作</title>
    <link href="https://wyukun.coding.me/2017/07/26/Python%E7%88%AC%E8%99%AB%E5%8D%81%E4%B9%8BSelenium+PhantomJS%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>https://wyukun.coding.me/2017/07/26/Python爬虫十之Selenium+PhantomJS基本操作/</id>
    <published>2017-07-26T09:07:22.000Z</published>
    <updated>2017-09-16T09:30:56.148Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>大家好，这篇文章我们来看一下<code>Selenium</code>库结合<code>PhantomJs</code>，<code>Chrome</code>等一些浏览器的操作。那么我们在之前的文章中，有提到过<code>Selenium</code>库和<code>PhantomJ</code>，说他们结合使用是万能的利器。那么，他们真的那么厉害吗，我们一起来看看<code>Selenium</code>库的用法吧。<br><a id="more"></a></p><h2 id="什么是Selenium"><a href="#什么是Selenium" class="headerlink" title="什么是Selenium"></a>什么是Selenium</h2><p><code>Selenium</code>是一个自动化测试工具，支持包括<code>Chrome</code>，<code>Firefox</code>，<code>Safari</code>，<code>PhantomJs</code>等一些浏览器。如果用于爬虫中，我们主要用来解决一些<code>JavaScript</code>渲染的问题。</p><p>我们在使用<code>Requests</code>库去请求一些网页的时候，比如 163music，我们获得的响应数据呢，并不全是我们在浏览器中看到的信息。他可能是通过<code>js</code>渲染出来的。那么，我们如果使用<code>Selenium</code>库，就不会再去关心如何去解决这种问题了。</p><p>因为我们的浏览器，比如<code>PhantomJs</code>，他就是一个无界面的浏览器，他用来渲染解析<code>js</code>，而<code>Selenium</code>库就负责给浏览器发送一些命令，模拟一些比如下拉，拖拽，翻页，输入表单等动作。这样他们两个结合，对于那些 JS 的渲染问题是不是完美解决了。</p><blockquote><p>文档地址：<a href="http://selenium-python.readthedocs.io/index.html" target="_blank" rel="external">here</a></p></blockquote><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>虽然<code>Selenium</code>库加上<code>PhantomJs</code>很好用，但是他毕竟是驱动一个浏览器，然后获取数据。所以在我们使用中，会发现他并没有我们使用一些解析库速度快。这其实就是他的弊端，所以我还是建议大家，不到实在找不到解决办法的时候，不去使用他们。</p><h2 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h2><p><code>pip</code>直接安装<code>Selenium</code>库：<code>pip install selenium</code></p><p>浏览器驱动的安装：</p><ul><li><code>Chrome</code>浏览器驱动：<a href="https://sites.google.com/a/chromium.org/chromedriver/downloads" target="_blank" rel="external">地址</a></li><li><code>PhantomJs</code>浏览器驱动：<a href="http://phantomjs.org/download.html" target="_blank" rel="external">地址</a></li></ul><p><strong>我们需要把安装好的浏览器驱动配置到我们的环境变量。对于Windows用户，配置环境变量比较麻烦。我们需要找到下载好的驱动位置，然后复制他的文件位置，见他粘贴到环境变量即可。</strong></p><h2 id="使用样例"><a href="#使用样例" class="headerlink" title="使用样例"></a>使用样例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</div><div class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</div><div class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</div><div class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</div><div class="line"></div><div class="line"></div><div class="line">browser = webdriver.Chrome()</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    browser.get(<span class="string">'http://www.yukunweb.com'</span>)</div><div class="line">    input = browser.find_element_by_id(<span class="string">'s'</span>)</div><div class="line">    input.send_keys(<span class="string">'Python'</span>)</div><div class="line">    input.send_keys(Keys.ENTER)</div><div class="line">    wait = WebDriverWait(browser, <span class="number">10</span>)</div><div class="line">    wait.until(EC.presence_of_element_located((By.ID, <span class="string">'main'</span>)))</div><div class="line">    print(browser.current_url)</div><div class="line">    print(browser.page_source)</div><div class="line"><span class="keyword">finally</span>:</div><div class="line">    browser.close()</div></pre></td></tr></table></figure><p>如果我们运行上面的代码，会看到本地打开了一个<code>Chrome</code>浏览器，然后在浏览器地址栏输入了我的博客网址，然后他会自动的在搜索栏输入‘Python’，并且点击了回车搜索。并且将结果页的<code>url</code>和源代码打印出来。</p><p>我们的例子都是使用<code>Chrome</code>浏览器来操作，因为<code>PhantomJs</code>是无界面的，不方便查看到效果。如果大家运行错误的话，一般情况是浏览器并没有打开，那么应该是大家没有安装好<code>Chrome</code>浏览器，或者没有将驱动配置环境变量。</p><p><em>那么这几行代码究竟是什么意思呢，我们究竟赋予了什么指令呢？</em></p><h3 id="声明浏览器对象"><a href="#声明浏览器对象" class="headerlink" title="声明浏览器对象"></a>声明浏览器对象</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"></div><div class="line">browser = webdriver.Chrome()</div><div class="line"><span class="comment"># 声明其他浏览器</span></div><div class="line">browser = webdriver.PhantomJs()</div><div class="line">browser = webdriver.Firefox()&lt;/pre&gt;</div><div class="line">这就相当于我们调用了Selenium库的webdriver方法，实例化一个Chrome浏览器给我们调用。</div><div class="line">&lt;h2&gt;访问页面&lt;/h2&gt;</div><div class="line">&lt;pre class="lang:python decode:true "&gt;from selenium import webdriver</div><div class="line"> </div><div class="line">browser = webdriver.Chrome()</div><div class="line">browser.get(<span class="string">'http://www.yukunweb.com'</span>)</div></pre></td></tr></table></figure><p>我们将要访问的<code>url</code>传给<code>get</code>方法。调用浏览器访问<code>url</code>。</p><h3 id="查找元素"><a href="#查找元素" class="headerlink" title="查找元素"></a>查找元素</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">input = browser.find_element_by_id(<span class="string">'s'</span>)</div></pre></td></tr></table></figure><p>这句代码调用<code>find_element_by_id</code>方法，顾名思义，就是查找<code>id</code>为‘s’的标签，那么如果是操作<code>class</code>为‘s’的话，就是<code>find_element_by_class(&#39;s&#39;)</code>。</p><p>当然，我们还可以使用 CSS选择器和<code>xpath</code>选择器查找元素：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">input = browser.find_element_by_css_selector(<span class="string">"#s"</span>)</div><div class="line">print(input)</div><div class="line">input = browser.find_element_by_xpath(<span class="string">'//*[@id="s"]'</span>)</div><div class="line">print(input)</div></pre></td></tr></table></figure><p>通过打印结果，可以看到不管使用什么选择器，查找结果都是一样的。下面是一些查找<code>api</code>：</p><ul><li>find_element_by_name</li><li>find_element_by_xpath</li><li>find_element_by_link_text</li><li>find_element_by_partial_link_text</li><li>find_element_by_tag_name</li><li>find_element_by_class_name</li><li>find_element_by_css_selector</li></ul><h3 id="查找多个元素"><a href="#查找多个元素" class="headerlink" title="查找多个元素"></a>查找多个元素</h3><p>如果我们查找的元素是网页中的<code>li</code>标签，是很多的元素。那么我们的查找方式和单个元素是相同的，只是对于查找的<code>api</code>我们需要在<code>element</code>后面加个复数形式 s。即是：</p><ul><li>find_elements_by_name</li><li>find_elements_by_xpath</li><li>find_elements_by_link_text</li><li>find_elements_by_partial_link_text</li><li>find_elements_by_tag_name</li><li>find_elements_by_class_name</li><li>find_elements_by_css_selector</li></ul><h3 id="元素交互操作"><a href="#元素交互操作" class="headerlink" title="元素交互操作"></a>元素交互操作</h3><p>即是对于我们获取的元素下达指令，调用交互的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">browser.get(<span class="string">'http://www.yukunweb.com'</span>)</div><div class="line">input = browser.find_element_by_id(<span class="string">'s'</span>)</div><div class="line">input.send_keys(<span class="string">'Python'</span>)</div><div class="line">input.send_keys(Keys.ENTER)</div></pre></td></tr></table></figure><p>这段代码中，我们首先查找到了<code>id</code>为‘s’的元素，然后传给他‘Python’值，然后调用交互方法，敲了回车。</p><p>当然，在大多是情况下，我们不能直接使用敲击回车的方法，因为我们不确定是不是敲了回车，表单就提交了。我们需要使用查找器查找到提交按钮元素，然后模拟点击:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">button = browser.find_element_by_class_name(<span class="string">'xxxx'</span>)</div><div class="line">button.click()</div><div class="line"><span class="comment"># 清除表单信息</span></div><div class="line">button.clear()</div></pre></td></tr></table></figure><p>那么，我们可以看到在模拟登陆时候，直接让我们手动的输入账号，密码，如果有验证码的话直接给一个<code>input</code>方法，我们手动输入验证码传给表单，是不是很简单的就模拟登录了了。</p><h3 id="交互动作"><a href="#交互动作" class="headerlink" title="交互动作"></a>交互动作</h3><p>元素交互动作与上面的操作是不同的。上面的操作需要获得一个特定的元素。然后对这个特定的元素调用一些指令，才可以完成交互。而这个交互是将这些动作附加到动作链中串行执行。</p><p>我们以拖拽元素为例(我们需要导入<code>ACtionChains</code>方法)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">rom selenium <span class="keyword">import</span> webdriver</div><div class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ActionChains</div><div class="line"></div><div class="line">browser = webdriver.Chrome()</div><div class="line"></div><div class="line">browser.get(url)</div><div class="line">source = browser.find_element_by_name(<span class="string">"source"</span>)</div><div class="line">target = browser.find_element_by_name(<span class="string">"target"</span>)</div><div class="line">actions = ActionChains(browser)</div><div class="line">actions.drag_and_drop(source, target).perform()</div></pre></td></tr></table></figure><p>这里的<code>sourcs</code>是我们要拖拽的元素，我们使用查找器找到他，<code>target</code>就是我们要拖拽到的位置元素。然后调用<code>ActionChains</code>方法，实现拖拽操作。</p><p>更多的操作可以查看文档：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains" target="_blank" rel="external">here</a></p><h3 id="执行JavaScript"><a href="#执行JavaScript" class="headerlink" title="执行JavaScript"></a>执行JavaScript</h3><p>有些动作呢，<code>Selenium</code>库并没有为我们提供特定的<code>api</code>，比如说将浏览器进度条下拉，这个实现起来是很难的。那么我们就可以通过让<code>Selenium</code>执行<code>JS</code>来实现进度条的下拉，这个得需要一些<code>js</code>的知识，不过还是很简单的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"></div><div class="line">browser = webdriver.Chrome()</div><div class="line">browser.get(<span class="string">'http://www.yukunweb.com'</span>)</div><div class="line">browser.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight)'</span>)</div><div class="line">browser.execute_script(<span class="string">'alert("到达底部")'</span>)</div></pre></td></tr></table></figure><p>这就相当于我们将一些JS命令传给<code>Selenium</code>的<code>execute_script</code>这个<code>api</code>，我们运行就可以看到浏览器下拉到底部，然后弹出会话框。</p><h3 id="获取元素文本值"><a href="#获取元素文本值" class="headerlink" title="获取元素文本值"></a>获取元素文本值</h3><p>如果我们查找得到一个元素，我们要怎样获得元素的一些属性和文本信息呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"></div><div class="line">browser = webdriver.Chrome()</div><div class="line"></div><div class="line">browser.get(<span class="string">'http://www.yukunweb.com'</span>)</div><div class="line">name = browser.find_element_by_css_selector(<span class="string">'#kratos-logo &amp;gt; a'</span>)</div><div class="line">print(name.text)</div><div class="line">print(name.get_attribute(<span class="string">'href'</span>))</div></pre></td></tr></table></figure><p>运行结果可以看到，他打印出了‘意外’和他的url。</p><h3 id="Frame框架"><a href="#Frame框架" class="headerlink" title="Frame框架"></a>Frame框架</h3><p>有些网页在我们直接使用<code>Selenium</code>驱动浏览器打印源码的时候，并没有如期获得想要的数据，那在我们查看网页源码的时候，可以看到网页的<code>iframe</code>标签包裹的一个一个的框架。那么这就需要我们请求对应框架，拿到源码了。</p><p>我们以网易云音乐的歌手栏为例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"></div><div class="line">browser = webdriver.Chrome()</div><div class="line">browser.get(<span class="string">'https://music.163.com/#/discover/artist/signed/'</span>)</div><div class="line"></div><div class="line">print(page_source)</div></pre></td></tr></table></figure><p>可以查看结果，并没有我们想要的信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"></div><div class="line">browser = webdriver.Chrome()</div><div class="line">browser.get(<span class="string">'https://music.163.com/#/discover/artist/signed/'</span>)</div><div class="line">browser.switch_to.frame(<span class="string">'contentFrame'</span>)</div><div class="line"></div><div class="line">print(page_source)</div></pre></td></tr></table></figure><p>这次打印，我们就可以看到我们需要的信息了，是不是很简单。</p><h3 id="显示等待"><a href="#显示等待" class="headerlink" title="显示等待"></a>显示等待</h3><p>在文章开始的时候，我们运行的那段代码中有一段代码是不是还没有说。那就是我们命令浏览器等待的操作。</p><p>等待有两种方式，一种是隐士等待，一种是显示等待。当使用了隐士等待执行时，如果浏览器没有找到指定元素，将继续等待，如果超出设定时间就会抛出找不到元素的异常。而大多数情况我们建议使用显示等待。</p><p>显示等待是你指定一个等待的条件，还指定一个最长等待时间。那么程序会在最长等待时间内，判断条件是否成立，如果成立，立即返回。如果不成立，他会一直等待，直到最长等待时间结束，如果条件仍然不满足，就返回异常。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wait = WebDriverWait(browser, <span class="number">10</span>)</div><div class="line">wait.until(EC.presence_of_element_located((By.ID, <span class="string">'main'</span>)))</div></pre></td></tr></table></figure><p>这里的<code>By.ID</code>方法实际上就是一个查找的万能方法，而我们直接查找或者使用<code>CSS</code>、<code>xpath</code>查找足够满足，我也不过多介绍，想要了解可以查看官方文档。</p><p>这里是知道查找到<code>id</code>为‘main’就返回。</p><p>显示等待的一些条件还有：</p><ul><li>title_is 标题是某内容</li><li>title_contains 标题包含某内容</li><li>presence_of_element_located 元素加载出，传入定位元组，如(By.ID, ‘p’)</li><li>visibility_of_element_located 元素可见，传入定位元组</li><li>visibility_of_element_located 元素可见，传入定位元组</li><li>visibility_of_element_located 元素可见，传入定位元组</li><li>visibility_of 可见，传入元素对象</li><li>presence_of_all_elements_located 所有元素加载出</li><li>text_to_be_present_in_element 某个元素文本包含某文字</li><li>text_to_be_present_in_element_value 某个元素值包含某文字</li><li>frame_to_be_available_and_switch_to_it frame加载并切换</li><li>invisibility_of_element_located 元素不可见</li><li>element_to_be_clickable 元素可点击</li><li>staleness_of 判断一个元素是否仍在DOM，可判断页面是否已经刷新</li><li>element_to_be_selected 元素可选择，传元素对象</li><li>element_located_to_be_selected 元素可选择，传入定位元组</li><li>element_selection_state_to_be 传入元素对象以及状态，相等返回True，否则返回False</li><li>element_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回False</li><li>alert_is_present 是否出现Alert</li></ul><h3 id="窗口选择"><a href="#窗口选择" class="headerlink" title="窗口选择"></a>窗口选择</h3><p>如果我们在表单输入关键词，提交表单后浏览器新打开了一个窗口，那么我们要怎么去操作新的窗口呢？索性<code>Selenium</code>为我们提供了对应的<code>api</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</div><div class="line"></div><div class="line">browser = webdriver.Chrome()</div><div class="line">browser.get(<span class="string">'http://www.23us.cc/'</span>)</div><div class="line">input = browser.find_element_by_id(<span class="string">'bdcs-search-form-input'</span>)</div><div class="line">input.send_keys(<span class="string">'斗破苍穹'</span>)</div><div class="line">input.send_keys(Keys.ENTER)</div><div class="line">browser.switch_to_window(browser.window_handles[<span class="number">1</span>])</div><div class="line">print(browser.current_url)</div><div class="line">time.sleep(<span class="number">1</span>)</div><div class="line">browser.switch_to_window(browser.window_handles[<span class="number">0</span>])</div><div class="line">print(browser.current_url)</div></pre></td></tr></table></figure><p>通过打印结果，不难看出先打印了搜索结果窗口<code>url</code>，然后打印了索引页<code>url</code>。要注意窗口的索引是从 0 开始的哦，这个大家都明白。</p><h3 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h3><p>异常处理和普通的异常处理一样，没有什么要说的，大家自己查看官方异常 api.<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions" target="_blank" rel="external">地址</a></p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>好了，通过本篇文章希望大家可以基本上了解<code>Selenium</code>库结合浏览器驱动的一些使用方法。我们例子里使用的是<code>Chrome</code>，但是大家在实际的代码里最好是使用<code>PhantomJs</code>，因为他是无界面的，运行起来相对好一点。</p><p>文章开始说过一般情况下不建议大家使用<code>Selenium</code>，因为他很慢。但是即使是慢，也很爽啊，是不是。</p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大家好，这篇文章我们来看一下&lt;code&gt;Selenium&lt;/code&gt;库结合&lt;code&gt;PhantomJs&lt;/code&gt;，&lt;code&gt;Chrome&lt;/code&gt;等一些浏览器的操作。那么我们在之前的文章中，有提到过&lt;code&gt;Selenium&lt;/code&gt;库和&lt;code&gt;PhantomJ&lt;/code&gt;，说他们结合使用是万能的利器。那么，他们真的那么厉害吗，我们一起来看看&lt;code&gt;Selenium&lt;/code&gt;库的用法吧。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(9):Cookie介绍和模拟登录</title>
    <link href="https://wyukun.coding.me/2017/07/21/Python%E7%88%AC%E8%99%AB%E4%B9%9D%E4%B9%8BCookie%E4%BB%8B%E7%BB%8D%E5%92%8C%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95/"/>
    <id>https://wyukun.coding.me/2017/07/21/Python爬虫九之Cookie介绍和模拟登录/</id>
    <published>2017-07-21T08:59:08.000Z</published>
    <updated>2017-09-16T09:07:02.228Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>大家好，这篇文章我们来看一下<code>Cookie</code>是什么，和他的一些用法。<br><a id="more"></a></p><h2 id="什么是Cookie？"><a href="#什么是Cookie？" class="headerlink" title="什么是Cookie？"></a>什么是Cookie？</h2><blockquote><p>在计算机术语中是指一种能够让网站服务器把少量数据储存到客户端的硬盘或内存，或是从客户端的硬盘读取数据的一种技术。</p></blockquote><p>先看看<code>Cookie</code>长什么样子，我们以知乎为例。打开<code>Chrome</code>的开发者工具<code>(F12)</code>，点开<code>Network</code>，点击一个请求<code>url</code>，就可以看到请求头<code>(Request Headers)</code>里面的一个<code>Cookie</code>信息了。当然我们从上面的响应头中可以看到一个<code>Set-Cookie</code>的信息，这就是服务器向浏览器设置<code>Cookie</code>的一些信息，比如<code>Cookie</code>的作用域，时间等。</p><p><img src="http://imgout.ph.126.net/56773106/cookie.jpg" alt="image"></p><h2 id="Cookie的用途"><a href="#Cookie的用途" class="headerlink" title="Cookie的用途"></a>Cookie的用途</h2><ul><li>存储用户登录信息，判断用户是否登录；</li><li>保存用户浏览足迹；</li></ul><p><code>Cookie</code>是服务器在客户端存储的信息。这些信息是有服务器生成服务器解释的。</p><p>请求时，客户端需要把未超时的<code>Cookies</code>发送给服务器。服务器需要解析<code>Cookies</code>判断用户信息。</p><p>我们的浏览器会在你每次浏览网页是自动存储<code>cookie</code>，比如你打开清除浏览器浏览记录时，就会有<code>cookie</code>信息。</p><p><code>cookie</code>给我们带来了很多便利，他还可以记录我们的浏览足迹，停留页面时间。比如你狂淘宝的时候，淘宝的推荐你喜欢的宝贝，就是根据你的<code>cookie</code>，获取你浏览过哪些商品，而生成的。</p><h2 id="模拟登录演示"><a href="#模拟登录演示" class="headerlink" title="模拟登录演示"></a>模拟登录演示</h2><p>我们还是以知乎为例。<code>https://www.zhihu.com/people/yu-kun-73/answers</code></p><p>这个<code>URL</code>是我的知乎信息页，当然大家可以使用自己的知乎页做例子。如果我们直接访问他，会看到上方的登录按钮，而是我已经登录了在访问呢。就不会显示登录按钮，而是个人编辑资料一些链接。</p><p>那么，如果我们使用爬虫去访问呢。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">headers = &#123;</div><div class="line"><span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'</span></div><div class="line">&#125;</div><div class="line">url = <span class="string">'https://www.zhihu.com/people/yu-kun-73/answers'</span></div><div class="line">resp = requests.get(url, headers=headers).text</div><div class="line">print(resp)</div></pre></td></tr></table></figure><p>通过打印的结果，我们可以看到，html 代码里是有登录按钮的。这就证明我们并没有登录这个页面。那么，如果我们在爬取一些需要登录账号才可以爬取数据的网站时，我们必然是要解决登录问题的。那么如何实现登录呢。</p><p>答案就是我们的<code>Cookie</code>了。我们在之前的文章中说过，<code>requests</code>库在处理<code>Cookie</code>时，是非常简单的。我们只需要向添加‘User-Agent’一样添加请求<code>cookies</code>就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">headers = &#123;</div><div class="line"><span class="string">'Cookie'</span>: <span class="comment"># 你的登录过后的浏览器cookies,</span></div><div class="line"><span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'</span></div><div class="line">&#125;</div><div class="line">url = <span class="string">'https://www.zhihu.com/people/yu-kun-73/answers'</span></div><div class="line">resp = requests.get(url, headers=headers).text</div><div class="line">print(resp)</div></pre></td></tr></table></figure><p>我们再次看打印信息，就可以看到有编辑个人资料的链接了。这就说明我们已经成功登录了。是不是很简单呢，别急，难的在后面呢。</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>那么，如果我们要登录一些复杂的网站，比如新浪微博这种必须要登录才能获取信息的网站，使用<code>cookie</code>登录是肯定行不通的。因为这些网站会经常更新一些网站的算法，我们的<code>cookie</code>就会在工作一会失效，那么这些网站我们就需要向登录页<code>post</code>我们的登录信息。</p><p>这些登录信息参数一般都会存放在登录页的请求头下面的<code>Form Data</code>里面，我们只要携带这些信息<code>Post</code>，就可以顺利登录。但是如果遇到验证码，那么就很麻烦了。验证码的问题一直是考验爬虫的一个门槛，这个大家自己研究。</p><p>但是我们有一个最后的大杀器，<code>Selenium+PhantomJS</code>。这是什么呢？我们后面单独介绍。</p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大家好，这篇文章我们来看一下&lt;code&gt;Cookie&lt;/code&gt;是什么，和他的一些用法。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(8):分析Ajax请求爬取果壳网</title>
    <link href="https://wyukun.coding.me/2017/07/17/Python%E7%88%AC%E8%99%AB%E5%85%AB%E4%B9%8B%E5%88%86%E6%9E%90Ajax%E8%AF%B7%E6%B1%82%E7%88%AC%E5%8F%96%E6%9E%9C%E5%A3%B3%E7%BD%91/"/>
    <id>https://wyukun.coding.me/2017/07/17/Python爬虫八之分析Ajax请求爬取果壳网/</id>
    <published>2017-07-17T08:38:39.000Z</published>
    <updated>2017-09-16T08:57:17.620Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>本篇文章我们来研究一下怎么分析网页的<code>Ajax</code>请求。</p><p>我们在平时爬取网页的时候，可能都遇到过有些网页直接请求得到的 HTML 代码里面，并没有我们需要的数据，也就是我们在浏览器中看到的内容。</p><p>这就是因为这些信息是通过<code>Ajax</code>加载的，并且通过<code>js</code>渲染生成的。这个时候我们就需要分析这个网页的请求了。<br><a id="more"></a></p><h2 id="什么是Ajax"><a href="#什么是Ajax" class="headerlink" title="什么是Ajax"></a>什么是Ajax</h2><blockquote><p>AJAX即“Asynchronous Javascript And XML”（异步JavaScript和XML），是指一种创建交互式网页应用的网页开发技术。<br>AJAX = 异步 JavaScript和XML（标准通用标记语言的子集）。<br>AJAX 是一种用于创建快速动态网页的技术。<br>AJAX 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。</p></blockquote><p>简单的说就是网页加载，浏览器地址栏的网址并没有变，是<code>javascript</code>异步加载的网页，应该是<code>ajax</code>。<code>AJAX</code>一般是通过<code>XMLHttpRequest</code> 对象接口发送请求的，<code>XMLHttpRequest</code> 一般被缩写为 <code>XHR</code>。</p><h2 id="分析果壳网站点"><a href="#分析果壳网站点" class="headerlink" title="分析果壳网站点"></a>分析果壳网站点</h2><p>我们目标网站就以果壳网来进行分析。<a href="http://www.guokr.com/scientific/" target="_blank" rel="external">地址</a></p><p>我们可以看到这个网页并没有翻页按钮，而当我们一直往下拉请求，网页会自动的给我们加载出更多内容。但是，当我们观察网页<code>url</code>时，发现它并没有随着网页的加载请求而变化。而当我们直接请求这个<code>url</code>时，显然我们只能获得到第一页的<code>html</code>内容。</p><p><img src="http://imgout.ph.126.net/56711076/guoke3.jpg" alt="image"></p><h3 id="那我们要怎么获得所有页的数据呢？"><a href="#那我们要怎么获得所有页的数据呢？" class="headerlink" title="那我们要怎么获得所有页的数据呢？"></a>那我们要怎么获得所有页的数据呢？</h3><p>我们在<code>Chrome</code>中打开开发者工具<code>(F12)</code>。我们点击<code>Network</code>，点击<code>XHR</code>标签。然后我们刷新网页，往下拉请求。这个时候我们就可以看到<code>XHR</code>标签，在网页每一次加载的时候就会跳出一个请求。</p><p>我们点击第一个请求，可以看到他的参数：</p><blockquote><p>retrieve_type:by_subject<br>limit:20<br>offset:18<br>-:1500265766286</p></blockquote><p>在点击第二个请求，参数如下：</p><blockquote><p>retrieve_type:by_subject<br>limit:20<br>offset:38<br>-:1500265766287</p></blockquote><p><code>limit</code>参数是网页每一页限制加载的文章数，<code>offset</code>就是页数了。接着往下看，我们会发现每一个请求的<code>offset</code>参数都会加 20。</p><p>我们接着看每一个请求的响应内容，这是一个就是格式的数据。我们点开<code>result</code>键，可以看到一个 20 篇文章的数据信息。这样我们就成功找到我们需要的信息位置了，我们可以在请求头中看到存放<code>json</code>数据的<code>url</code>地址。<code>http://www.guokr.com/apis/minisite/article.json?retrieve_type=by_subject&amp;amp;limit=20&amp;amp;offset=18</code></p><p><img src="http://imgout.ph.126.net/56710039/guoke6.jpg" alt="image"></p><h2 id="爬取流程"><a href="#爬取流程" class="headerlink" title="爬取流程"></a>爬取流程</h2><ul><li>分析Ajax请求获得每一页的文章url信息；</li><li>解析每一篇文章，获得需要数据；</li><li>将获得的数据保存数据库；</li><li>开启多进程，大量抓取。</li></ul><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><p>我们的工具仍然使用<code>requests</code>请求，<code>BeautifulSoup</code>解析。</p><p>首先我们要通过分析<code>Ajax</code>请求，获得所有页的信息，通过对上面对网页的分析，可以得到<code>Ajax</code>加载的<code>json</code>数据的<code>URL</code>地址为：<code>http://www.guokr.com/apis/minisite/article.json?retrieve_type=by_subject&amp;amp;limit=20&amp;amp;offset=18</code></p><p>我们需要构造这个 URL。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入可能要用到的模块</span></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</div><div class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ConnectionError</div><div class="line"></div><div class="line"><span class="comment"># 获得索引页的信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_index</span><span class="params">(offset)</span>:</span></div><div class="line">    base_url = <span class="string">'http://www.guokr.com/apis/minisite/article.json?'</span></div><div class="line">    data = &#123;</div><div class="line">        <span class="string">'retrieve_type'</span>: <span class="string">"by_subject"</span>,</div><div class="line">        <span class="string">'limit'</span>: <span class="string">"20"</span>,</div><div class="line">        <span class="string">'offset'</span>: offset</div><div class="line">    &#125;</div><div class="line">    params = urlencode(data)</div><div class="line">    url = base_url + params</div><div class="line">    </div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        resp = requests.get(url)</div><div class="line">        <span class="keyword">if</span> resp.status_code == <span class="number">200</span>:</div><div class="line">            <span class="keyword">return</span> resp.text</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span></div><div class="line">    <span class="keyword">except</span> ConnectionError:</div><div class="line">        print(<span class="string">'Error.'</span>)</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span></div></pre></td></tr></table></figure><p>我们把上面分析页面得到的请求参数构造成一个字典<code>data</code>，然后我们可以手动的构造这个<code>url</code>，但是<code>urllib</code>库已经给我们提供了一个编码方法，我们直接使用，就可以构造出完整的<code>url</code>了。然后是标准的<code>requests</code>请求页面内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> json</div><div class="line"></div><div class="line"><span class="comment"># 解析json，获得文章url</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_json</span><span class="params">(text)</span>:</span></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        result = json.loads(text)</div><div class="line">        <span class="keyword">if</span> result:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> result.get(<span class="string">'result'</span>):</div><div class="line">                <span class="comment"># print(i.get('url'))</span></div><div class="line">                <span class="keyword">yield</span> i.get(<span class="string">'url'</span>)</div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure><p>我们使用<code>josn.loads</code>方法解析<code>json</code>，将其转化成一个<code>json</code>对象。然后直接通过字典的操作，获得文章的<code>url</code>地址。这里使用<code>yield</code>，每次请求返回一个<code>url</code>，降低内存的消耗。由于我在后面抓取的时候出跳出一个<code>json</code>解析的错误，这里直接过滤就好。</p><p>这里我们可以试着打印看看，是不是成功运行。</p><p>既然获得了文章的<code>url</code>，那么对于获得文章的数据就显得很简单了。这里不在进行详细的叙述。我们的目标是获得文章的标题，作者和内容。<br>由于有的文章里面包含一些图片，我们直接过滤掉文章内容里的图片就好了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line"><span class="comment"># 解析文章页</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(text)</span>:</span></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        soup = BeautifulSoup(text, <span class="string">'lxml'</span>)</div><div class="line">        content = soup.find(<span class="string">'div'</span>, class_=<span class="string">"content"</span>)</div><div class="line">        title = content.find(<span class="string">'h1'</span>, id=<span class="string">"articleTitle"</span>).get_text()</div><div class="line">        author = content.find(<span class="string">'div'</span>, class_=<span class="string">"content-th-info"</span>).find(<span class="string">'a'</span>).get_text()</div><div class="line">        article_content = content.find(<span class="string">'div'</span>, class_=<span class="string">"document"</span>).find_all(<span class="string">'p'</span>)</div><div class="line">        all_p = [i.get_text() <span class="keyword">for</span> i <span class="keyword">in</span> article_content <span class="keyword">if</span> <span class="keyword">not</span> i.find(<span class="string">'img'</span>) <span class="keyword">and</span> <span class="keyword">not</span> i.find(<span class="string">'a'</span>)]</div><div class="line">        article = <span class="string">'\n'</span>.join(all_p)</div><div class="line">        <span class="comment"># print(title,'\n',author,'\n',article)</span></div><div class="line">        data = &#123;</div><div class="line">            <span class="string">'title'</span>: title,</div><div class="line">            <span class="string">'author'</span>: author,</div><div class="line">            <span class="string">'article'</span>: article</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> data</div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure><p>这里在进行多进程抓取的时候，<code>BeautifulSoup</code>也会出现一个错误，依然直接过滤。我们把得到的数据保存为字典的形式，方便保存数据库。</p><p>接下来就是保存数据库的操作了，这里我们使用<code>Mongodb</code>进行数据的存储。具体的方法在上一篇文章里有说过。不在对他进行详细叙述。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"><span class="keyword">from</span> config <span class="keyword">import</span> *</div><div class="line"></div><div class="line">client = pymongo.MongoClient(MONGO_URL, <span class="number">27017</span>)</div><div class="line">db = client[MONGO_DB]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_database</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="keyword">if</span> db[MONGO_TABLE].insert(data):</div><div class="line">        print(<span class="string">'Save to Database successful'</span>, data)</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">False</span></div></pre></td></tr></table></figure><p>我们把数据库的名字，和表名保存到<code>config</code>配置文件中，在把配置信息导入文件，这样会方便代码的管理。</p><p>最后呢，由于果壳网数据还是比较多的，如果想要大量的抓取，我们可以使用多进程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"></div><div class="line"><span class="comment"># 定义一个主函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(offset)</span>:</span></div><div class="line">    text = get_index(offset)</div><div class="line">    all_url = parse_json(text)</div><div class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> all_url:</div><div class="line">        resp = get_page(url)</div><div class="line">        data = parse_page(resp)</div><div class="line">        <span class="keyword">if</span> data:</div><div class="line">            save_database(data)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    pool = Pool()</div><div class="line">    offsets = ([<span class="number">0</span>] + [i*<span class="number">20</span>+<span class="number">18</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>)])</div><div class="line">    pool.map(main, offsets)</div><div class="line">    pool.close()</div><div class="line">    pool.join()</div></pre></td></tr></table></figure><p>函数的参数<code>offset</code>就是页数了。经过我的观察，果壳网最后一页页码是 12758，有 637 页。这里我们就抓取 500 页。进程池的<code>map</code>方法和<code>Python</code>内置的<code>map</code>方法使用类似。</p><p><img src="http://imgout.ph.126.net/56710040/guoke5.jpg" alt="image"></p><p>好了，对于一些使用<code>Ajax</code>加载的网页，我们就可以这么抓取了。</p><h2 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h2><p><a href="https://github.com/Blackyukun/GuoKe" target="_blank" rel="external">here</a></p><p>如果觉得有帮助，不妨<strong>star</strong>。</p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章我们来研究一下怎么分析网页的&lt;code&gt;Ajax&lt;/code&gt;请求。&lt;/p&gt;
&lt;p&gt;我们在平时爬取网页的时候，可能都遇到过有些网页直接请求得到的 HTML 代码里面，并没有我们需要的数据，也就是我们在浏览器中看到的内容。&lt;/p&gt;
&lt;p&gt;这就是因为这些信息是通过&lt;code&gt;Ajax&lt;/code&gt;加载的，并且通过&lt;code&gt;js&lt;/code&gt;渲染生成的。这个时候我们就需要分析这个网页的请求了。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>一切都会好的</title>
    <link href="https://wyukun.coding.me/2017/07/12/%E4%B8%80%E5%88%87%E9%83%BD%E4%BC%9A%E5%A5%BD%E7%9A%84/"/>
    <id>https://wyukun.coding.me/2017/07/12/一切都会好的/</id>
    <published>2017-07-12T08:32:01.000Z</published>
    <updated>2017-09-16T08:38:10.813Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>博客好久没更新了，但是最近真的有很多话想说，但是又不知如何表达才显得自然。虽然有时候会想反正也没有几个人看，但是话到键盘上，又不知该说什么。这种只可让人意会，而不可言传的感觉真是憋人。</p><p>我的大三在十天前结束，转眼就要大四。三年来一事无成，现在想想都觉得后背发凉。这个后背发凉是真实的感受，毕竟气温30多度，感觉到凉是我对自己迷雾般的未来的担忧。<br><a id="more"></a></p><p>今天早晨163muc给我推荐了一首歌，听着觉得音乐很是伤感，但是没人歌唱，想着是纯音乐就点进去看看什么名字。但是空空的歌词面板却有一句歌词。</p><blockquote><p>一切都会好的，城南的花都开了。</p></blockquote><p>当时窗外是烈日炎炎，远处那颗高高的树上知了无忧无虑的叫。不知道是被这个歌词触动了，还是被音乐的旋律触动了，我只感觉鼻子莫名的酸。饶有兴趣的点开评论区，才知道这音乐是作者写给一个骨癌晚期还有半年时间的听友的。城南花已开是他的网易云id。出于好奇，开始一条一条的翻看评论。作者3.26发布的音乐，城南花已开这个 id 从 3.26 开始一直更新他的动态，回复这评论区里的问候。4.10 日是我翻到的这个 id 最后一条评论，没有说身体状况，直说他很感谢他的父母。</p><p>现在7月份了，不知道他现在怎么样，评论去里每天都有人出来问君是否安好。伤感又充满希望的旋律，总会让人心生感伤，每个人都留着每个人的故事，但是再也没有他的回复。</p><h2 id="音乐分享"><a href="#音乐分享" class="headerlink" title="音乐分享"></a>音乐分享</h2><p>由于网易云版权问题，外链不支持。所以只能分享下他的链接。</p><p><a href="http://music.163.com/#/song?id=468176711" target="_blank" rel="external">城南花已开</a></p><h2 id="Simpleblog"><a href="#Simpleblog" class="headerlink" title="Simpleblog"></a>Simpleblog</h2><p>最近，利用空余的时间写了个社交型网站。有兴趣的可以前往看看，由于没有着力去做移动端页面，所以建议不要用手机点开。</p><p>网站是模仿简书的风格，追求一种简单但是功能完备的优雅。适用于多人创作交流，界面清晰自然的社交型网站。</p><h3 id="大致功能"><a href="#大致功能" class="headerlink" title="大致功能"></a>大致功能</h3><ul><li>管理员功能（包含管理文章，文章评论，添加网站公告等）；</li><li>用户设置个人头像、资料，相互关注；</li><li>发布文章（支持markdown），评论交流；</li><li>文章点赞；</li><li>评论，点赞，关注消息显示；</li><li>站内私信；</li><li>…</li></ul><h3 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h3><p><a href="https://github.com/Blackyukun/Simpleblog" target="_blank" rel="external">here</a>，喜欢不妨 <strong>star</strong></p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;博客好久没更新了，但是最近真的有很多话想说，但是又不知如何表达才显得自然。虽然有时候会想反正也没有几个人看，但是话到键盘上，又不知该说什么。这种只可让人意会，而不可言传的感觉真是憋人。&lt;/p&gt;
&lt;p&gt;我的大三在十天前结束，转眼就要大四。三年来一事无成，现在想想都觉得后背发凉。这个后背发凉是真实的感受，毕竟气温30多度，感觉到凉是我对自己迷雾般的未来的担忧。&lt;br&gt;
    
    </summary>
    
      <category term="音乐" scheme="https://wyukun.coding.me/categories/%E9%9F%B3%E4%B9%90/"/>
    
    
      <category term="随笔" scheme="https://wyukun.coding.me/tags/%E9%9A%8F%E7%AC%94/"/>
    
      <category term="音乐" scheme="https://wyukun.coding.me/tags/%E9%9F%B3%E4%B9%90/"/>
    
  </entry>
  
  <entry>
    <title>忙忙碌碌</title>
    <link href="https://wyukun.coding.me/2017/06/20/%E5%BF%99%E5%BF%99%E7%A2%8C%E7%A2%8C/"/>
    <id>https://wyukun.coding.me/2017/06/20/忙忙碌碌/</id>
    <published>2017-06-20T08:28:36.000Z</published>
    <updated>2017-09-16T08:31:29.572Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>临近学期末，各项考试也陆陆续续开始了。忙完这门课的作业，接着忙那门课的复习。可是忙不单行，科目二考试就定在6月19日，两边都很是考试，只忙一边，另一边肯定会生变故。如果两边一起忙，那是必然不可能的。</p><p>因为驾校离学校有40分钟的公交路程（最快最方便），如果骑ofo单车大约35分钟，如果步行的话，时间待实践。<br><a id="more"></a></p><p><img src="http://imgout.ph.126.net/55908028/sun-2256571_960_720.jpg" alt="image"></p><p>18号去考场模拟考试，和室友坐地铁到考场附近，骑着单车，看着手机导航。可能天气太热，网络都很困乏。按着导航骑了半天，确认位置，发现我们早已偏离方向。室友悲愤不已，盯着手机屏幕，大有一种高举之，猛掷之的冲动。</p><p>索性我们发现的及时，找对方向骑了将近10分钟，又不知路在何方。室友见前方有加油站，便上前询问，师傅摇摇头，表示并不知道附近有什么车管所什么科目二考场。</p><p>由于时间紧迫，我们很是焦躁，加上天气炎热，路边卡车叫的响亮。一个室友暗叹一声掉头准备返回，没走多久便在一个路口喊我们过去。我们骑过去见他表情古怪，然后便看到路口一个大大的蓝色路牌，一个大大的箭头指向我们的目的地。</p><p>都说好事多磨，以为赶到考场能立即进去模拟，然后成绩不错，明天考试轻松通过。可现实总是很残酷，考场等待的人真是奇多。等到我们上车模拟时早已是傍晚时分，教练看了我一眼，问我吃饭了没，我说没有，他说那赶紧上车。我大为感动，以为遇上一个好教练，能给我一个好一点的模拟体验。</p><p>我上车，教练见我准备好后，匆匆扔掉手里没抽几口的烟，暗示我速度开始。我心领神会，开始考试项目。可能是不熟悉这辆车，也可能是考场外的大卡车喇叭声过于刺耳。刚准备我的表演，机器就报我左车轮压到黄线。由于是刚开始，犯错犯得太快，我很是惭愧望向教练，以为他会很愤怒，然后告诉我出错原因，教导我如何规避这样的错误。但我只看到他若有若无的笑，他似乎感觉到我在望他，脸色瞬间变得严肃，然后问我吃完饭没，催促我赶紧继续下一个项目。我甚是感动，这教练不仅不会骂我，还很关怀的一直问我吃没吃饭。</p><p>后面的模拟不出意外的犯了很多错，教练不出意外的多次提醒我吃晚饭。模拟结束，情况很是糟糕，感觉明天的考试败券在握。临走时教练坐在车内略带同情的对我说情况不怎么样建议我继续交钱模拟，顺带告诉我他去吃晚饭了。<br>晚上难以入睡，如果考试不通过，那这几天的太阳就白晒了。一想到我这熊猫胳膊就很是伤感。和室友交流一会，心中暗示自己模拟不好只不过是当时心情糟糕，卡车的喇叭声太吵。明天只要按照平常的练习肯定能过。</p><p>19号正常考试，起床特意穿了平时练车穿的鞋子。可能是鞋子真的很舒服，也可能是早晨喝的脉动让我找回自己，考试异乎寻常的顺利。走出考场后长舒一口气，感觉很是轻松。觉得南京的天空异常的蓝，对着考生指指点点的考官特别的和蔼，一个个教练生气的对着自己学员说话，都在询问他们吃没吃午饭。</p><p>虽然科目二的考试通过，但是车技并不会有多厉害。不过有付出必然有回报，我现在的自行车技术倒是一流。也算是收获。</p><p><strong>哈哈</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;临近学期末，各项考试也陆陆续续开始了。忙完这门课的作业，接着忙那门课的复习。可是忙不单行，科目二考试就定在6月19日，两边都很是考试，只忙一边，另一边肯定会生变故。如果两边一起忙，那是必然不可能的。&lt;/p&gt;
&lt;p&gt;因为驾校离学校有40分钟的公交路程（最快最方便），如果骑ofo单车大约35分钟，如果步行的话，时间待实践。&lt;br&gt;
    
    </summary>
    
      <category term="生活" scheme="https://wyukun.coding.me/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="随笔" scheme="https://wyukun.coding.me/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>返老还童</title>
    <link href="https://wyukun.coding.me/2017/06/15/%E8%BF%94%E8%80%81%E8%BF%98%E7%AB%A5/"/>
    <id>https://wyukun.coding.me/2017/06/15/返老还童/</id>
    <published>2017-06-15T08:15:42.000Z</published>
    <updated>2017-09-16T08:23:59.373Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>昨天晚上在知乎上看到 <a href="https://www.zhihu.com/question/53487942" target="_blank" rel="external">你听过最有力量的歌词有哪些？</a></p><p>看了第一条回答，就很喜欢这句台词，于是找了这部电影，静静的看了一下午。影片名《本杰明.巴顿奇事》，又叫《返老还童》。</p><blockquote><p>“一件事无论太晚或者对于我来说太早，都不会阻拦你成为你想成为的那个人，这个过程没有时间的期限，只要你想，随时都可以开始，要改变或者保留原状都无所谓，做事本不应该有所束缚，我们可以办好这件事却也可以把它搞砸，但我希望最终你能成为你想成为的人。我希望你有时能驻足于这个令你感到惊叹的世界，体会你从未有过的感觉。我希望你能见到其他与你观点不同的人们。我希望你能有一个值得自豪的人生。如果你想象的生活不一样，我希望你能有勇气重新启程。”</p></blockquote><a id="more"></a><p>这是本杰明写给她女儿的话。这个时候他已经饱经人生变迁，可身型却已经是少年模样。他本可以在花甲之年和他的爱人和自己的女儿幸福生活，但是他明白女儿需要的是 一个父亲，而不是玩伴。此时的他外表是一个青年。站在窗外望着黛西和女儿，丝丝无奈，很是孤独。在生命的最后日子里，他已经老年痴呆，不记得任何人任何事。他不喜欢别人碰他，他不相信养老院的人，但对他的爱人黛西格外信任。他终于变成了婴儿，躺在黛西怀里安然睡去。</p><p>这本是两条不该相交的平行线，却在命运之神的手上，奇迹的相遇。孩童时期的本杰明，只能像养老院里普通的老人一样，坐在轮椅里，望着天空。享受一天一天的时光。十几岁的本杰明第一次遇到黛西，就在也忘不掉她蓝色的眼睛。本应该是青梅竹马的相遇，却让他们成了忘年之交。他们第一次在床下谈话，被戴西的奶奶拉走。本杰明第一次明白，有些人是不能够被人们理解的。</p><p>后来他遇到了改变它一生的迈克船长，他带他嫖妓，带他成为海军。他目睹了战争的残酷，1328条生命死去。迈克船长死去的时候看着自己被子弹打穿的身体，气愤的说敌人破坏了他的艺术纹身。他看着远远的大海，目光逐渐暗淡，说：“有时事与愿违，你大可以像疯狗一样咆哮，怨天尤人，诅咒命运，悔不当初……但走到最后一刻，还是不得不安静地放手归去…… ”。</p><p>而此时的黛西正值事业巅峰，没经历过挫折，挥霍青春。而中年的本杰明，从二战中回到家乡无法融入她青春的激流中，默默离去。“生命并不是如此复杂，要是你喜欢，大可以说你是在探索生命。”</p><p>而命运总是有很多意外，如果不是黛西朋友的鞋带没断，如果那辆货车早些开走，女士的包裹早早准备好，男孩和女孩没有分手，男人定了闹钟和平时一样起床，司机没有停车喝咖啡。。。黛西就不会出车祸，还在继续跳舞。康复后的黛西找到本杰明，过了幸福的生活。而女儿的出生打破了平静的生活。已是少年模样的本杰明选择离开。而之后的黛西也有了新的丈夫。</p><blockquote><p>“时间是1985年4月4日  地点是新奥尔良.这是我最后的遗言，也是遗嘱。我没留下什么，没财产，没钱，真的。我孤独的来，孤独的去。生不带来，死不带去。我仅有的就是我的故事，我现在就写着，趁我还记得的时候。”</p></blockquote><p>这是本杰明迟暮之年写下的日记开篇。</p><p>但是正如他自己说的一般。他的出生不寻常，在一战结束的那晚，他的母亲难产去世。他的父亲觉得他是怪物将他抛弃。好在被善良的黑人女子奎尼收养。刚出生的本杰明相当于80岁老人，身体蹉跎干枯，医生说他没有多久时日。</p><p>影片并不是本杰明返老还童的无聊历程，而是通过战争，亲情，爱情，这些普通的方面，以回忆的形式讲述出来。1918年火车站建成，请来了最好的钟表匠盖图先生。因为战争失去儿子的盖图先生造出了一个倒着走的大钟，他说他希望所有的不幸全都消散，战场上死了的儿子可以回到家乡耕种，安度余生。但是这并不可能实现，倒着走的大钟继续走着，而该老去的依旧老去，该死去的依旧要死去。</p><p>影片中道斯先生曾被雷电劈七次，他说：“当我瞎了一个眼睛，几乎听不见晨间喧嚣，动不动就抽搐，总是丢三落四，上帝却依然扔下闪电来提醒我，能够活着已经是桩幸运的事了”。</p><p>或许你在生活中不善交流与别人格格不入，但是我们在空余的时间里一个人读读书，听听音乐。可怕的不是孤独，而是惧怕孤独。其实孤独没什么不好，至少你会有大把时间去满足自己的好奇心，去丰富自己的生活。世间没有什么是永恒的，遇到在困难的事都是会过去的，我们要知道生活本就是奇迹，没人知道明天会发生什么。</p><p>打开门窗，外面一片阳光。</p><blockquote><p>-Goodnight, Daisy.<br>-Goodnight, Benjamin.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;昨天晚上在知乎上看到 &lt;a href=&quot;https://www.zhihu.com/question/53487942&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;你听过最有力量的歌词有哪些？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;看了第一条回答，就很喜欢这句台词，于是找了这部电影，静静的看了一下午。影片名《本杰明.巴顿奇事》，又叫《返老还童》。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“一件事无论太晚或者对于我来说太早，都不会阻拦你成为你想成为的那个人，这个过程没有时间的期限，只要你想，随时都可以开始，要改变或者保留原状都无所谓，做事本不应该有所束缚，我们可以办好这件事却也可以把它搞砸，但我希望最终你能成为你想成为的人。我希望你有时能驻足于这个令你感到惊叹的世界，体会你从未有过的感觉。我希望你能见到其他与你观点不同的人们。我希望你能有一个值得自豪的人生。如果你想象的生活不一样，我希望你能有勇气重新启程。”&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="电影" scheme="https://wyukun.coding.me/categories/%E7%94%B5%E5%BD%B1/"/>
    
    
      <category term="随笔" scheme="https://wyukun.coding.me/tags/%E9%9A%8F%E7%AC%94/"/>
    
      <category term="电影" scheme="https://wyukun.coding.me/tags/%E7%94%B5%E5%BD%B1/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(7):多进程抓取拉钩网十万数据</title>
    <link href="https://wyukun.coding.me/2017/06/12/Python%E7%88%AC%E8%99%AB%E4%B8%83%E4%B9%8B%E5%A4%9A%E8%BF%9B%E7%A8%8B%E6%8A%93%E5%8F%96%E6%8B%89%E9%92%A9%E7%BD%91%E5%8D%81%E4%B8%87%E6%95%B0%E6%8D%AE/"/>
    <id>https://wyukun.coding.me/2017/06/12/Python爬虫七之多进程抓取拉钩网十万数据/</id>
    <published>2017-06-12T07:50:39.000Z</published>
    <updated>2017-09-16T08:16:27.501Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><blockquote><p>由于拉钩网页面做了一些更新，之前的程序无法正常工作，本篇文章做一次更新。只更新一些程序和一些程序的实现方法。由于没有仔细修改，可能前后语言不通顺，大家谅解。</p></blockquote><p>大家好，几天没有更新了。相信大家经过前两篇的练手爬虫，大家已经知道如何抓取一个简单网站。</p><p>这篇文章我们来抓取 <a href="https://www.lagou.com/" target="_blank" rel="external">拉钩网</a> 的招聘信息。全部抓取大概十几万条全国招聘信息，并且保存数据库。<br><a id="more"></a></p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><h3 id="安装Mongodb数据库"><a href="#安装Mongodb数据库" class="headerlink" title="安装Mongodb数据库"></a>安装Mongodb数据库</h3><p>其实不是一定要使用<code>MongoDB</code>，大家完全可以使用<code>MySQL</code>或者<code>Redis</code>，全看大家喜好。这篇文章我们的例子是<code>Mongodb</code>，所以大家需要 <a href="https://www.mongodb.com/" target="_blank" rel="external">下载</a> 它。</p><p>在<code>Windows</code>中。由于<code>MongoDB</code>默认的数据目录为<code>C:\data\db</code>，建议大家直接在安装的时候更改默认路径为<code>C:\MongoDB</code>.</p><p>然后创建如下目录文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">C:\data\log\mongod.log  //用于存储数据库的日志</div><div class="line">C:\data\db    //用于存储数据库数据</div></pre></td></tr></table></figure><p>然后在<code>C:\MongoDB</code>文件夹下（安装 Mongodb 路径）创建配置文件<code>mongod.cfg</code>。并且在配置文件里写入以下配置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">systemLog:</div><div class="line"> destination: file</div><div class="line"> path: C:\data\log\mongod.log</div><div class="line">storage:</div><div class="line"> dbPath: C:\data\db</div></pre></td></tr></table></figure><p>大家记住要打开文件后缀名，不然我们可能创建了一个<code>mongod.cfg.txt</code>文件。</p><p>最后我们需要打开管理员权限的 CMD 窗口，执行如下命令，安装数据库成服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&quot;C:\mongodb\bin\mongod.exe&quot; --config &quot;C:\mongodb\mongod.cfg&quot; --install</div></pre></td></tr></table></figure><p>设置为服务后，需要在管理员权限打开的<code>windows cmd</code>窗口用服务的方式启动或停止<code>MongoDB</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">net start mongodb    //启动mongodb服务</div><div class="line">net stop mongodb     //关闭mongodb服务</div></pre></td></tr></table></figure><p>好了，安装好<code>Mongodb</code>数据库后，我们需要安装<code>PyMongo</code>，它是<code>MongoDB</code>的<code>Python</code>接口开发包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install pymongo</div></pre></td></tr></table></figure><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><p>准备完成后，我们就开始浏览拉勾网。我们可以发现拉勾网所有的招聘职位都在左侧分类里。如图：</p><p><img src="http://imgout.ph.126.net/56100044/lagou1.jpg" alt="image"></p><p>我们先获取首页HTML文件:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</div><div class="line"></div><div class="line">url = <span class="string">'https://www.lagou.com/'</span></div><div class="line"><span class="comment"># 获取页面源码函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page_resp</span><span class="params">(url)</span>:</span></div><div class="line">    headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'</span>&#125;</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        resp = requests.get(url, headers=headers)</div><div class="line">        <span class="keyword">if</span> resp.status_code == <span class="number">200</span>:</div><div class="line">            <span class="keyword">return</span> resp.text</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span></div><div class="line">    <span class="keyword">except</span> RequestException:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span></div><div class="line"></div><div class="line">soup = BeautifulSoup(get_page_resp(url), <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure><p>然后我们打开开发者工具，找到招聘职业的位置。</p><p><img src="http://imgout.ph.126.net/56111019/lagou2.jpg" alt="image"></p><p>大家还记得BeautifulSoup的CSS选择器吧，我们直接使用<code>.select()</code>方法获取标签信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">all_positions = soup.select(<span class="string">'div.menu_sub.dn &gt; dl &gt; dd &gt; a'</span>)</div><div class="line">print(all_positions)</div><div class="line">print(len(all_positions))</div></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[&lt;a class="curr" href="https://www.lagou.com/zhaopin/Java/" data-lg-tj-cid="idnull" data-lg-tj-id="4O00" data-lg-tj-no="0101"&gt;Java&lt;/a&gt;, &lt;a class="curr" href="https://www.lagou.com/zhaopin/C%2B%2B/" data-lg-tj-cid="idnull" data-lg-tj-id="4O00" data-lg-tj-no="0102"&gt;C++&lt;/a&gt;, # ... 省略部分 &lt;a class="" href="https://www.lagou.com/zhaopin/fengxiankongzhizongjian/" data-lg-tj-cid="idnull" data-lg-tj-id="4U00" data-lg-tj-no="0404"&gt;风控总监&lt;/a&gt;, &lt;a class="" href="https://www.lagou.com/zhaopin/zongcaifuzongcai/" data-lg-tj-cid="idnull" data-lg-tj-id="4U00" data-lg-tj-no="0405"&gt;副总裁&lt;/a&gt;] </div><div class="line"><span class="number">260</span></div></pre></td></tr></table></figure><p>获取到所有职位标签的<code>a</code>标签后，我们只需要提取标签的<code>href</code>属性和标签内内容，就可以获得到职位的招聘链接和招聘职位的名称了。我们准备信息生成一个字典。方便我们后续程序的调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 解析首页获得所有职位信息的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">()</span>:</span></div><div class="line">    url = <span class="string">'https://www.lagou.com/'</span></div><div class="line">    soup = BeautifulSoup(get_html(url), <span class="string">'lxml'</span>)</div><div class="line">    all_positions = soup.select(<span class="string">'div.menu_sub.dn &gt; dl &gt; dd &gt; a'</span>)</div><div class="line">    joburls = [i[<span class="string">'href'</span>] <span class="keyword">for</span> i <span class="keyword">in</span> all_positions]</div><div class="line">    jobnames = [i.get_text() <span class="keyword">for</span> i <span class="keyword">in</span> all_positions]</div><div class="line"></div><div class="line">    <span class="keyword">for</span> joburl, jobname <span class="keyword">in</span> zip(joburls, jobnames):</div><div class="line">        data = &#123;</div><div class="line">            <span class="string">'url'</span> : joburl,</div><div class="line">            <span class="string">'name'</span> : jobname</div><div class="line">        &#125;</div><div class="line">        <span class="comment"># 这里使用yield语法糖，不熟悉的同学自己查看资料哦</span></div><div class="line">        <span class="keyword">yield</span> data</div></pre></td></tr></table></figure><p>这里我们用<code>zip</code>函数，同时迭代两个<code>list</code>。生成一个键值对。</p><p>接下来我们可以随意点击一个职位分类，分析招聘页面的信息。</p><h2 id="分页"><a href="#分页" class="headerlink" title="分页"></a>分页</h2><p>我们首先来分析下网站页数信息。经过我的观察，每个职位的招聘信息最多不超过 30 页。也就是说，我们只要从第 1 页循环到第 30 页，就可以得到所有招聘信息了。但是也可以看到有的职位招聘信息，页数并不到 30 页。以下图为例：</p><p><img src="http://imgout.ph.126.net/56111020/lagou3.jpg" alt="image"></p><p>如果我们访问页面：<code>https://www.lagou.com/zhaopin/Java/31/</code></p><p>也就是第 31 页。我们会得到 404 页面。所以我们需要在访问到404页面时进行过滤。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> resp.status_code == <span class="number">404</span>:</div><div class="line">    <span class="keyword">pass</span></div></pre></td></tr></table></figure><p>这样我们就可以放心的 30 页循环获得每一页招聘信息了。</p><p>我们的每一页<code>url</code>使用<code>format</code>拼接出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">link = <span class="string">'&#123;&#125;&#123;&#125;/'</span>.format(url, str(pages))</div></pre></td></tr></table></figure><h2 id="获取信息"><a href="#获取信息" class="headerlink" title="获取信息"></a>获取信息</h2><p><img src="http://imgout.ph.126.net/56111021/lagou4.jpg" alt="image"></p><p>我们可以看到上面划线的信息。这就是我们要抓取的信息了。</p><p>当然。抓取的方法千篇一律，我们可以使用<code>find()</code>或<code>find_all()</code></p><p>选择器，当然也可以使用 CSS选择器。但是 CSS选择器相对于前两者代码量稍微少一些。这里大家自己动手抓取，我们直接上代码供大家借鉴。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 职位信息 </span></div><div class="line">positions = soup.select(<span class="string">'ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_top &gt; a &gt; h3'</span>) </div><div class="line"><span class="comment"># 工作地址 </span></div><div class="line">adds = soup.select(<span class="string">'ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_top &gt; a &gt; span &gt; em'</span>) </div><div class="line"><span class="comment"># 发布时间 </span></div><div class="line">publishs = soup.select(<span class="string">'ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_top &gt; span'</span>) </div><div class="line"><span class="comment"># 薪资信息</span></div><div class="line"> moneys = soup.select(<span class="string">'ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_bot &gt; div &gt; span'</span>) </div><div class="line"><span class="comment"># 工作需求 </span></div><div class="line">needs = soup.select(<span class="string">'ul &gt; li &gt; div.list_item_top &gt; div.position &gt; div.p_bot &gt; div'</span>) </div><div class="line"><span class="comment"># 发布公司 </span></div><div class="line">companys = soup.select(<span class="string">'ul &gt; li &gt; div.list_item_top &gt; div.company &gt; div.company_name &gt; a'</span>) </div><div class="line">tags = [] </div><div class="line"><span class="comment"># 由于我发现有的招聘信息没有标签信息，if判断防止没有标签报错 </span></div><div class="line"><span class="keyword">if</span> soup.find(<span class="string">'div'</span>, class_=<span class="string">'li_b_l'</span>): </div><div class="line">    <span class="comment"># 招聘信息标签 </span></div><div class="line">    tags = soup.select(<span class="string">'ul &gt; li &gt; div.list_item_bot &gt; div.li_b_l'</span>) </div><div class="line"><span class="comment"># 公司福利 </span></div><div class="line">fulis = soup.select(<span class="string">'ul &gt; li &gt; div.list_item_bot &gt; div.li_b_r'</span>)</div></pre></td></tr></table></figure><p>获取到全部信息后，我们同样的把他们组成键值对字典。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> position,add,publish,money,need,company,tag,fuli <span class="keyword">in</span> \</div><div class="line">        zip(positions,adds,publishs,moneys,needs,companys,tags,fulis):</div><div class="line">    data = &#123;</div><div class="line">        <span class="string">'position'</span> : position.get_text(),</div><div class="line">        <span class="string">'add'</span> : add.get_text(),</div><div class="line">        <span class="string">'publish'</span> : publish.get_text(),</div><div class="line">        <span class="string">'money'</span> : money.get_text(),</div><div class="line">        <span class="string">'need'</span> : need.get_text().split(<span class="string">'\n'</span>)[<span class="number">2</span>],</div><div class="line">        <span class="string">'company'</span> : company.get_text(),</div><div class="line">        <span class="string">'tag'</span> : tag.get_text().replace(<span class="string">'\n'</span>,<span class="string">'-'</span>),</div><div class="line">        <span class="string">'fuli'</span> : fuli.get_text()</div><div class="line">    &#125;</div></pre></td></tr></table></figure><p>组成字典的目的是方便我们将全部信息保存到数据库。</p><h2 id="保存数据库"><a href="#保存数据库" class="headerlink" title="保存数据库"></a>保存数据库</h2><p>保存数据库前我们需要配置数据库信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</div><div class="line">lagou = client[<span class="string">'lagou'</span>]</div><div class="line">url_list = lagou[<span class="string">'url_list'</span>]</div></pre></td></tr></table></figure><p>这里我们导入了<code>pymongo</code>库，并且与<code>MongoDB</code>建立连接，这里是默认连接本地的<code>MongoDB</code>数据。创建并选择一个数据库<code>lagou</code>，并在这个数据库中，创建一个<code>table</code>，即<code>url_list</code>。然后，我们进行数据的保存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> url_list.insert_one(data):</div><div class="line">    print(<span class="string">'保存数据库成功'</span>, data)</div></pre></td></tr></table></figure><p>如果保存成功，打印出成功信息。</p><h2 id="多进程抓取"><a href="#多进程抓取" class="headerlink" title="多进程抓取"></a>多进程抓取</h2><p>十万多条数据是不是抓取的有点慢，有办法，我们使用多进程同时抓取。由于<code>Python</code>的历史遗留问题，多线程在<code>Python</code>中始终是个美丽的梦。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(pages)</span>:</span></div><div class="line">    datas = parse_index()</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> datas:</div><div class="line">        url = i[<span class="string">'url'</span>]</div><div class="line">        print(url)</div><div class="line">        mongo_table = i[<span class="string">'name'</span>]</div><div class="line">        <span class="comment"># 因为有的职位是以'.'开头的，比如.Net，数据库表名不能以.开头</span></div><div class="line">        <span class="keyword">if</span> mongo_table[<span class="number">0</span>] == <span class="string">'.'</span>:</div><div class="line">            mongo_table = mongo_table[<span class="number">1</span>:]</div><div class="line">        <span class="comment"># 我们把之前抓取职位所有招聘信息的程序整理为parse_link()函数</span></div><div class="line">        <span class="comment"># 这个函数接收职位url，页码，和数据库表名为参数</span></div><div class="line">        parse_link(url, pages, mongo_table)</div></pre></td></tr></table></figure><p>我们把之前提取职位招聘信息的代码，写成一个函数，方便我们调用。这里的<code>parse_link()</code>就是这个函数，他就收职位的 url 和所有页数为参数。我们<code>get_alllink_data()</code>函数里面使用<code>for</code>循环 30 页的数据。然后这个作为主函数传给多进程内部调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    pool = Pool(processes=<span class="number">6</span>)</div><div class="line">    pages = ([p <span class="keyword">for</span> p <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">31</span>)])</div><div class="line">    pool.map(main,pages)</div><div class="line">    pool.close() </div><div class="line">    pool.join()</div></pre></td></tr></table></figure><p>这里是一个<code>pool</code>进程池，我们调用进程池的<code>map</code>方法.</p><p><code>map(func, iterable[,chunksize=None])</code></p><p>多进程<code>Pool</code>类中的<code>map</code>方法，与<code>Python</code>内置的<code>map</code>函数用法行为基本一致。它会使进程阻塞，直到返回结果。需要注意，虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。<code>join()</code></p><p>方法等待子进程结束后再继续往下运行，通常用于进程间的同步.</p><h2 id="针对反爬"><a href="#针对反爬" class="headerlink" title="针对反爬"></a>针对反爬</h2><p>如果大家就这样整理完代码，直接就开始抓取的话。相信在抓取的不久后就会出现程序中止不走了。我刚刚第一次中止后，我以为是网站限制了我的 ip。于是我做了如下改动。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">user_agent_list = [</div><div class="line">    <span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'</span>,</div><div class="line">    <span class="string">' Mozilla/5.0 (Windows; U; Windows NT 5.2) Gecko/2008070208 Firefox/3.0.1'</span>,</div><div class="line">    <span class="string">'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.2)'</span></div><div class="line">]</div><div class="line">user_agent = random.choice(user_agent_list)</div><div class="line">headers  = &#123;</div><div class="line">    <span class="string">'User-Agent'</span>:user_agent,</div><div class="line">    <span class="string">'Connection'</span>:<span class="string">'keep-alive'</span></div><div class="line">&#125;</div><div class="line">proxy_list = [</div><div class="line">    <span class="string">'http://140.224.76.21:808'</span>,</div><div class="line">    <span class="string">'http://60.178.14.90:8081'</span>,</div><div class="line">    <span class="string">'http://121.232.146.13:9000'</span>,</div><div class="line">    ]</div><div class="line">proxy_ip = random.choice(proxy_list)</div><div class="line">proxies = &#123;<span class="string">'http'</span>: proxy_ip&#125;</div><div class="line">resp = requests.get(url, headers=headers, proxies=proxies)</div><div class="line">time.sleep(<span class="number">1</span>)</div></pre></td></tr></table></figure><p>这里我是直接在在网上找了一些免费的<code>ip</code>，还自己找了几个浏览器的<code>user-agent</code>。利用<code>Python</code>内置的<code>random</code>库，开始随机选择列表里的<code>user-agent</code>和<code>ip</code>。并且将参数传入<code>requests</code>内继续抓取。为了防止请求频率过快，我们设置每次请求结束停留一秒。然后我以为问题这么结束了。就开始继续抓取，但是在之前中断的位置突然有中断了。</p><p>于是，我在代码抓取信息位置添加了一句打印<code>url</code>的代码。我得到了中断爬取的<code>url</code>，然后我手动多点进去，发现了这个网页。当然出现这个情况并不是更换<code>ip</code>不可以，而是我们的ip太少了，一个<code>ip</code>可能仍然出现多次抓取的情况，后续我将会和大家一起写一个自己的<code>ip</code>代理池。</p><p><img src="http://imgout.ph.126.net/56126036/lagou5.jpg" alt="image"></p><h2 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h2><p>没错这是一个登录界面，不知道是不是这里的浏览器头，或者ip请求过多。如果继续增多<code>ip</code>和<code>user_agent</code>就不会出现这种情况。</p><p>但是如何面对这种需要登录才能持续爬取的网站呢，很简单，我们只要打开浏览器的开发者工具。登录自己的账号，在<code>Network</code>标签找一个请求，查看你的请求头部信息，找到自己的<code>cookies</code>。这个<code>cookies</code>就是你的登录信息了，我们需要将他和你的<code>user-agent</code>一样，添加到请求头就可以了。如果大家不明白这个<code>cookies</code>是什么，没关系，后面会有专门的讲解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">headers = &#123;</div><div class="line">    <span class="string">'Cookie'</span>:<span class="string">'user_trace_token=20170603115043-d0c257a054ee44f99177a3540d44dda1; LGUID=20170603115044-d1e2b4d1-480f-11e7-96cf-525400f775ce; JSESSIONID=ABAAABAAAGHAABHAA8050BE2E1D33E6C2A80E370FE9167B; _gat=1; PRE_UTM=; PRE_HOST=; PRE_SITE=; PRE_LAND=https%3A%2F%2Fwww.lagou.com%2F; index_location_city=%E5%85%A8%E5%9B%BD; login=false; unick=""; _putrc=""; _ga=GA1.2.922290439.1496461627; X_HTTP_TOKEN=3876430f68ebc0ae0b8fac6c9f163d45; _ga=GA1.3.922290439.1496461627; LGSID=20170720174323-df1d6e50-6d2f-11e7-ac93-5254005c3644; LGRID=20170720174450-12fc5214-6d30-11e7-b32f-525400f775ce; Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1500541369; Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1500543655'</span>,</div><div class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>好了我们继续抓取，发现这次程序并没有中止。<del>~但是由于博主网速过慢，抓取太慢了，还没有全部抓取下来，不知道后面会不会出现问题。</del>~</p><p><strong>这次更新，我是完全抓取玩全站才过来的，没错我已经抓取了全部的信息。用时1412 .9008133411407 秒。</strong></p><p><img src="http://imgout.ph.126.net/56749148/lag.jpg" alt="image"></p><p>好了，说了这么多，基本也都说完了。</p><h2 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h2><p>对整理代码有疑惑的伙伴可以点击 <a href="https://github.com/Blackyukun/LaGou" target="_blank" rel="external">这里</a>。</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>这是我已经抓取的一部分。这里如果大家希望看到<code>Mongodb</code>数据库里的保存内容。</p><p>我们需要安装一个<code>Mongodb</code>可视化应用 <a href="https://robomongo.org/" target="_blank" rel="external">robomango</a>。</p><p>安装没什么要说的，大家链接到我们的<code>lagou</code>数据库，就可以看到里面的数据了。</p><p><img src="http://imgout.ph.126.net/56760100/laaa.jpg" alt="image"></p><p>如果博主后面把全部数据抓下来后，可以和大家一起进行数据分析，分析<code>Python</code>招聘的一些信息啊什么的。大家加油。</p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;由于拉钩网页面做了一些更新，之前的程序无法正常工作，本篇文章做一次更新。只更新一些程序和一些程序的实现方法。由于没有仔细修改，可能前后语言不通顺，大家谅解。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;大家好，几天没有更新了。相信大家经过前两篇的练手爬虫，大家已经知道如何抓取一个简单网站。&lt;/p&gt;
&lt;p&gt;这篇文章我们来抓取 &lt;a href=&quot;https://www.lagou.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;拉钩网&lt;/a&gt; 的招聘信息。全部抓取大概十几万条全国招聘信息，并且保存数据库。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(6):煎蛋网全站妹子图爬虫</title>
    <link href="https://wyukun.coding.me/2017/06/04/Python%E7%88%AC%E8%99%AB%E5%85%AD%E4%B9%8B%E7%85%8E%E8%9B%8B%E7%BD%91%E5%85%A8%E7%AB%99%E5%A6%B9%E5%AD%90%E5%9B%BE%E7%88%AC%E8%99%AB/"/>
    <id>https://wyukun.coding.me/2017/06/04/Python爬虫六之煎蛋网全站妹子图爬虫/</id>
    <published>2017-06-04T07:34:02.000Z</published>
    <updated>2017-09-16T07:48:50.764Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>上一篇文章中我们抓取了豆瓣图书的数据，如果大家运行成功，并且看到文件夹下的 txt 文件了。是不是有一种刚接触编程，第一次输出<code>Hello world!</code>时的欣喜。和上一篇实践不同，我们这一次来爬取 <a href="http://jandan.net/ooxx" target="_blank" rel="external">煎蛋网</a> 全站妹子图，并且保存到指定文件夹下。<br><a id="more"></a></p><h2 id="爬取流程"><a href="#爬取流程" class="headerlink" title="爬取流程"></a>爬取流程</h2><ul><li>从煎蛋网妹子图第一页开始抓取；</li><li>爬取分页标签获得最后一页数字；</li><li>根据最后一页页数，获得所有页<code>URL</code>；</li><li>迭代所有页，对页面所有妹子图片<code>url</code>进行抓取；访问图片<code>URL</code>并且保存图片到文件夹。</li></ul><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><p>通过上一篇文章的爬取过程，我们基本上理解了抓取一个网站的大致流程。因为一个网站虽然有很多页，但是大部分网站每一页的<code>HTML</code>标签内容都是相同的。我们只要获取到一页的内容，就可以获得所有页的内容了。那么开始之前，我们来分析一下煎蛋网妹子图页面的<code>URL</code>。</p><p>第一页的 url：<code>http://jandan.net/ooxx/page-1</code></p><p>第二页：<code>http://jandan.net/ooxx/page-2</code></p><p>最后一页：<code>http://jandan.net/ooxx/page-93</code></p><p>不难发现，煎蛋网的 url 的规律是比较简单的，每一页后面 page 的数字就是几。那么我们可以通过一个循环就可以获得所有的页面 URL 了。但是大家应该想到，这个网站每天都会更新，今天是 93 页，明天就会增加到94页了。如果每一都要爬一次的话，那么每次都要改一下代码页数信息了。这样实现起来虽然可以，但是不免有些愚蠢。</p><p>所以我们需要通过页面的标签信息让程序自己获得页数，我们访问<code>http://jandan.net/ooxx/</code>这个页面时，就相当于我们直接访问了最后一页。大家可以自己试试看。</p><p><img src="http://imgout.ph.126.net/55990007/jiandan.jpg" alt="image"></p><p>图中我们可以明确的看到最后一页的数字是94.只要通过这个页面的URL进行抓取就可以得到。我们先获得源码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line">url = <span class="string">'http://jandan.net/ooxx/'</span></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(url, headers=headers)</div><div class="line"></div><div class="line">soup = BeautifulSoup(resp.text, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure><p>我们按下<code>f12</code>，从页面源码中找到最后一页 94 所在的标签：</p><p><img src="http://imgout.ph.126.net/55994003/jiandan2.jpg" alt="image"></p><p>原来 94 就在这个<code>span</code>标签啊。接下来是不是很简单了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 获得最高页码数</span></div><div class="line">allpage = soup.find(<span class="string">'span'</span>, class_=<span class="string">"current-comment-page"</span>).get_text()[<span class="number">1</span>:<span class="number">-1</span>]</div></pre></td></tr></table></figure><p>由于标签内的 94 边包含一个<code>[]</code>，大家不要以为只是一个<code>list</code>，只要<code>[0]</code>就可以获得的。我们完全可以使用<code>type()</code>,看一下他的属性，就知道它是一个字符串，我们利用切片去掉第一个和最后一个字符，就得到了页数了。<br>得到页数后，我们利用循环就可以得到所有页的url了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">urllist = []</div><div class="line"><span class="comment"># for循环迭代出所有页面，得到url</span></div><div class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, int(allpage)+<span class="number">1</span>):</div><div class="line">    allurl = base_url + <span class="string">'page-'</span> + str(page)</div><div class="line">    urllist.append(allurl)</div></pre></td></tr></table></figure><p>我们把它保存到一个<code>list</code>中。<br>那么现在，我们得到所有页面的 url，就可以来获取每一页的内容了。我们以最后一页为例来进行抓取。</p><p><img src="http://imgout.ph.126.net/56001006/QQCDBCC6AC20170604142955.jpg" alt="image"></p><p>我们仍然使用审查元素，找到图片 url 所在的标签。仍然是老方法，获取到页面所有包含图片的<code>img</code>标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># css选择器</span></div><div class="line">allimgs = soup.select(<span class="string">'div.text &amp;gt; p &amp;gt; img'</span>)</div></pre></td></tr></table></figure><p>只要一行代码，我们就成功获得所有标签了。这里使用了 CSS选择器，大家是否还记得这个方法呢。可以查看之前的文章或者<code>BeautifulSoup</code>的官方文档了解哦。如果大家对 CSS 不是很熟悉，或者根本不知道。也没关系，反正<code>find_all()</code>和<code>find()</code>方法也是可以实现的。不过这里我教大家一个简单的CSS选择器方法。</p><p>我们只要按<code>f12</code>打开浏览器的开发者工具，找到标签的位置，右击标签。就可以看到这个情况：</p><p><img src="http://imgout.ph.126.net/55987008/select.jpg" alt="image"></p><p>没错，我们直接<code>Copy selector</code>的内容，粘贴出来就是这样的字符串：<code>#comment-3468457 &gt; div &gt; div &gt; div.text &gt; p &gt; img</code></p><p>我们稍微去掉前面的一些标签，大多数情况下保留到父标签后面的内容就可以了。就是这样：<code>div.text &gt; p &gt; img</code></p><p>我们放到代码中，运行一下就知道是不是成功了。</p><p>结果只一个列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[&lt;img onload=<span class="string">"add_img_loading_mask(this, load_sina_gif);"</span> org_src=<span class="string">"//wx4.sinaimg.cn/mw690/6adc108fly1fg90v6vzipg20ak05xqv5.gif"</span> src=<span class="string">"//wx4.sinaimg.cn/thumb180/6adc108fly1fg90v6vzipg20ak05xqv5.gif"</span>/&gt;, &lt;img onload=<span class="string">"add_img_loading_mask(this, load_sina_gif);"</span> org_src=<span class="string">"//wx4.sinaimg.cn/mw690/6adc108fly1fg90qymd8pg20dc0dcnph.gif"</span> src=<span class="string">"//wx4.sinaimg.cn/thumb180/6adc108fly1fg90qymd8pg20dc0dcnph.gif"</span>/&gt;, &lt;img onload=<span class="string">"add_img_loading_mask(this, load_sina_gif);"</span> org_src=<span class="string">"//wx4.sinaimg.cn/mw690/6adc108fly1fg90s98qsbg207e08mu10.gif"</span> src=<span class="string">"//wx4.sinaimg.cn/thumb180/6adc108fly1fg90s98qsbg207e08mu10.gif"</span>/&gt;, &lt;img onload=<span class="string">"add_img_loading_mask(this, load_sina_gif);"</span> org_src=<span class="string">"//wx3.sinaimg.cn/mw690/6adc108fly1fg90v51p5eg20a00dchdv.gif"</span> src=<span class="string">"//wx3.sinaimg.cn/thumb180/6adc108fly1fg90v51p5eg20a00dchdv.gif"</span>/&gt;, &lt;img src=<span class="string">"//wx4.sinaimg.cn/mw600/a1b56627gy1fdb7851js0j20ku4xcx6q.jpg"</span>/&gt;, &lt;img src=<span class="string">"//wx4.sinaimg.cn/mw600/a1b56627gy1fdb78fw14yj20dc46pb2a.jpg"</span>/&gt;, &lt;img src=<span class="string">"//wx3.sinaimg.cn/mw600/a1b56627gy1fdb78hqr1kj20dc1i0x02.jpg"</span>/&gt;, &lt;img src=<span class="string">"//wx4.sinaimg.cn/mw600/a1b56627gy1fdb78ldnwbj20ku3ei4qq.jpg"</span>/&gt;, &lt;img src=<span class="string">"//wx4.sinaimg.cn/mw600/a82b014bly1fg8gkj95toj21kw1kwh3s.jpg"</span>/&gt;, &lt;img src=<span class="string">"//wx4.sinaimg.cn/mw600/a82b014bly1fg8gkhmfraj21kw11xgth.jpg"</span>/&gt;, &lt;img src=<span class="string">"//wx2.sinaimg.cn/mw600/a82b014bly1fg8gklclivj21kw2dc1dg.jpg"</span>/&gt;, &lt;img src=<span class="string">"//wx2.sinaimg.cn/mw600/a82b014bly1fg8eohjekwj21421jkah5.jpg"</span>/&gt;]</div></pre></td></tr></table></figure><p>很显然，这一页的图片URL都在这里。<br>接下来就是提取img标签的src属性了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> img <span class="keyword">in</span> list:</div><div class="line">    urls = img[<span class="string">'src'</span>]</div><div class="line">    <span class="comment"># 判断url是否完整</span></div><div class="line">    <span class="keyword">if</span> urls[<span class="number">0</span>:<span class="number">5</span>] == <span class="string">'http:'</span>:</div><div class="line">        img_url = urls</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        img_url = <span class="string">'http:'</span> + urls</div></pre></td></tr></table></figure></p><p>由于有的标签内<code>url</code>并不完整，所以这里我们进行一次判断。如果不完整就给他补全。</p><p>好了，图片的 url 都获得了，接下来就是保存图片了。大家还记得之前我们介绍<code>Requests</code>模块的时候，有过保存图片的演示。<br>因为万维网中每个图片，每个视频都有唯一的 url 指向它们。所以我们只要访问这个 url，并且获得图片的二进制数据，保存到本地就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">imgs = requests.get(img_url，headers=headers)</div><div class="line">filename = img_url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</div><div class="line"><span class="comment"># 保存图片</span></div><div class="line"><span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">    <span class="comment"># 直接过滤掉保存失败的图片，不终止程序</span></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        f.write(imgs.content)</div><div class="line">        print(<span class="string">'Sucessful image:'</span>,filename)</div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        print(<span class="string">'Failed:'</span>,filename)</div></pre></td></tr></table></figure><p>大家注意，获取图片二进制数据是<code>.content</code>方法，而不是<code>.text</code>。这里我们有一个错误过滤，因为保存的过程中会有一些文件保存错误的情况，我们直接过滤掉，不终止程序的运行。</p><p>好了，爬虫程序到这里基本上已经全部实现了。但是我们如果把所有的图片存放在一个文件夹中，而且还是代码所在文件夹，不免有些难看。我们可以自己指定他们存放的位置。这里需要用的<code>Python</code>内置的<code>os</code>库了，不清楚的伙伴可以自己查看资料哈。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 创建文件夹的函数，保存到D盘</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkdir</span><span class="params">(path)</span>:</span></div><div class="line">    <span class="comment"># os.path.exists(name)判断是否存在路径</span></div><div class="line">    <span class="comment"># os.path.join(path, name)连接目录与文件名</span></div><div class="line">    isExists = os.path.exists(os.path.join(<span class="string">"D:\jiandan"</span>, path))</div><div class="line">    <span class="comment"># 如果不存在</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isExists:</div><div class="line">        print(<span class="string">'makedir'</span>, path)</div><div class="line">        <span class="comment"># 创建文件夹</span></div><div class="line">        os.makedirs(os.path.join(<span class="string">"D:\jiandan"</span>, path))</div><div class="line">        <span class="comment"># 切换到创建的文件夹</span></div><div class="line">        os.chdir(os.path.join(<span class="string">"D:\jiandan"</span>, path))</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="comment"># 如果存在了就返回False</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        print(path, <span class="string">'already exists'</span>)</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div></pre></td></tr></table></figure><p>我们只要传给这个函数一个路径参数就可以创建文件夹了。全部的功能都已经实现了，如果不出现以外的话，大家就可以在 d 盘看到这个文件夹了。</p><p><img src="http://imgout.ph.126.net/55991009/jiandan.jpg" alt="image"></p><p>如果程序出现错误，可能是我们的程序访问过于频繁，网站封禁了我们的ip。这时，我们就要使用一个代理了。网上免费的代理很多，大家可以自己找，这里简单做一个使用代理的演示。由于是免费ip，不会存活太久就会不能使用，大家不要直接使用代码中的ip。后续可以带着大家一起代建一个自己的代理池。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">proxies = &#123;<span class="string">'http'</span>: <span class="string">'111.23.10.27:8080'</span>&#125;</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="comment"># Requests库的get请求</span></div><div class="line">    resp = requests.get(url, headers=headers)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    <span class="comment"># 如果请求被阻，就使用代理</span></div><div class="line">    resp = requests.get(url, headers=headers, proxies=proxies)</div></pre></td></tr></table></figure><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>好了，最后上一下完整代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding:utf-8 -*-</span></div><div class="line"><span class="comment">#  author: yukun</span></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 发出请求获得HTML源码</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_html</span><span class="params">(url)</span>:</span></div><div class="line">    <span class="comment"># 指定一个浏览器头</span></div><div class="line">    headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'</span>&#125;</div><div class="line">    <span class="comment"># 代理，免费的代理只能维持一会可能就没用了，自行更换</span></div><div class="line">    proxies = &#123;<span class="string">'http'</span>: <span class="string">'111.23.10.27:8080'</span>&#125;</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        <span class="comment"># Requests库的get请求</span></div><div class="line">        resp = requests.get(url, headers=headers)</div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        <span class="comment"># 如果请求被阻，就使用代理</span></div><div class="line">        resp = requests.get(url, headers=headers, proxies=proxies)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> resp</div><div class="line"></div><div class="line"><span class="comment"># 创建文件夹的函数，保存到D盘</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkdir</span><span class="params">(path)</span>:</span></div><div class="line">    <span class="comment"># os.path.exists(name)判断是否存在路径</span></div><div class="line">    <span class="comment"># os.path.join(path, name)连接目录与文件名</span></div><div class="line">    isExists = os.path.exists(os.path.join(<span class="string">"D:\jiandan"</span>, path))</div><div class="line">    <span class="comment"># 如果不存在</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isExists:</div><div class="line">        print(<span class="string">'makedir'</span>, path)</div><div class="line">        <span class="comment"># 创建文件夹</span></div><div class="line">        os.makedirs(os.path.join(<span class="string">"D:\jiandan"</span>, path))</div><div class="line">        <span class="comment"># 切换到创建的文件夹</span></div><div class="line">        os.chdir(os.path.join(<span class="string">"D:\jiandan"</span>, path))</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="comment"># 如果存在了就返回False</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        print(path, <span class="string">'already exists'</span>)</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># 获得图片地址调用download函数进行下载</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_imgs</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># 调用函数获得所有页面</span></div><div class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> all_page():</div><div class="line">        path = url.split(<span class="string">'-'</span>)[<span class="number">-1</span>]</div><div class="line">        <span class="comment"># 创建文件夹的函数</span></div><div class="line">        mkdir(path)</div><div class="line">        <span class="comment"># 调用请求函数获得HTML源码</span></div><div class="line">        html = get_html(url).text</div><div class="line">        <span class="comment"># 使用lxml解析器，也可以使用html.parser</span></div><div class="line">        soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">        <span class="comment"># css选择器</span></div><div class="line">        allimgs = soup.select(<span class="string">'div.text &amp;gt; p &amp;gt; img'</span>)</div><div class="line">        <span class="comment"># 调用download函数下载保存</span></div><div class="line">        download(allimgs)</div><div class="line">    <span class="comment"># 执行完毕打出ok</span></div><div class="line">    print(<span class="string">'ok'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 获得所有页面</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">all_page</span><span class="params">()</span>:</span></div><div class="line">    base_url = <span class="string">'http://jandan.net/ooxx/'</span></div><div class="line">    <span class="comment"># BeautifulSoup解析页面得到最高页码数</span></div><div class="line">    soup = BeautifulSoup(get_html(base_url).text, <span class="string">'lxml'</span>)</div><div class="line">    <span class="comment"># 获得最高页码数</span></div><div class="line">    allpage = soup.find(<span class="string">'span'</span>, class_=<span class="string">"current-comment-page"</span>).get_text()[<span class="number">1</span>:<span class="number">-1</span>]</div><div class="line">    urllist = []</div><div class="line">    <span class="comment"># for循环迭代出所有页面，得到url</span></div><div class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, int(allpage)+<span class="number">1</span>):</div><div class="line">        allurl = base_url + <span class="string">'page-'</span> + str(page)</div><div class="line">        urllist.append(allurl)</div><div class="line">    <span class="keyword">return</span> urllist</div><div class="line"></div><div class="line"><span class="comment"># 保存图片函数，传入的参数是一页所有图片url集合</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(list)</span>:</span></div><div class="line">    <span class="keyword">for</span> img <span class="keyword">in</span> list:</div><div class="line">        urls = img[<span class="string">'src'</span>]</div><div class="line">        <span class="comment"># 判断url是否完整</span></div><div class="line">        <span class="keyword">if</span> urls[<span class="number">0</span>:<span class="number">5</span>] == <span class="string">'http:'</span>:</div><div class="line">            img_url = urls</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            img_url = <span class="string">'http:'</span> + urls</div><div class="line">        filename = img_url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</div><div class="line">        <span class="comment"># 保存图片</span></div><div class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">            <span class="comment"># 直接过滤掉保存失败的图片，不终止程序</span></div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                f.write(get_html(img_url).content)</div><div class="line">                print(<span class="string">'Sucessful image:'</span>,filename)</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">                print(<span class="string">'Failed:'</span>,filename)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="comment"># 计时</span></div><div class="line">    t1 = time.time()</div><div class="line">    <span class="comment"># 调用函数</span></div><div class="line">    get_imgs()</div><div class="line">    print(time.time() - t1)</div></pre></td></tr></table></figure><p>经过计时，我们只需要146秒就爬取了全站的妹子图片了，大家注意身体。</p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇文章中我们抓取了豆瓣图书的数据，如果大家运行成功，并且看到文件夹下的 txt 文件了。是不是有一种刚接触编程，第一次输出&lt;code&gt;Hello world!&lt;/code&gt;时的欣喜。和上一篇实践不同，我们这一次来爬取 &lt;a href=&quot;http://jandan.net/ooxx&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;煎蛋网&lt;/a&gt; 全站妹子图，并且保存到指定文件夹下。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(5):豆瓣读书练手爬虫</title>
    <link href="https://wyukun.coding.me/2017/06/03/Python%E7%88%AC%E8%99%AB%E4%BA%94%E4%B9%8B%E8%B1%86%E7%93%A3%E8%AF%BB%E4%B9%A6%E7%BB%83%E6%89%8B%E7%88%AC%E8%99%AB/"/>
    <id>https://wyukun.coding.me/2017/06/03/Python爬虫五之豆瓣读书练手爬虫/</id>
    <published>2017-06-03T07:14:39.000Z</published>
    <updated>2017-09-16T09:04:48.237Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>我们在之前的文章中基本上掌握了<code>Python</code>爬虫的原理和方法，不知道大家有没有练习呢。今天我就来找一个简单的网页进行爬取，就当是给之前的兵书做一个实践。不然不就是纸上谈兵的赵括了吗。</p><p>好了，我们这次的目标是<code>豆瓣图书Top250</code>，地址是：<code>https://book.douban.com/top250?start=0</code><br><a id="more"></a></p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>爬一个网页我们至少要知道自己需要什么信息，大家看截图：</p><p><img src="http://imgout.ph.126.net/55981022/QQCDBCC6AC20170603171444.jpg" alt="image"></p><p>红色箭头标记的地方就是我们要获取的信息了，包括书的名字，作者和出版社信息，豆瓣评分和一句话简介。我们有了目标信息，就需要找到信息所在的页面源码，然后通过解析源码来获取到信息数据。那么，我们怎样获得页面 HTML 源代码呢？翻阅兵书，我们知道可以使用<code>requests</code>之计。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'https://book.douban.com/top250?start=0'</span>)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>运行程序，我们就轻松的获得了敌军的 HTML 信息了。但是问题又来了，我们得到 HTML 信息后，怎样得到我们的目标数据呢？</p><p><strong>深夜了，一轮弯月躲在云朵后面，窗外下着雨，我们坐在烛火前，翻阅兵书，顿时茅塞顿开，BeautifulSoup大法好。</strong></p><p>我们打开浏览器，按<code>f12</code>到开发者工具，我们从网页源码里查找到数据位置，截图如下：</p><p><img src="http://imgout.ph.126.net/55981045/QQCDBCC6AC20170603171452.jpg" alt="image"></p><p>可以看到书名信息包含在<code>class=&#39;pl2&#39;</code> <code>div</code>里面的<code>a</code>标签内，是<code>a</code>标签的<code>title</code>属性。发现目标位置后，就简单多了。我们利用<code>BeautifulSoup</code>来获得一个对象，按找标准的缩进显示的<code>html</code>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line">soup = BeautifulSoup(resp.text, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure><p>推荐大家使用lxml解析器，因为他快。如果安装lxml遇到问题的可以参考 <a href="https://yukunweb.com/2017/06/01/Python%E7%88%AC%E8%99%AB%E5%9B%9B%E4%B9%8BBeautifulSoup%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/" target="_blank" rel="external">上一篇文章</a> 的方法。当然，如果大家怕麻烦，也完全可以使用Python的内置标准库<code>html.parser</code>.对我们获得结果并没有影响。</p><h2 id="开始工作"><a href="#开始工作" class="headerlink" title="开始工作"></a>开始工作</h2><p>现在我们要用到<code>BeautifulSoup</code>的<code>find_all()</code>选择器，因为我们这一页有很多书，而每一本书的信息都包含在<code>class=pl2</code>的<code>div</code>标签内，我们使用<code>find_all()</code>就可以直接得到本页所有书的书名了。我们用<code>find()</code>方法和<code>find_all()</code>方法来做一个比较：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># find_all()方法，</span></div><div class="line"><span class="comment"># 注意class是Python关键词，后面要加下划线_：</span></div><div class="line">alldiv = soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'pl2'</span>)</div><div class="line"><span class="keyword">for</span> a <span class="keyword">in</span> alldiv:</div><div class="line">    names = a.find(<span class="string">'a'</span>)[<span class="string">'title'</span>]</div><div class="line">    print(<span class="string">'find_all():'</span>, names)</div><div class="line"><span class="comment"># find()方法：</span></div><div class="line">alldiv2 = soup.find(<span class="string">'div'</span>, class_=<span class="string">'pl2'</span>)</div><div class="line">names2 = alldiv2.find(<span class="string">'a'</span>)[<span class="string">'title'</span>]</div><div class="line">print(<span class="string">'find():'</span>, names2 )</div></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">find_all(): 追风筝的人</div><div class="line">find_all(): 小王子</div><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="comment"># ...省略部分</span></div><div class="line"><span class="comment"># ...</span></div><div class="line">find_all(): 三体Ⅲ</div><div class="line">find(): 追风筝的人</div><div class="line"></div><div class="line">Process finished <span class="keyword">with</span> exit code <span class="number">0</span></div></pre></td></tr></table></figure><p>我们通过结果就可以看到两者之间的差距了，前者输出了一页的数据，而后者只输出了第一条数据。所以包括后面的信息，由于每一天数据所在标签是一样的，我们都是用<code>find_all()</code>方法。</p><p>上面的代码写的优雅点，就是这样实现,注意结果是一个 list：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 书名, 注意是L小写，不是阿拉伯数字1</span></div><div class="line">alldiv = soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'pl2'</span>)</div><div class="line">names = [a.find(<span class="string">'a'</span>)[<span class="string">'title'</span>] <span class="keyword">for</span> a <span class="keyword">in</span> alldiv]</div><div class="line">print(names)</div></pre></td></tr></table></figure><p>这样书名数据我们就得到了，接下来是作者信息。方法和获取书名方法一样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 作者,由于信息在一个p标签内部，</span></div><div class="line"><span class="comment"># 我们获取到标签直接get_text()方法获得文本内容</span></div><div class="line">allp = soup.find_all(<span class="string">'p'</span>, class_=<span class="string">'pl'</span>)</div><div class="line">authors = [p.get_text() <span class="keyword">for</span> p <span class="keyword">in</span> allp]</div></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[&apos;[美] 卡勒德·胡赛尼 / 李继宏 / 上海人民出版社 / 2006-5 / 29.00元&apos;, </div><div class="line">&apos;[法] 圣埃克苏佩里 / 马振聘 / 人民文学出版社 / 2003-8 / 22.00元&apos;, </div><div class="line">&apos;钱锺书 / 人民文学出版社 / 1991-2 / 19.00&apos;, </div><div class="line">&apos;余华 / 南海出版公司 / 1998-5 / 12.00元&apos;, </div><div class="line"># ...</div><div class="line"># ...省略部分结果</div><div class="line"># ...</div><div class="line">&apos;高铭 / 武汉大学出版社 / 2010-2 / 29.80元&apos;, </div><div class="line">&apos;刘慈欣 / 重庆出版社 / 2010-11 / 38.00元&apos;]</div></pre></td></tr></table></figure><p>后面的评分内容和简介内容也是一样获得，只是标签不同，但是方法一样，具体也不需要多余赘述。直接看实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 评分</span></div><div class="line">starspan = soup.find_all(<span class="string">'span'</span>, class_=<span class="string">'rating_nums'</span>)</div><div class="line">scores = [s.get_text() <span class="keyword">for</span> s <span class="keyword">in</span> starspan]</div><div class="line"><span class="comment"># 简介</span></div><div class="line">sumspan = soup.find_all(<span class="string">'span'</span>, class_=<span class="string">'inq'</span>)</div><div class="line">sums = [i.get_text() <span class="keyword">for</span> i <span class="keyword">in</span> sumspan]</div></pre></td></tr></table></figure><p>程序运行成功，我们就获得了4个list，分别是书名，作者，评分和简介内容。我们要把他们放在一起，打印出来，就是一页的数据信息了。<br>这里我们使用<code>zip()</code>函数，<code>zip()</code>函数在运算时，会以一个或多个序列做为参数，返回一个元组的列表。同时将这些序列中并排的元素配对。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> name, author, score, sum <span class="keyword">in</span> zip(names, authors, scores, sums):</div><div class="line">    name = <span class="string">'书名：'</span> + str(name) + <span class="string">'\n'</span></div><div class="line">    author = <span class="string">'作者：'</span> + str(author) + <span class="string">'\n'</span></div><div class="line">    score = <span class="string">'评分：'</span> + str(score) + <span class="string">'\n'</span></div><div class="line">    sum = <span class="string">'简介：'</span> + str(sum) + <span class="string">'\n'</span></div><div class="line">    data = name + author + score + sum</div></pre></td></tr></table></figure><p>我们使用换行符’<code>\n</code>‘给数据信息一点整齐的样式。我们可以查看到打印的结果，并没有所有数据黏在一起，显得丑陋。<br>获得信息后，就是保存数据了。保存数据也很简单，<code>Python</code>的文件读写操作就可以实现。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 文件名</span></div><div class="line">filename = <span class="string">'豆瓣图书Top250.txt'</span></div><div class="line"><span class="comment"># 保存文件操作</span></div><div class="line"><span class="keyword">with</span> open(filename, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</div><div class="line">    <span class="comment"># 保存数据</span></div><div class="line">    f.writelines(data + <span class="string">'======================='</span> + <span class="string">'\n'</span>)</div><div class="line">print(<span class="string">'保存成功'</span>)</div></pre></td></tr></table></figure><p>运行成功，我们就可以看到项目文件下面的 txt 文件了，上面保存了我们上面打印出来的内容。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">书名：追风筝的人</div><div class="line">作者：[美] 卡勒德·胡赛尼 / 李继宏 / 上海人民出版社 / 2006-5 / 29.00元</div><div class="line">评分：8.8</div><div class="line">简介：为你，千千万万遍</div><div class="line">==================</div><div class="line"># ...</div><div class="line"># ...</div><div class="line">书名：活着</div><div class="line">作者：余华 / 南海出版公司 / 1998-5 / 12.00元</div><div class="line">评分：9.1</div><div class="line">简介：活着本身就是人生最大的意义</div><div class="line">==================</div></pre></td></tr></table></figure><p>但是，我们要的是 250 条数据，而不是一页的十几条数据，那么要怎么获得到所有的数据呢。我们可以检查页面的信息，可以看到页面一共 10 页，第一页的URL是<code>https://book.douban.com/top250?start=0</code>。而最后一页的 URL 是<code>https://book.douban.com/top250?start=225</code><br>我们接着多看几页，第二页是<code>https://book.douban.com/top250?start=25</code>，第三页是<code>https://book.douban.com/top250?start=50</code>。</p><p>规律已经很清晰了，我们的页面的页数信息是最后的<code>start=</code>后面的数字。而且数字从0开始到225，每一页数字加 25.这就很简单了，我们以<code>https://book.douban.com/top250?start=</code>为基层URL，每一页在后面加页面的页数数字。就可以得到所有的页面 url 了。再以<code>for</code>循环迭代每一个 url，使用上面获取数据的方法，获得所有的数据信息。</p><p>获取所有页面URL的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">base_url = <span class="string">'https://book.douban.com/top250?start='</span></div><div class="line">urllist = []</div><div class="line"><span class="comment"># 从0到225，间隔25的数组</span></div><div class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">250</span>, <span class="number">25</span>):</div><div class="line">    allurl = base_url + str(page)</div><div class="line">    urllist.append(allurl)</div></pre></td></tr></table></figure><p>我们把他保存在 list 里面，好用循环迭代。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>那么，所有的功能都实现了。现在，我们只要将所有的代码组合起来，就可以实现我们需要的所有功能了。<br>上代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding:utf-8 -*-</span></div><div class="line"><span class="comment">#  author: yukun</span></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 发出请求获得HTML源码的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_html</span><span class="params">(url)</span>:</span></div><div class="line"><span class="comment"># 伪装成浏览器访问</span></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(url, headers=headers).text</div><div class="line"></div><div class="line"><span class="keyword">return</span> resp</div><div class="line"></div><div class="line"><span class="comment"># 解析页面，获得数据信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_parse</span><span class="params">()</span>:</span></div><div class="line"><span class="comment"># 调用函数，for循环迭代出所有页面</span></div><div class="line"><span class="keyword">for</span> url <span class="keyword">in</span> all_page():</div><div class="line"><span class="comment"># BeautifulSoup的解析</span></div><div class="line">soup = BeautifulSoup(get_html(url), <span class="string">'lxml'</span>)</div><div class="line"><span class="comment"># 书名</span></div><div class="line">alldiv = soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'pl2'</span>)</div><div class="line">names = [a.find(<span class="string">'a'</span>)[<span class="string">'title'</span>] <span class="keyword">for</span> a <span class="keyword">in</span> alldiv]</div><div class="line"><span class="comment"># 作者</span></div><div class="line">allp = soup.find_all(<span class="string">'p'</span>, class_=<span class="string">'pl'</span>)</div><div class="line">authors = [p.get_text() <span class="keyword">for</span> p <span class="keyword">in</span> allp]</div><div class="line"><span class="comment"># 评分</span></div><div class="line">starspan = soup.find_all(<span class="string">'span'</span>, class_=<span class="string">'rating_nums'</span>)</div><div class="line">scores = [s.get_text() <span class="keyword">for</span> s <span class="keyword">in</span> starspan]</div><div class="line"><span class="comment"># 简介</span></div><div class="line">sumspan = soup.find_all(<span class="string">'span'</span>, class_=<span class="string">'inq'</span>)</div><div class="line">sums = [i.get_text() <span class="keyword">for</span> i <span class="keyword">in</span> sumspan]</div><div class="line"><span class="keyword">for</span> name, author, score, sum <span class="keyword">in</span> zip(names, authors, scores, sums):</div><div class="line">name = <span class="string">'书名：'</span> + str(name) + <span class="string">'\n'</span></div><div class="line">author = <span class="string">'作者：'</span> + str(author) + <span class="string">'\n'</span></div><div class="line">score = <span class="string">'评分：'</span> + str(score) + <span class="string">'\n'</span></div><div class="line">sum = <span class="string">'简介：'</span> + str(sum) + <span class="string">'\n'</span></div><div class="line">data = name + author + score + sum</div><div class="line"><span class="comment"># 保存数据</span></div><div class="line">f.writelines(data + <span class="string">'======================='</span> + <span class="string">'\n'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 获得所有页面的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">all_page</span><span class="params">()</span>:</span></div><div class="line">base_url = <span class="string">'https://book.douban.com/top250?start='</span></div><div class="line">urllist = []</div><div class="line"><span class="comment"># 从0到225，间隔25的数组</span></div><div class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">250</span>, <span class="number">25</span>):</div><div class="line">allurl = base_url + str(page)</div><div class="line">urllist.append(allurl)</div><div class="line"></div><div class="line"><span class="keyword">return</span>  urllist</div><div class="line"></div><div class="line"><span class="comment"># 文件名</span></div><div class="line">filename = <span class="string">'豆瓣图书Top250.txt'</span></div><div class="line"><span class="comment"># 保存文件操作</span></div><div class="line">f = open(filename, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</div><div class="line"><span class="comment"># 调用函数</span></div><div class="line">html_parse()</div><div class="line">f.close()</div><div class="line">print(<span class="string">'保存成功。'</span>)</div></pre></td></tr></table></figure><p>我们只用了36行的代码（去掉空行和注释）就实现了抓取豆瓣图书的数据了。大家是不是觉得很简单了，不要兴奋，这只是一个小白最基础的练手项目，大家快去找更有挑战性的项目实现吧。大家加油。</p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们在之前的文章中基本上掌握了&lt;code&gt;Python&lt;/code&gt;爬虫的原理和方法，不知道大家有没有练习呢。今天我就来找一个简单的网页进行爬取，就当是给之前的兵书做一个实践。不然不就是纸上谈兵的赵括了吗。&lt;/p&gt;
&lt;p&gt;好了，我们这次的目标是&lt;code&gt;豆瓣图书Top250&lt;/code&gt;，地址是：&lt;code&gt;https://book.douban.com/top250?start=0&lt;/code&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>行走世间都是妖怪</title>
    <link href="https://wyukun.coding.me/2017/06/02/%E8%A1%8C%E8%B5%B0%E4%B8%96%E9%97%B4%E9%83%BD%E6%98%AF%E5%A6%96%E6%80%AA/"/>
    <id>https://wyukun.coding.me/2017/06/02/行走世间都是妖怪/</id>
    <published>2017-06-02T07:05:48.000Z</published>
    <updated>2017-09-16T07:10:08.724Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>热的时候喊热是无济于事的。顶着火红的太阳，看着姗姗来迟的 138 路公交，无奈的翻翻白眼。</p><p>南京的天气一如既往的热，由于科目二考试将至，连安静呆在宿舍的想法都无法满足。不过年轻人就是应该多出来走走，不然就无法看到外面美丽的姑娘们了。钱钟书说暴露的姑娘是局部的真理，我觉得这再合适不过了，真理总是赤裸的。正值躁动青春期的室友，掩饰不住自己对真理渴望的心，时不时露出莫名其妙的笑。<br><a id="more"></a></p><iframe src="//music.163.com/outchain/player?type=2&amp;id=28953457&amp;auto=0&amp;height=66" width="330" height="86" frameborder="no" marginwidth="0" marginheight="0"></iframe><p>倒车，转弯，刹车，踩离合。时间就在这些机械的步骤中流过，真让人兴奋，终于可以回到凉爽的宿舍了。室友说他要在中途有个事，说不坐公交回去了。我说开什么玩笑，不坐公交怎么回去，难道请我打车。然后在我惊讶的目光下，他推来辆自行车。</p><p>其实我一直搞不清楚共享单车的盈利方式，扫个码就可以免费骑，而且还可以扫两辆。于是我们骑着自行车，行驶在烈日之下，行驶在公交车行驶30分钟的路程上，挥汗如雨。路边不断的有单身的或者不是单身的姑娘走过，为了自己的形象，即使在疲惫也要摆出轻松的姿势，面露沉稳的微笑，匆匆驶过，流下一阵燥热的风。</p><p>我今年二十多岁了，一直单身。我仍然不明白有些人究竟是有多么的空虚，多么的寂寞。分个手吵个架，遇到了不舒服的事就发个朋友圈，发个微博，感时运之不济，叹人性之不真。然而我在这条通往目的地的道路上，面对着周围美丽的真理，我竟没有一点想法。</p><p>我想那些天天矫情的分享人生感悟的人，并不是他们在人生的道路上遇到了磨难，而是他们都太闲了。走出房门，扫个码骑个自行车，行驶个40分钟，自然会明白还是屋里舒服。</p><p><img src="http://imgout.ph.126.net/55964042/timg.jpg" alt="image"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;热的时候喊热是无济于事的。顶着火红的太阳，看着姗姗来迟的 138 路公交，无奈的翻翻白眼。&lt;/p&gt;
&lt;p&gt;南京的天气一如既往的热，由于科目二考试将至，连安静呆在宿舍的想法都无法满足。不过年轻人就是应该多出来走走，不然就无法看到外面美丽的姑娘们了。钱钟书说暴露的姑娘是局部的真理，我觉得这再合适不过了，真理总是赤裸的。正值躁动青春期的室友，掩饰不住自己对真理渴望的心，时不时露出莫名其妙的笑。&lt;br&gt;
    
    </summary>
    
      <category term="生活" scheme="https://wyukun.coding.me/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="随笔" scheme="https://wyukun.coding.me/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(4):Beautiful Soup的常用方法</title>
    <link href="https://wyukun.coding.me/2017/06/01/Python%E7%88%AC%E8%99%AB%E5%9B%9B%E4%B9%8BBeautifulSoup%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://wyukun.coding.me/2017/06/01/Python爬虫四之BeautifulSoup的常用方法/</id>
    <published>2017-06-01T06:25:07.000Z</published>
    <updated>2017-09-16T07:08:51.236Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><code>Requests</code>库的用法大家肯定已经熟练掌握了，但是当我们使用<code>Requests</code>获取到网页的 HTML 代码信息后，我们要怎样才能抓取到我们想要的信息呢？我相信大家肯定尝试过很多办法，比如字符串的 find 方法，还有高级点的正则表达式。虽然正则可以匹配到我们需要的信息，但是我相信大家在匹配某个字符串一次一次尝试着正则匹配的规则时，一定很郁闷。</p><p>那么，我们就会想有没有方便点的工具呢。答案是肯定的，我们还有一个强大的工具，叫<code>BeautifulSoup</code>。有了它我们可以很方便地提取出<code>HTML</code>或<code>XML</code>标签中的内容，这篇文章就让我们了解下<code>BeautifulSoup</code>的常用方法吧。<br><a id="more"></a></p><h2 id="什么是BeautifulSoup？"><a href="#什么是BeautifulSoup？" class="headerlink" title="什么是BeautifulSoup？"></a>什么是BeautifulSoup？</h2><p><code>Python</code>的网页解析可以用正则表达式去完成，那么我们在写的时候，要挨个的去把代码拿出来匹配，而且还要写匹配的规则，整体实现起来就很复杂。<code>BeautifulSoup</code>呢，它是一个方便的网页解析库，处理高效，支持多种解析器。大部分情况下，利用它我们不在需要编写正则表达式就可以方便的实现网页信息的提取。</p><p><a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="external">官方文档</a></p><p>安装：<code>$ pip install beautifulsoup4</code></p><p><code>BeautifulSoup</code>是一个网页解析库，它支持很多解析器，不过最主流的有两个。一个是<code>Python</code>标准库，一个是<code>lxml</code> HTML 解析器。两者的使用方法相似：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line"><span class="comment"># Python的标准库</span></div><div class="line">BeautifulSoup(html, <span class="string">'html.parser'</span>)</div><div class="line"></div><div class="line"><span class="comment"># lxml</span></div><div class="line">BeautifulSoup(html, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure><p><code>Python</code>内置标准库的执行速度一般，但是低版本的<code>Python</code>中，中文的容错能力比较差。<code>lxml</code>HTML 解析器的执行速度快，但是需要安装 C语言的依赖库。</p><h2 id="lxml的安装"><a href="#lxml的安装" class="headerlink" title="lxml的安装"></a>lxml的安装</h2><p>由于<code>lxml</code>安装需要依赖C语言库，所以当<code>lxml</code>在<code>Windows</code>上安装时，我们会发现各种奇怪的报错，当然脸好的使用<code>pip install lxml</code></p><p>安装也是可以成功的。不过大部分人都是会倒在这里。</p><p>这里推荐大家使用<code>lxml</code>的<code>.whl</code>文件来安装。首先我们需要安装一下<code>wheel</code>库，有了这个库我们才可以正常安装<code>.whl</code>文件。<code>pip install wheel</code></p><p>从官方网站下载与系统，<code>Python</code>版本匹配的lxml文件：<a href="https://pypi.python.org/pypi/lxml/3.6.0" target="_blank" rel="external">地址</a>。</p><p>另外，不知道自己系统和<code>python</code>版本信息的伙伴。需要进入系统管理员工具（CMD）或者python的 IDLE，输入以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pip</div><div class="line"></div><div class="line">print(pip.pep425tags.get_supported())</div></pre></td></tr></table></figure><p>这时我们就可以看到打印出来的<code>Python</code>版本信息了。<br>下载好<code>lxml</code>的文件后，我们需要找到文件的位置，然后进入管理员工具，使用<code>pip</code>安装：<code>pip install whl文件的全名</code></p><p>安装完成后，可以进入<code>Python</code>，<code>import</code>一下，如果没有报错，那么恭喜你安装成功。<br>如果有的伙伴觉得麻烦，那我推荐大家安装<code>anaconda</code> <a href="https://www.continuum.io/downloads" target="_blank" rel="external">下载地址</a>（如果安装速度慢，可以找国内镜像），不知道是什么的小伙伴可以谷歌一下，有了他，那些在<code>windows</code>上<code>pip</code>安装出错的问题将不再存在。</p><h2 id="BeautifulSoup的基本标签选择方法"><a href="#BeautifulSoup的基本标签选择方法" class="headerlink" title="BeautifulSoup的基本标签选择方法"></a>BeautifulSoup的基本标签选择方法</h2><p>虽然<code>Python</code>内置的标准库解析器还不错，但是我还是推荐大家使用<code>lxml</code>，因为它够快。那么后面的代码我们都是用<code>lxml</code>解析器来进行演示。<br>我们先导入官方文档的例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">html_doc = <span class="string">"""</span></div><div class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</span></div><div class="line"><span class="string">&lt;body&gt;</span></div><div class="line"><span class="string">&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were</span></div><div class="line"><span class="string">&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="string">&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and</span></div><div class="line"><span class="string">&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;</span></div><div class="line"><span class="string">and they lived at the bottom of a well.&lt;/p&gt;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;</span></div><div class="line"><span class="string">"""</span></div></pre></td></tr></table></figure><p>HTML 代码,我们能够得到一个<code>BeautifulSoup</code>的对象,并能按照标准的缩进格式的结构输出:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">soup = BeautifulSoup(html_doc, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure><p>我们可以看到上面的 HTML 代码并不完整，接下来我们使用<code>prettify()</code>方法来进行自动补全，注释部分就是运行的输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">print(soup.prettify())</div><div class="line"><span class="comment"># &lt;html&gt;</span></div><div class="line"><span class="comment">#  &lt;head&gt;</span></div><div class="line"><span class="comment">#   &lt;title&gt;</span></div><div class="line"><span class="comment">#    The Dormouse's story</span></div><div class="line"><span class="comment">#   &lt;/title&gt;</span></div><div class="line"><span class="comment">#  &lt;/head&gt;</span></div><div class="line"><span class="comment">#  &lt;body&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="title"&gt;</span></div><div class="line"><span class="comment">#    &lt;b&gt;</span></div><div class="line"><span class="comment">#     The Dormouse's story</span></div><div class="line"><span class="comment">#    &lt;/b&gt;</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="story"&gt;</span></div><div class="line"><span class="comment">#    Once upon a time there were three little sisters; and their names were</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;</span></div><div class="line"><span class="comment">#     Elsie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    ,</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;</span></div><div class="line"><span class="comment">#     Lacie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    and</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/tillie" id="link2"&gt;</span></div><div class="line"><span class="comment">#     Tillie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    ; and they lived at the bottom of a well.</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="story"&gt;</span></div><div class="line"><span class="comment">#    ...</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#  &lt;/body&gt;</span></div><div class="line"><span class="comment"># &lt;/html&gt;</span></div></pre></td></tr></table></figure><h3 id="获取标签"><a href="#获取标签" class="headerlink" title="获取标签"></a>获取标签</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(soup.title)</div><div class="line"><span class="comment"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></div></pre></td></tr></table></figure><p>通过输出结果，我们可以看到获取内容的属性，实际上就是 HTML 代码里的一个<code>title</code>标签。</p><h3 id="获取名称"><a href="#获取名称" class="headerlink" title="获取名称"></a>获取名称</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(soup.title.name)</div><div class="line"><span class="comment"># 'title'</span></div></pre></td></tr></table></figure><p>实际上就是标签的名称。</p><h3 id="获取属性"><a href="#获取属性" class="headerlink" title="获取属性"></a>获取属性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">print(soup.p.attrs[<span class="string">'class'</span>])</div><div class="line"><span class="comment"># 'title'</span></div><div class="line"></div><div class="line">print(soup.p[<span class="string">'class'</span>])</div><div class="line"><span class="comment"># 'title'</span></div></pre></td></tr></table></figure><p>获取标签的属性我们可以使用<code>attrs</code>方法，传给他属性名，就可以得到标签的属性。通过结果我们可以看到，直接传给p标签属性名，一样可以获取到标签属性。</p><h3 id="获取内容"><a href="#获取内容" class="headerlink" title="获取内容"></a>获取内容</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(soup.title.string)</div><div class="line"><span class="comment"># 'The Dormouse's story'</span></div></pre></td></tr></table></figure><p>我们还可以使用嵌套的选择，比如我们获得body标签里面p标签的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(soup.body.p.string)</div><div class="line"><span class="comment"># 'The Dormouse's story'</span></div></pre></td></tr></table></figure><h2 id="常见用法"><a href="#常见用法" class="headerlink" title="常见用法"></a>常见用法</h2><h3 id="标准选择器"><a href="#标准选择器" class="headerlink" title="标准选择器"></a>标准选择器</h3><p>虽然<code>BeautifulSoup</code>的基本用法，标签获取，内容获取，可以解析一些 html代码。但是在遇到很多复杂的页面时，上面的方法是完全不足的，或者是很繁琐的，因为有时候有的标签会有几个属性（class、id等）。</p><p>索性<code>BeautifulSoup</code>给我们提供了很方便的标准选择器，也就是 API 方法，这里着重介绍2个: <code>find()</code> 和 <code>find_all()</code> 。其它方法的参数和用法类似,大家举一反三吧。</p><h3 id="find-all"><a href="#find-all" class="headerlink" title="find_all()"></a>find_all()</h3><p><code>find_all(name, attrs, recursive, text, **kwargs)</code>可以根据标签，属性，内容查找文档。<br><code>find_all()</code>其实和正则表达式的原理很相似，他能找出所有能满足匹配模式的结果，在把结果以列表的形式返回。<br>仍然是文档的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">html_doc = <span class="string">"""</span></div><div class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</span></div><div class="line"><span class="string">&lt;body&gt;</span></div><div class="line"><span class="string">&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were</span></div><div class="line"><span class="string">&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="string">&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and</span></div><div class="line"><span class="string">&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;</span></div><div class="line"><span class="string">and they lived at the bottom of a well.&lt;/p&gt;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;</span></div><div class="line"><span class="string">"""</span></div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line">soup = BeautifulSoup(html_doc, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure><h3 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h3><p><a href="https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/#id28" target="_blank" rel="external">文档参考</a><br>介绍 <code>find_all()</code> 方法前,大家可以参考一下过滤器的类型。过滤器只能作为搜索文档的参数,或者说应该叫参数类型更为贴切。这些过滤器贯穿整个搜索的API。过滤器可以被用在 tag 的<code>name</code>中,节点的属性中,字符串中或他们的混合中。</p><p><code>find_all()</code> 方法搜索当前 tag 的所有 tag 子节点,并判断是否符合过滤器的条件。这里有几个例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div><div class="line"></div><div class="line">soup.find_all(<span class="string">"p"</span>, <span class="string">"title"</span>)</div><div class="line"><span class="comment"># [&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;]</span></div><div class="line"></div><div class="line">soup.find_all(<span class="string">"a"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div><div class="line"></div><div class="line">soup.find_all(id=<span class="string">"link2"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><p>有几个方法很相似,还有几个方法是新的,参数中的 <code>string</code> 和<code>id</code>是什么含义? 为什么 <code>find_all(&quot;p&quot;, &quot;title&quot;)</code> 返回的是CSS Class为”title”的标签? 我们来仔细看一下<code>find_all()</code>的参数:</p><h4 id="name参数"><a href="#name参数" class="headerlink" title="name参数"></a>name参数</h4><p>name 参数可以查找所有名字为 name 的 tag,字符串对象会被自动忽略掉。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># [The Dormouse's story]</span></div></pre></td></tr></table></figure><p>搜索 name 参数的值可以使任一类型的过滤器,字符窜,正则表达式,列表,方法或是<code>True</code> 。<br>我们常用的 name 参数是搜索文档的标签名。</p><h4 id="keyword参数"><a href="#keyword参数" class="headerlink" title="keyword参数"></a>keyword参数</h4><p>如果我们的 HTML代码中有几个<code>div</code>标签，但是我们只想获取到<code>class</code>属性为<code>top</code>的<code>div</code>标签，我们怎么出来呢。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'top'</span>)</div><div class="line"><span class="comment"># 这里注意下，class是Python的内部关键词，我们需要在css属性class后面加一个下划线'_'，不然会报错。</span></div></pre></td></tr></table></figure><p>仍然以上面的代码实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'a'</span>, id=<span class="string">'link2'</span>)</div><div class="line"><span class="comment"># [&lt;a id="link2" href="http://example.com/lacie"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><p>这样我们就只获取到<code>id</code>为<code>link2</code>的<code>a</code>标签。</p><h4 id="limit参数"><a href="#limit参数" class="headerlink" title="limit参数"></a>limit参数</h4><p><code>find_all()</code> 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢。如果我们不需要全部结果,可以使用 <code>limit</code> 参数限制返回结果的数量。效果与 SQL 中的<code>limit</code>关键字类似,当搜索到的结果数量达到<code>limit</code>的限制时,就停止搜索返回结果。</p><p>比如我们要搜索出<code>a</code>标签，但是满足的有3个，我们只想要得到2个：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"a"</span>, limit=<span class="number">2</span>)</div><div class="line"><span class="comment"># [&lt;a id="link1" class="sister" href="http://example.com/elsie"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a id="link2" class="sister" href="http://example.com/lacie"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><p>其他的参数，不是经常用到，大家如需了解可以参考官方文档。</p><h3 id="find"><a href="#find" class="headerlink" title="find()"></a>find()</h3><p><code>find_all()</code>返回的是所有元素列表，<code>find()</code>返回单个元素。</p><p><code>find( name , attrs , recursive , string , **kwargs )</code></p><p><code>find_all()</code>方法将返回文档中符合条件的所有 tag,尽管有时候我们只想得到一个结果。比如文档中只有一个标签,那么使用<code>find_all()</code> 方法来查找标签就不太合适, 使用<code>find_all</code>方法并设置<code>limit=1</code>参数不如直接使用<code>find()</code>方法。下面两行代码是等价的:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'title'</span>, limit=<span class="number">1</span>)</div><div class="line"><span class="comment"># [The Dormouse's story]</span></div><div class="line"></div><div class="line">soup.find(<span class="string">'title'</span>)</div><div class="line"><span class="comment">#The Dormouse's story</span></div></pre></td></tr></table></figure><p>唯一的区别是<code>find_all()</code>方法的返回结果是值包含一个元素的列表,而<code>find()</code>方法直接返回结果。<code>find_all()</code>方法没有找到目标是返回空列表, <code>find()</code>方法找不到目标时,返回<code>None</code>。</p><h3 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h3><p><code>Beautiful Soup</code>支持大部分的 CSS选择器。在<code>Tag</code>或<code>BeautifulSoup</code>对象的<code>.select()</code>方法中传入字符串参数, 即可使用 CSS选择器的语法找到 tag。我们在写 css 时，标签 class类名加”<code>.</code>“，id属性加”<code>#</code>“。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># [The Dormouse's story]</span></div></pre></td></tr></table></figure><p>通过 tag标签逐层查找:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">"body a"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/lacie"  id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div><div class="line"></div><div class="line">soup.select(<span class="string">"html head title"</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div></pre></td></tr></table></figure><p>找到某个 tag标签下的直接子标签:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">"head &gt; title"</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div><div class="line"></div><div class="line">soup.select(<span class="string">"p &gt; a"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/lacie"  id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div><div class="line"></div><div class="line">soup.select(<span class="string">"p &gt; #link1"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]</span></div><div class="line"></div><div class="line">soup.select(<span class="string">"body &gt; a"</span>)</div><div class="line"><span class="comment"># []</span></div></pre></td></tr></table></figure><p>通过 CSS 的 class类名查找:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">".sister"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><p>通过 tag 的 id 查找:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">"#link1"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]</span></div><div class="line"></div><div class="line">soup.select(<span class="string">"a#link2"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><p>同时用多种 CSS选择器查询元素，使用逗号隔开:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">"#link1,#link2"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><h2 id="提取标签内容"><a href="#提取标签内容" class="headerlink" title="提取标签内容"></a>提取标签内容</h2><p>如果我们得到了几个标签：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">list = [&lt;a href="http://www.baidu.com/"&gt;百度&lt;/a&gt;,</div><div class="line"></div><div class="line">&lt;a href="http://www.163.com/"&gt;网易&lt;/a&gt;,</div><div class="line"></div><div class="line">&lt;a href="http://www.sina.com/"新浪&lt;/a&gt;]</div></pre></td></tr></table></figure><p>我们要怎样提取他里面的内容呢。我们开始的时候有提及。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> list:</div><div class="line">    print(i.get_text()) <span class="comment"># 我们使用get_text()方法获得标签内容</span></div><div class="line">    print(i.get[<span class="string">'href'</span>] <span class="comment"># get['attrs']方法获得标签属性</span></div><div class="line">    print(i[<span class="string">'href'</span>]) <span class="comment"># 简写结果一样</span></div></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">百度</div><div class="line">网易</div><div class="line">新浪</div><div class="line">http://www.baidu.com/</div><div class="line">http://www<span class="number">.163</span>.com/</div><div class="line">http://www.sina.com/</div><div class="line">http://www.baidu.com/</div><div class="line">http://www<span class="number">.163</span>.com/</div><div class="line">http://www.sina.com/</div></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><code>BeautifulSoup</code>的解析库，推荐使用<code>lxml</code>，如果出现乱码的情况下，可以使用<code>html.parser</code>；</li><li><code>BeautifulSoup</code>的标签选择筛选方法，虽然弱但是速度快；</li><li>推荐使用<code>find_all()</code>,<code>find()</code>方法搜索标签，当然如果对css选择器熟悉，推荐使用<code>.select()</code>方法；</li><li><code>get_text()</code>方法获取标签文本内容，<code>get[attrs]</code>方法获取标签属性值。</li></ul><p>本篇我们就基本上整理了<code>BeautifulSoup</code>的常用方法。如果大家希望了解更高级的用法，可以查看<code>BeautifulSoup</code>的官方文档。</p><p>最后，大家可以结合<code>Requests</code>库写出自己的爬虫吧。</p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Requests&lt;/code&gt;库的用法大家肯定已经熟练掌握了，但是当我们使用&lt;code&gt;Requests&lt;/code&gt;获取到网页的 HTML 代码信息后，我们要怎样才能抓取到我们想要的信息呢？我相信大家肯定尝试过很多办法，比如字符串的 find 方法，还有高级点的正则表达式。虽然正则可以匹配到我们需要的信息，但是我相信大家在匹配某个字符串一次一次尝试着正则匹配的规则时，一定很郁闷。&lt;/p&gt;
&lt;p&gt;那么，我们就会想有没有方便点的工具呢。答案是肯定的，我们还有一个强大的工具，叫&lt;code&gt;BeautifulSoup&lt;/code&gt;。有了它我们可以很方便地提取出&lt;code&gt;HTML&lt;/code&gt;或&lt;code&gt;XML&lt;/code&gt;标签中的内容，这篇文章就让我们了解下&lt;code&gt;BeautifulSoup&lt;/code&gt;的常用方法吧。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(3):Requests的高级用法</title>
    <link href="https://wyukun.coding.me/2017/05/29/Python%E7%88%AC%E8%99%AB%E4%B8%89%E4%B9%8BRequests%E7%9A%84%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/"/>
    <id>https://wyukun.coding.me/2017/05/29/Python爬虫三之Requests的高级用法/</id>
    <published>2017-05-29T15:28:36.000Z</published>
    <updated>2017-09-16T04:56:30.899Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>上一篇文章我们整理了<code>Requests</code>库的基本用法，相信大家已经经过爬取一些简单网页的练习，已经很熟练了。</p><p>这一篇文章我们来 看一下<code>Requests</code>库的高级操作。<br><a id="more"></a></p><h2 id="高级操作"><a href="#高级操作" class="headerlink" title="高级操作"></a>高级操作</h2><h3 id="1-文件上传"><a href="#1-文件上传" class="headerlink" title="1.文件上传"></a>1.文件上传</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">files = &#123;<span class="string">'file'</span> : open(<span class="string">'logo.gif'</span>,<span class="string">'rb'</span>)&#125;</div><div class="line">resp = requests.post(<span class="string">'http://httpbin.org/post'</span>, files=files)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>文件上传的操作只要我们从文件夹中把文件读取出来，并且赋值给 files 参数，就可以了，打印出源代码我们就可以看待上传文件的字节流了。</p><h3 id="2-获取Cookie"><a href="#2-获取Cookie" class="headerlink" title="2.获取Cookie"></a>2.获取Cookie</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">import</span> requests</div><div class="line"></div><div class="line">&gt;&gt;&gt;resp = requests.get(<span class="string">'http://www.baidu.com'</span>)</div><div class="line">&gt;&gt;&gt;print(resp.cookies)</div><div class="line">&lt;RequestsCookieJar[]&gt;</div><div class="line">&gt;&gt;&gt;<span class="keyword">for</span> key, value <span class="keyword">in</span> resp.cookies.items():</div><div class="line"><span class="meta">... </span>   print(key + <span class="string">'='</span> + value)</div><div class="line">BDORZ=<span class="number">27315</span></div></pre></td></tr></table></figure><p>我们可以通过获取字典的键值对来查看<code>cookie</code>.</p><h3 id="3-会话维持"><a href="#3-会话维持" class="headerlink" title="3.会话维持"></a>3.会话维持</h3><p>我们获得到了<code>cookie</code>就可以做一个会话维持，可以维持一个登录的状态，也就是做模拟登录。我们来看实现方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">s = requests.Session()</div><div class="line">s.get(<span class="string">'http://httpbin.org/cookies/set/number/123456789'</span>) <span class="comment"># 设置了一个cookie</span></div><div class="line">resp = s.get(<span class="string">'http://httpbin.org/cookies'</span>)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>这就相当于模拟了一个会话，比如做登陆验证，可以用<code>session</code>，POST 一下，登陆一下，然后保持会话信息，在访问登录过页面的话，就可以正常获取登录后的页面了。如果你要模拟登录，可以通过申明<code>Session</code>对象，再用<code>Session</code>对象发起两次get请求，那么这两次请求相当于在一个浏览器里面，先访问<code>set cookie</code>页面，在访问<code>get cookie</code>页面。当然，<code>cookie</code>是自动处理的，不需要担心写一些处理<code>cookies</code>的方法。</p><p>建议模拟登录用<code>requests</code>的<code>Session</code>对象。</p><h3 id="4-SSL证书验证"><a href="#4-SSL证书验证" class="headerlink" title="4.SSL证书验证"></a>4.SSL证书验证</h3><p><code>Requests</code>可以为 HTTPS 请求验证 SSL 证书，就像 web浏览器一样。要想检查某个主机的 SSL证书，你可以使用 verify参数:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">import</span> requests</div><div class="line"></div><div class="line">&gt;&gt;&gt;requests.get(<span class="string">'https://kennethreitz.com'</span>, verify=<span class="keyword">True</span>) <span class="comment"># verify参数默认值为True</span></div><div class="line">requests.exceptions.SSLError: hostname <span class="string">'kennethreitz.com'</span> doesn<span class="string">'t match either of '</span>*.herokuapp.com<span class="string">', '</span>herokuapp.com<span class="string">'</span></div></pre></td></tr></table></figure><p>如果不想他报这个错误，我们可以把参数<code>verify</code>的值设为<code>False</code>.运行后发现程序没有报错，但是会出现警告信息，警告我们要验证 SSL证书。如果要消除这个警告，我们需要调用原生包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">import</span> requests</div><div class="line">&gt;&gt;&gt;<span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</div><div class="line"></div><div class="line">urllib3.disable_warnings()</div><div class="line">&gt;&gt;&gt;requests.get(<span class="string">'https://kennethreitz.com'</span>, verify=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><p>我们还可以自己指定一个证书：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">import</span> requests</div><div class="line"></div><div class="line">&gt;&gt;&gt;resp = requests.get(<span class="string">'https://kennethreitz.com'</span>, cert=(<span class="string">'/path/server.crt'</span>, <span class="string">'/path/key'</span>))</div><div class="line">&gt;&gt;&gt;print(resp.status_code)</div><div class="line"><span class="number">200</span></div></pre></td></tr></table></figure><h3 id="5-代理设置"><a href="#5-代理设置" class="headerlink" title="5.代理设置"></a>5.代理设置</h3><p>有些网站会限制 IP 访问频率，超过频率就断开连接。这个时候我们就需要使用到代理，我们可以通过为任意请求方式提供<code>proxies</code>参数来配置单个请求。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">proxies = &#123;</div><div class="line">    <span class="string">"http"</span>: <span class="string">"http://10.10.1.10:3128"</span>,</div><div class="line">    <span class="string">"https"</span>: <span class="string">"http://10.10.1.10:1080"</span>,</div><div class="line">&#125;</div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com'</span>, proxies=proxies)</div><div class="line">print(resp.status_code)</div></pre></td></tr></table></figure><p>也可以通过环境变量 <code>HTTP_PROXY</code> 和 <code>HTTPS_PROXY</code> 来配置代理。<br>有些代理需要加上用户名和密码的，代理可以使用<code>http://user:password@host/</code>语法，比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">proxies = &#123;</div><div class="line">    <span class="string">"http"</span>: <span class="string">"http://user:pass@10.10.1.10:3128/"</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>除了基本的 HTTP代理，<code>Requests</code>还支持<code>SOCKS</code>协议的代理，如果需要用的，可以安装带三方库：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ pip install requests[socks]</div></pre></td></tr></table></figure><p>安装好依赖以后，使用 SOCKS 代理和使用 HTTP 代理一样简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">proxies = &#123;</div><div class="line">    <span class="string">"http"</span>: <span class="string">'socks5://user:pass@host:port'</span>,</div><div class="line">    <span class="string">"https"</span>: <span class="string">'socks5://user:pass@host:port'</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="6-超时设置"><a href="#6-超时设置" class="headerlink" title="6.超时设置"></a>6.超时设置</h3><p>超时设置就是设置请求的时间，如果在规定的时间内没有返回应答，就抛出异常.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com'</span>, timeout=<span class="number">0.5</span>)</div><div class="line">print(resp.status_code)</div></pre></td></tr></table></figure><p>如果在0.5秒内没有返回，就会报出<code>ReadTimeout</code>的异常。<br>如果远端服务器很慢，你可以让<code>Request</code>永远等待，传入一个<code>None</code>作为<code>timeout</code>值，然后就冲咖啡去吧。</p><h3 id="7-认证设置"><a href="#7-认证设置" class="headerlink" title="7.认证设置"></a>7.认证设置</h3><p>有一些网站在访问的时候需要我们输入用户名和密码，那么这种网站我们要怎样处理呢。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> requests.auth <span class="keyword">import</span> HTTPBasicAuth</div><div class="line"></div><div class="line">resp = requests.get(url, auth=HTTPBasicAuth(<span class="string">'username'</span>,<span class="string">'password'</span>))</div><div class="line">print(resp.status_code)</div></pre></td></tr></table></figure><p>调用<code>HTTPBasicAuth</code>类，直接传入用户名和密码就可以了。</p><h3 id="8-异常处理"><a href="#8-异常处理" class="headerlink" title="8.异常处理"></a>8.异常处理</h3><p>如果你遇到无法访问的网站，或者是你的网速不够快，你的访问超时，就会导致程序的中断。显然我们在实际的抓取中不愿意看到爬取到一半的程序突然中断的情况，那么我们能够避免这种程序中断的情况吗，答案是肯定的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ReadTimeout, ConnectionError, RequestException</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    resp = requests.get(<span class="string">'http://httpbin.org/get'</span>, timeout=<span class="number">0.5</span>)</div><div class="line">    print(resp.status_code)</div><div class="line"><span class="keyword">except</span> ReadTimeout： <span class="comment"># 访问超时的错误</span></div><div class="line">    print(<span class="string">'Timeout'</span>)</div><div class="line"><span class="keyword">except</span> ConnectionError: <span class="comment"># 网络中断连接错误</span></div><div class="line">    print(<span class="string">'Connect error'</span>)</div><div class="line"><span class="keyword">except</span> RequestException: <span class="comment"># 父类错误</span></div><div class="line">    print(<span class="string">'Error'</span>)</div></pre></td></tr></table></figure><p>这样我们就可以把<code>requests</code>抓取过程中常见的异常都处理捕获了，捕获错误应该先捕获子类异常在捕获父类异常，这样做能够更加直观清楚的应对程序中出现的错误了。</p><p>如果我们能够自己捕获了这些异常，就可以保证我们的爬虫一直运行了。</p><p>好了，<code>Requests</code>的大部分用法已经全部说完了，大家是否已经学会了这门屠龙之术了呢。快找个网页练练手吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇文章我们整理了&lt;code&gt;Requests&lt;/code&gt;库的基本用法，相信大家已经经过爬取一些简单网页的练习，已经很熟练了。&lt;/p&gt;
&lt;p&gt;这一篇文章我们来 看一下&lt;code&gt;Requests&lt;/code&gt;库的高级操作。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(2):Requests的基本用法</title>
    <link href="https://wyukun.coding.me/2017/05/29/Python%E7%88%AC%E8%99%AB%E4%BA%8C%E4%B9%8BRequests%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/"/>
    <id>https://wyukun.coding.me/2017/05/29/Python爬虫二之Requests的基本用法/</id>
    <published>2017-05-29T15:08:56.000Z</published>
    <updated>2017-09-16T04:33:48.308Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>虽然Python有内置的<code>urllib</code>库，可以实现网络的请求，但是我并不推荐。因为<code>urllib</code>在很多时候使用起来不方便，比如加一个代理，处理<code>Cookie</code>时API都很繁琐，再比如发送一个<code>POST</code>请求也很麻烦。</p><p>而<code>Requests</code>就相当于<code>urllib</code>的升级版本，简化了<code>urllib</code>的使用方法。有了<code>Requests</code>，我们可以用几句代码实现代理的设置，<code>Cookie</code>的设置，非常方便。下面我就给大家整理了<code>Requests</code>库的使用方法和细节。详细可以参考<code>Requests</code><a href="http://docs.python-requests.org/zh_CN/latest/" target="_blank" rel="external">官方文档</a>。<br><a id="more"></a></p><h2 id="什么是Requests？"><a href="#什么是Requests？" class="headerlink" title="什么是Requests？"></a>什么是Requests？</h2><p><code>Requests</code>是<code>Python</code>语言编写，基于<code>urllib3</code>，采用<code>Apache2 Licensed</code>开源协议的HTTP库。</p><p>它比<code>urllib</code>更加方便，可以节约我们大量的工作，完全满足<code>HTTP</code>测试需求。是<code>Python</code>实现的简单易用的<code>HTTP</code>库。</p><p>安装也很简单：<code>pip install requests</code></p><h2 id="Requests的语法操作"><a href="#Requests的语法操作" class="headerlink" title="Requests的语法操作"></a>Requests的语法操作</h2><h3 id="1-实例引入"><a href="#1-实例引入" class="headerlink" title="1.实例引入"></a>1.实例引入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">response = requests.get(<span class="string">'http://www.baidu.com/'</span>)</div><div class="line">print(response.status_code)</div><div class="line">print(type(response.text))</div><div class="line">print(response.text)</div><div class="line">print(response.cookies)</div></pre></td></tr></table></figure><p>运行结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">200</div><div class="line">&lt;class &apos;str&apos;&gt;</div><div class="line"> </div><div class="line"># ...HTML网页源码..</div><div class="line">&lt;RequestsCookieJar[]&gt;</div></pre></td></tr></table></figure></p><p>可以看到，我们非常方便的就获取到了<code>Cookies</code>.</p><h3 id="2-各种请求方式"><a href="#2-各种请求方式" class="headerlink" title="2.各种请求方式"></a>2.各种请求方式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">requests.get(<span class="string">'http://httpbin.org/get'</span>) <span class="comment"># 发送get请求</span></div><div class="line">requests.post(<span class="string">'http://httpbin.org/post'</span>) <span class="comment"># 发送post请求，只要调用post方法，传入一个url参数</span></div><div class="line">requests.put(<span class="string">'http://httpbin.org/put'</span>)</div><div class="line">requests.delete(<span class="string">'http://httpbin.org/delete'</span>)</div></pre></td></tr></table></figure><p>官方文档里提供的这个网址足够我们测试这些请求方式了。</p><h2 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h2><h3 id="1-基本GET请求"><a href="#1-基本GET请求" class="headerlink" title="1.基本GET请求"></a>1.基本GET请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'http://httpbin.org/get'</span>)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>这个我们前面有使用过，也是最常用的方法。运行成功就可以看到网页的源码了。</p><h3 id="2-带参数的GET请求"><a href="#2-带参数的GET请求" class="headerlink" title="2.带参数的GET请求"></a>2.带参数的GET请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">data = &#123;</div><div class="line">    <span class="string">'name'</span> : <span class="string">'jack'</span>,</div><div class="line">    <span class="string">'age'</span> : <span class="number">20</span></div><div class="line">&#125;</div><div class="line">resp = requests.get(<span class="string">'http://httpbin.org/get'</span>, params=data)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>传入参数只需要我们把数据生成一个字典，然后调用<code>params</code>参数，赋值给他就可以，是不是很方便。</p><h3 id="3-解析json"><a href="#3-解析json" class="headerlink" title="3.解析json"></a>3.解析json</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> json</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'http://httpbin.org/get'</span>)</div><div class="line">print(resp.text)</div><div class="line">print(resp.json())</div><div class="line">print(json.loads(resp.text))</div><div class="line">print(type(resp.json()))</div></pre></td></tr></table></figure><p>运行结果：</p><p><img src="http://imgout.ph.126.net/55901018/QQCDBCC6AC20170529120909.jpg" alt="image"></p><p>可以看出<code>Requests</code>的<code>jaon</code>解析和<code>json</code>的<code>loads</code>方法解析出来的结果是完全一样的。所以<code>Requests</code>可以很方便的解析<code>json</code>数据。</p><h3 id="4-获取二进制数据"><a href="#4-获取二进制数据" class="headerlink" title="4.获取二进制数据"></a>4.获取二进制数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com/img/baidu_jgylogo3.gif'</span>)</div><div class="line">print(resp.content)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>运行成功我们可以看到<code>content</code>方法获取的图片页面源码是二进制数据，而<code>text</code>获取的则是字符串代码。显然获取图片这种二进制数据需要使用<code>content</code>方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> open(<span class="string">'logo.gif'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(resp.content)</div></pre></td></tr></table></figure><p>这样我们就保存了图片，我们可以在文件夹下看到这张图片。</p><h3 id="5-添加headers"><a href="#5-添加headers" class="headerlink" title="5.添加headers"></a>5.添加headers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com'</span>, headers=headers)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>有些网页如果我们直接去请求的话，他会查看请求的对象是不是浏览器，如果没有浏览器信息就会禁止我们爬虫的访问，这个时候我们就要给爬虫加一个<code>headers</code>，加一个浏览器的<code>user-agent</code>信息。这样我们就可以正常访问了。如果有的伙伴不知道怎么得到<code>User-Agent</code>，可以打开浏览器的审查元素，找到<code>network</code>，随便点击一个链接就可以看到<code>User-Agent</code>的信息了。</p><p><img src="http://imgout.ph.126.net/55898004/QQCDBCC6AC20170529131930.jpg" alt="image"></p><h3 id="6-基本POST请求"><a href="#6-基本POST请求" class="headerlink" title="6.基本POST请求"></a>6.基本POST请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">data = &#123;</div><div class="line">    <span class="string">'name'</span> : <span class="string">'jack'</span>,</div><div class="line">    <span class="string">'age'</span> : <span class="number">20</span></div><div class="line">&#125;</div><div class="line">resp = requests.post(<span class="string">'http://httpbin.org/post'</span>, data=data)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>一个<code>POST</code>必然是要有一个<code>Form Data</code>的表单提交的，我们只要把信息传给<code>data</code>参数就可以了。一个<code>POST</code>请求只需要调用<code>post</code>方法，是不是特别方便呢。如果不觉得方便的话，可以去参考<code>urllib</code>的使用方法。</p><h2 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h2><h3 id="1-response属性"><a href="#1-response属性" class="headerlink" title="1.response属性"></a>1.response属性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">response = requests.get(<span class="string">'http://www.baidu.com/'</span>)</div><div class="line">print(type(response.status_code)) <span class="comment"># 状态码</span></div><div class="line">print(type(response.text)) <span class="comment"># 网页源码</span></div><div class="line">print(type(response.headers)) <span class="comment"># 头部信息</span></div><div class="line">print(type(response.cookies)) <span class="comment"># Cookie</span></div><div class="line">print(type(response.url)) <span class="comment"># 请求的url</span></div><div class="line">print(type(response.history)) <span class="comment"># 访问的历史记录</span></div></pre></td></tr></table></figure><p>获取这些信息只需要简单的调用就可以实现了。</p><h3 id="2-状态码判断"><a href="#2-状态码判断" class="headerlink" title="2.状态码判断"></a>2.状态码判断</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">import</span> requests</div><div class="line"> </div><div class="line">&gt;&gt;&gt;response = requests.get(<span class="string">'http://www.baidu.com/'</span>)</div><div class="line">&gt;&gt;&gt;exit() <span class="keyword">if</span> <span class="keyword">not</span> resp.status_code == <span class="number">200</span> <span class="keyword">else</span> print(<span class="string">'Sucessful'</span>)</div><div class="line">Sucessful</div></pre></td></tr></table></figure><p>如果发送了一个错误请求(一个4XX客户端错误，或者5XX服务器错误响应)，我们可以通过 <code>Response.raise_for_status()</code> 来抛出异常：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;bad_r = requests.get(<span class="string">'http://httpbin.org/status/404'</span>)</div><div class="line">&gt;&gt;&gt;bad_r.status_code</div><div class="line"><span class="number">404</span></div><div class="line">&gt;&gt;&gt;bad_r.raise_for_status()</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"requests/models.py"</span>, line <span class="number">832</span>, <span class="keyword">in</span> raise_for_status</div><div class="line">    <span class="keyword">raise</span> http_error</div><div class="line">requests.exceptions.HTTPError: <span class="number">404</span> Client Error</div></pre></td></tr></table></figure><p>好了，这篇文章我们了解了<code>Requests</code>库的基本语法操作，相信大家对<code>Requests</code>库的请求和响应已经很清楚了，大家完全可以抓取一些网页了。</p><p>纸上得来终觉浅，绝知此事要躬行，大家加油！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然Python有内置的&lt;code&gt;urllib&lt;/code&gt;库，可以实现网络的请求，但是我并不推荐。因为&lt;code&gt;urllib&lt;/code&gt;在很多时候使用起来不方便，比如加一个代理，处理&lt;code&gt;Cookie&lt;/code&gt;时API都很繁琐，再比如发送一个&lt;code&gt;POST&lt;/code&gt;请求也很麻烦。&lt;/p&gt;
&lt;p&gt;而&lt;code&gt;Requests&lt;/code&gt;就相当于&lt;code&gt;urllib&lt;/code&gt;的升级版本，简化了&lt;code&gt;urllib&lt;/code&gt;的使用方法。有了&lt;code&gt;Requests&lt;/code&gt;，我们可以用几句代码实现代理的设置，&lt;code&gt;Cookie&lt;/code&gt;的设置，非常方便。下面我就给大家整理了&lt;code&gt;Requests&lt;/code&gt;库的使用方法和细节。详细可以参考&lt;code&gt;Requests&lt;/code&gt;&lt;a href=&quot;http://docs.python-requests.org/zh_CN/latest/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方文档&lt;/a&gt;。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(1):基本原理</title>
    <link href="https://wyukun.coding.me/2017/05/28/Python%E7%88%AC%E8%99%AB%E4%B8%80%E4%B9%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/"/>
    <id>https://wyukun.coding.me/2017/05/28/Python爬虫一之基本原理/</id>
    <published>2017-05-28T14:44:37.000Z</published>
    <updated>2017-09-16T06:25:37.932Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>大家好，今天周末，希望大家周末愉快。</p><p>这篇文章我来梳理一下爬虫的基本原理。用过<code>Python</code>的伙伴都知道<code>Python</code>用来写爬虫是件很简单很爽的事情。但是有些伙伴不了解爬虫到底是什么，会纳闷为什么爬虫要设置什么请求头、请求体啊，到底要怎么样去发送这个请求啊。当然有的伙伴使用过很多爬虫的请求库，解析库，写过很多爬虫，但是可能他们的思路脉络也不是太清晰。那么，这篇文章就来帮大家理清爬虫原理的脉络。<br><a id="more"></a></p><h2 id="什么是爬虫？"><a href="#什么是爬虫？" class="headerlink" title="什么是爬虫？"></a>什么是爬虫？</h2><p><img src="http://imgout.ph.126.net/55876004/spider-1673428_960_720.jpg" alt="spider"></p><p>爬虫即网络爬虫，英文是<code>Web Spider</code>。翻译过来就是网络上爬行的蜘蛛，如果把互联网看作一张大网，那么爬虫就是在大网上爬来爬去的蜘蛛，碰到想要的食物，就把他抓取出来。</p><p>我们在浏览器中输入一个网址，敲击回车，看到网站的页面信息。这就是浏览器请求了网站的服务器，获取到网络资源。那么，爬虫也相当于模拟浏览器发送请求，获得到<code>HTML</code>代码。<code>HTML</code>代码里通常包含了标签和文字信息，我们就从中提取到我们想要的信息。</p><p>通常爬虫是从某个网站的某个页面开始，爬取这个页面的内容，找到网页中的其他链接地址，然后从这个地址爬到下一个页面，这样一直不停的爬下去，进去批量的抓取信息。那么，我们可以看出网络爬虫就是一个不停爬取网页抓取信息的程序。</p><h2 id="爬虫的基本流程"><a href="#爬虫的基本流程" class="headerlink" title="爬虫的基本流程"></a>爬虫的基本流程</h2><h3 id="1-发起请求："><a href="#1-发起请求：" class="headerlink" title="1.发起请求："></a>1.发起请求：</h3><p>通过HTTP库向目标站点发起请求，即发送一个<code>Request</code>，请求可以包含额外的<code>headers</code>等信息，然后等待服务器响应。这个请求的过程就像我们打开浏览器，在浏览器地址栏输入网址：<code>www.baidu.com</code>，然后点击回车。这个过程其实就相当于浏览器作为一个浏览的客户端，向服务器端发送了 一次请求。</p><h3 id="2-获取响应内容："><a href="#2-获取响应内容：" class="headerlink" title="2.获取响应内容："></a>2.获取响应内容：</h3><p>如果服务器能正常响应，我们会得到一个<code>Response</code>，<code>Response</code>的内容便是所要获取的内容，类型可能有<code>HTML</code>、<code>Json</code>字符串，二进制数据(图片，视频等）等类型。这个过程就是服务器接收客户端的请求，进过解析发送给浏览器的网页<code>HTML</code>文件。</p><h3 id="3-解析内容："><a href="#3-解析内容：" class="headerlink" title="3.解析内容："></a>3.解析内容：</h3><p>得到的内容可能是<code>HTML</code>，可以使用正则表达式，网页解析库进行解析。也可能是<code>Json</code>，可以直接转为<code>Json</code>对象解析。可能是二进制数据，可以做保存或者进一步处理。这一步相当于浏览器把服务器端的文件获取到本地，再进行解释并且展现出来。</p><h3 id="4-保存数据："><a href="#4-保存数据：" class="headerlink" title="4.保存数据："></a>4.保存数据：</h3><p>保存的方式可以是把数据存为文本，也可以把数据保存到数据库，或者保存为特定的jpg，mp4 等格式的文件。这就相当于我们在浏览网页时，下载了网页上的图片或者视频。</p><h2 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h2><h3 id="1-什么是Request？"><a href="#1-什么是Request？" class="headerlink" title="1.什么是Request？"></a>1.什么是Request？</h3><p>浏览器发送信息给该网址所在的服务器，这个过程就叫做<code>HTTP Request</code>。</p><h3 id="2-Request中包含什么？"><a href="#2-Request中包含什么？" class="headerlink" title="2.Request中包含什么？"></a>2.Request中包含什么？</h3><ul><li>请求方式：请求方式的主要类型是<code>GET</code>，<code>OST</code>两种，另外还有<code>HEAD</code>、<code>PUT</code>、<code>DELETE</code>等。GET 请求的请求参数会显示在URL链接的后面，比如我们打开百度，搜索“图片”，我们会看到请求的URL链接为<code>https://www.baidu.com/s?wd=图片</code>。而 POST 请求的请求参数会存放在<code>Request</code>内，并不会出现在 URL 链接的后面，比如我们登录知乎，输入用户名和密码，我们会看到浏览器开发者工具的<code>Network</code>页，<code>Request</code>请求有<code>Form Data</code>的键值对信息，那里就存放了我们的登录信息，有利于保护我们的账户信息安全；</li><li>请求 URL：URL 全称是统一资源定位符，也就是我们说的网址。比如一张图片，一个音乐文件，一个网页文档等都可以用唯一URL来确定，它包含的信息指出文件的位置以及浏览器应该怎么去处理它；</li><li>请求头<code>(Request Headers)</code>：请求头包含请求时的头部信息，如<code>User-Agent</code>（指定浏览器的请求头），<code>Host</code>，<code>Cookies</code>等信息；</li><li>请求体：请求体是请求是额外携带的数据，比如登录表单提交的登录信息数据。</li></ul><h2 id="Response"><a href="#Response" class="headerlink" title="Response"></a>Response</h2><h3 id="1-什么是Response？"><a href="#1-什么是Response？" class="headerlink" title="1.什么是Response？"></a>1.什么是Response？</h3><p>服务器收到浏览器发送的信息后，能够根据浏览器发送信息的内容，做出相应的处理，然后把消息回传给浏览器，这个过程就叫做<code>HTTP Response</code>。</p><h3 id="2-Response中包含什么？"><a href="#2-Response中包含什么？" class="headerlink" title="2.Response中包含什么？"></a>2.Response中包含什么？</h3><ul><li>响应状态：有多种响应状态，比如200代表成功，301 跳转页面，404 表示找不到页面，502 表示服务器错误；</li><li>响应头<code>(Response Headers)</code>：比如内容类型，内容长度，服务器信息，设置<code>Cookie</code>等；</li><li>响应体：响应体最主要的部分，包含了请求资源的内容，比如网页 HTML 代码，图片二进制数据等。</li></ul><h2 id="简单演示"><a href="#简单演示" class="headerlink" title="简单演示"></a>简单演示</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests <span class="comment"># 导入requests库，需要安装</span></div><div class="line"></div><div class="line"><span class="comment"># 模拟成浏览器访问的头</span></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(<span class="string">'https://www.baidu.com'</span>,headers=headers)</div><div class="line">print(resp.text) <span class="comment"># 打印出网页源代码</span></div><div class="line">print(resp.status_code) <span class="comment"># 打印出状态码</span></div></pre></td></tr></table></figure><p>运行成功后可以看到打印出来的 html 源代码和 200 状态码了。这就基本上实现了爬虫的<code>Request</code>和<code>Response</code>的过程。</p><h2 id="能抓到什么样的数据？"><a href="#能抓到什么样的数据？" class="headerlink" title="能抓到什么样的数据？"></a>能抓到什么样的数据？</h2><ul><li>网页文本：如 HTML 文档，<code>Ajax</code>加载的<code>Json</code>格式文本等；</li><li>图片，视频等：获取到的是二进制文件，保存为图片或视频格式；</li><li>其他只要能请求到的，都能获取。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com/img/baidu_jgylogo3.gif'</span>,headers=headers)</div><div class="line">print(resp.content) <span class="comment"># 二进制文件使用content</span></div><div class="line"><span class="comment"># 保存图片</span></div><div class="line"><span class="keyword">with</span> open(<span class="string">'logo.gif'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(resp.content)</div><div class="line">    print(<span class="string">'Ok'</span>)</div></pre></td></tr></table></figure><p>成功运行就可以看到打印出来的图片的二进制数据，可保存成功后打印的 OK，这个时候我们打开文件夹就可以看到下载下来的图片了。这几行代码就简单的演示了爬虫保存文件的过程。</p><h2 id="解析方式有哪些？"><a href="#解析方式有哪些？" class="headerlink" title="解析方式有哪些？"></a>解析方式有哪些？</h2><ul><li>直接处理，比如简单的页面文档，只要去除一些空格的数据；</li><li>Json解析，处理Ajax加载的页面；</li><li>正则表达式；</li><li>BeautifulSoup库；</li><li>PyQuery；</li><li>XPath。</li></ul><p>看到这里，大家是不是已经对爬虫的基本工作原理有了清晰的认识了呢。当然，罗马并不是一天建成的，只要积累了足够多的经验，大家肯定能成为爬虫大神的。</p><p>接下来，我会给大家继续分享一些我在学习爬虫时的经验总结。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大家好，今天周末，希望大家周末愉快。&lt;/p&gt;
&lt;p&gt;这篇文章我来梳理一下爬虫的基本原理。用过&lt;code&gt;Python&lt;/code&gt;的伙伴都知道&lt;code&gt;Python&lt;/code&gt;用来写爬虫是件很简单很爽的事情。但是有些伙伴不了解爬虫到底是什么，会纳闷为什么爬虫要设置什么请求头、请求体啊，到底要怎么样去发送这个请求啊。当然有的伙伴使用过很多爬虫的请求库，解析库，写过很多爬虫，但是可能他们的思路脉络也不是太清晰。那么，这篇文章就来帮大家理清爬虫原理的脉络。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>五月的事</title>
    <link href="https://wyukun.coding.me/2017/05/25/%E4%BA%94%E6%9C%88%E7%9A%84%E4%BA%8B/"/>
    <id>https://wyukun.coding.me/2017/05/25/五月的事/</id>
    <published>2017-05-25T11:43:14.000Z</published>
    <updated>2017-09-16T04:33:43.158Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>之前无聊的时候写过很多博客，但是那时候这个网站还没有上线，大多博客都写在知乎上。但是一些牢骚话自然不会发在知乎的，都是保存了草稿，然后自己删掉，发布的都是 Python 的分享。毕竟申请的是 Python 专栏，<a href="https://zhuanlan.zhihu.com/yukun" target="_blank" rel="external">专栏地址</a>。其实这种感觉其实特别难受，好比打了一半的嗝又被咽回去了。<br><a id="more"></a></p><p><img src="http://imgout.ph.126.net/55814031/55814031.jpg" alt="image"></p><iframe src="//music.163.com/outchain/player?type=2&amp;id=31445772&amp;auto=0&amp;height=66" width="330" height="86" frameborder="no" marginwidth="0" marginheight="0"></iframe><p>转眼五月就要结束了，时间就像一匹欢快的小马，你永远抓不住他的尾巴。人家说天上一日，地上一年，大概就是说快乐地时光总是过得很快。上一次有时间的概念是网站备案的时候，阿里云说要等十几天才能备案成功，我才感觉时间漫长。一天一天过去，备案的十几天似乎还很遥远。可渐渐的忘记这件事的时候，时间的小马就匆匆跑过来告诉你，十几天又过去了，备案通过了，马上到六月了。</p><p>回想这个月，我也想不到自己到底干了些什么。处了这个网站成功上线，在知乎写了两篇博客，其他都没什么印象。每天重复的3点式生活占去了大部分记忆。不过也很好，毕竟这样也让我少去很多不必要的打扰。这个月窝在宿舍倒是把廖雪峰 JavaScript 的教程看了一遍，也算不枉这个月的时光。写到这我突然明白，如果一直这样与美丽的姑娘们毫无交集，可能就永远单身了=。=。</p><p>这个月开始天气开始热起来，越来越有夏天的感觉了。天气一热，心就开始浮躁，连看一部完整电影的耐心都没有。希望今后能静下心来，非宁静无以致远吗。毕竟专业课还有很多作业要补，下个月在不全部上交估计这门课要挂，欠下的总是要还的，这话说的一点没错。</p><p>今天是五月二十五日，在过五天就是端午节，虽然不会有多少人可以看到，我还是要提前衷心的祝大家端午快乐。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前无聊的时候写过很多博客，但是那时候这个网站还没有上线，大多博客都写在知乎上。但是一些牢骚话自然不会发在知乎的，都是保存了草稿，然后自己删掉，发布的都是 Python 的分享。毕竟申请的是 Python 专栏，&lt;a href=&quot;https://zhuanlan.zhihu.com/yukun&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;专栏地址&lt;/a&gt;。其实这种感觉其实特别难受，好比打了一半的嗝又被咽回去了。&lt;br&gt;
    
    </summary>
    
      <category term="生活" scheme="https://wyukun.coding.me/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="随笔" scheme="https://wyukun.coding.me/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
</feed>

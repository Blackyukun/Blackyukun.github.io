<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>意外</title>
  
  <subtitle>俞坤的博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wyukun.coding.me/"/>
  <updated>2017-09-16T07:29:31.964Z</updated>
  <id>https://wyukun.coding.me/</id>
  
  <author>
    <name>俞坤</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python爬虫(5):豆瓣读书练手爬虫</title>
    <link href="https://wyukun.coding.me/2017/06/03/Python%E7%88%AC%E8%99%AB%E4%BA%94%E4%B9%8B%E8%B1%86%E7%93%A3%E8%AF%BB%E4%B9%A6%E7%BB%83%E6%89%8B%E7%88%AC%E8%99%AB/"/>
    <id>https://wyukun.coding.me/2017/06/03/Python爬虫五之豆瓣读书练手爬虫/</id>
    <published>2017-06-03T07:14:39.000Z</published>
    <updated>2017-09-16T07:29:31.964Z</updated>
    
    <content type="html"><![CDATA[<p>我们在之前的文章中基本上掌握了<code>Python</code>爬虫的原理和方法，不知道大家有没有练习呢。今天我就来找一个简单的网页进行爬取，就当是给之前的兵书做一个实践。不然不就是纸上谈兵的赵括了吗。</p><p>好了，我们这次的目标是<code>豆瓣图书Top250</code>，地址是：<code>https://book.douban.com/top250?start=0</code><br><a id="more"></a></p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>爬一个网页我们至少要知道自己需要什么信息，大家看截图：</p><p><img src="http://imgout.ph.126.net/55981022/QQCDBCC6AC20170603171444.jpg" alt="image"></p><p>红色箭头标记的地方就是我们要获取的信息了，包括书的名字，作者和出版社信息，豆瓣评分和一句话简介。我们有了目标信息，就需要找到信息所在的页面源码，然后通过解析源码来获取到信息数据。那么，我们怎样获得页面 HTML 源代码呢？翻阅兵书，我们知道可以使用<code>requests</code>之计。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'https://book.douban.com/top250?start=0'</span>)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>运行程序，我们就轻松的获得了敌军的 HTML 信息了。但是问题又来了，我们得到 HTML 信息后，怎样得到我们的目标数据呢？</p><p><strong>深夜了，一轮弯月躲在云朵后面，窗外下着雨，我们坐在烛火前，翻阅兵书，顿时茅塞顿开，BeautifulSoup大法好。</strong></p><p>我们打开浏览器，按<code>f12</code>到开发者工具，我们从网页源码里查找到数据位置，截图如下：</p><p><img src="http://imgout.ph.126.net/55981045/QQCDBCC6AC20170603171452.jpg" alt="image"></p><p>可以看到书名信息包含在<code>class=&#39;pl2&#39;</code> <code>div</code>里面的<code>a</code>标签内，是<code>a</code>标签的<code>title</code>属性。发现目标位置后，就简单多了。我们利用<code>BeautifulSoup</code>来获得一个对象，按找标准的缩进显示的<code>html</code>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line">soup = BeautifulSoup(resp.text, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure><p>推荐大家使用lxml解析器，因为他快。如果安装lxml遇到问题的可以参考 <a href="http://www.yukunweb.com/276.html" target="_blank" rel="external">上一篇文章</a> 的方法。当然，如果大家怕麻烦，也完全可以使用Python的内置标准库<code>html.parser</code>.对我们获得结果并没有影响。</p><h2 id="开始工作"><a href="#开始工作" class="headerlink" title="开始工作"></a>开始工作</h2><p>现在我们要用到<code>BeautifulSoup</code>的<code>find_all()</code>选择器，因为我们这一页有很多书，而每一本书的信息都包含在<code>class=pl2</code>的<code>div</code>标签内，我们使用<code>find_all()</code>就可以直接得到本页所有书的书名了。我们用<code>find()</code>方法和<code>find_all()</code>方法来做一个比较：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># find_all()方法，</span></div><div class="line"><span class="comment"># 注意class是Python关键词，后面要加下划线_：</span></div><div class="line">alldiv = soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'pl2'</span>)</div><div class="line"><span class="keyword">for</span> a <span class="keyword">in</span> alldiv:</div><div class="line">    names = a.find(<span class="string">'a'</span>)[<span class="string">'title'</span>]</div><div class="line">    print(<span class="string">'find_all():'</span>, names)</div><div class="line"><span class="comment"># find()方法：</span></div><div class="line">alldiv2 = soup.find(<span class="string">'div'</span>, class_=<span class="string">'pl2'</span>)</div><div class="line">names2 = alldiv2.find(<span class="string">'a'</span>)[<span class="string">'title'</span>]</div><div class="line">print(<span class="string">'find():'</span>, names2 )</div></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">find_all(): 追风筝的人</div><div class="line">find_all(): 小王子</div><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="comment"># ...省略部分</span></div><div class="line"><span class="comment"># ...</span></div><div class="line">find_all(): 三体Ⅲ</div><div class="line">find(): 追风筝的人</div><div class="line"></div><div class="line">Process finished <span class="keyword">with</span> exit code <span class="number">0</span></div></pre></td></tr></table></figure><p>我们通过结果就可以看到两者之间的差距了，前者输出了一页的数据，而后者只输出了第一条数据。所以包括后面的信息，由于每一天数据所在标签是一样的，我们都是用<code>find_all()</code>方法。</p><p>上面的代码写的优雅点，就是这样实现,注意结果是一个 list：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 书名, 注意是L小写，不是阿拉伯数字1</span></div><div class="line">alldiv = soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'pl2'</span>)</div><div class="line">names = [a.find(<span class="string">'a'</span>)[<span class="string">'title'</span>] <span class="keyword">for</span> a <span class="keyword">in</span> alldiv]</div><div class="line">print(names)</div></pre></td></tr></table></figure><p>这样书名数据我们就得到了，接下来是作者信息。方法和获取书名方法一样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 作者,由于信息在一个p标签内部，</span></div><div class="line"><span class="comment"># 我们获取到标签直接get_text()方法获得文本内容</span></div><div class="line">allp = soup.find_all(<span class="string">'p'</span>, class_=<span class="string">'pl'</span>)</div><div class="line">authors = [p.get_text() <span class="keyword">for</span> p <span class="keyword">in</span> allp]</div></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[&apos;[美] 卡勒德·胡赛尼 / 李继宏 / 上海人民出版社 / 2006-5 / 29.00元&apos;, </div><div class="line">&apos;[法] 圣埃克苏佩里 / 马振聘 / 人民文学出版社 / 2003-8 / 22.00元&apos;, </div><div class="line">&apos;钱锺书 / 人民文学出版社 / 1991-2 / 19.00&apos;, </div><div class="line">&apos;余华 / 南海出版公司 / 1998-5 / 12.00元&apos;, </div><div class="line"># ...</div><div class="line"># ...省略部分结果</div><div class="line"># ...</div><div class="line">&apos;高铭 / 武汉大学出版社 / 2010-2 / 29.80元&apos;, </div><div class="line">&apos;刘慈欣 / 重庆出版社 / 2010-11 / 38.00元&apos;]</div></pre></td></tr></table></figure><p>后面的评分内容和简介内容也是一样获得，只是标签不同，但是方法一样，具体也不需要多余赘述。直接看实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 评分</span></div><div class="line">starspan = soup.find_all(<span class="string">'span'</span>, class_=<span class="string">'rating_nums'</span>)</div><div class="line">scores = [s.get_text() <span class="keyword">for</span> s <span class="keyword">in</span> starspan]</div><div class="line"><span class="comment"># 简介</span></div><div class="line">sumspan = soup.find_all(<span class="string">'span'</span>, class_=<span class="string">'inq'</span>)</div><div class="line">sums = [i.get_text() <span class="keyword">for</span> i <span class="keyword">in</span> sumspan]</div></pre></td></tr></table></figure><p>程序运行成功，我们就获得了4个list，分别是书名，作者，评分和简介内容。我们要把他们放在一起，打印出来，就是一页的数据信息了。<br>这里我们使用<code>zip()</code>函数，<code>zip()</code>函数在运算时，会以一个或多个序列做为参数，返回一个元组的列表。同时将这些序列中并排的元素配对。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> name, author, score, sum <span class="keyword">in</span> zip(names, authors, scores, sums):</div><div class="line">    name = <span class="string">'书名：'</span> + str(name) + <span class="string">'\n'</span></div><div class="line">    author = <span class="string">'作者：'</span> + str(author) + <span class="string">'\n'</span></div><div class="line">    score = <span class="string">'评分：'</span> + str(score) + <span class="string">'\n'</span></div><div class="line">    sum = <span class="string">'简介：'</span> + str(sum) + <span class="string">'\n'</span></div><div class="line">    data = name + author + score + sum</div></pre></td></tr></table></figure><p>我们使用换行符’<code>\n</code>‘给数据信息一点整齐的样式。我们可以查看到打印的结果，并没有所有数据黏在一起，显得丑陋。<br>获得信息后，就是保存数据了。保存数据也很简单，<code>Python</code>的文件读写操作就可以实现。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 文件名</span></div><div class="line">filename = <span class="string">'豆瓣图书Top250.txt'</span></div><div class="line"><span class="comment"># 保存文件操作</span></div><div class="line"><span class="keyword">with</span> open(filename, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</div><div class="line">    <span class="comment"># 保存数据</span></div><div class="line">    f.writelines(data + <span class="string">'======================='</span> + <span class="string">'\n'</span>)</div><div class="line">print(<span class="string">'保存成功'</span>)</div></pre></td></tr></table></figure><p>运行成功，我们就可以看到项目文件下面的 txt 文件了，上面保存了我们上面打印出来的内容。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">书名：追风筝的人</div><div class="line">作者：[美] 卡勒德·胡赛尼 / 李继宏 / 上海人民出版社 / 2006-5 / 29.00元</div><div class="line">评分：8.8</div><div class="line">简介：为你，千千万万遍</div><div class="line">==================</div><div class="line"># ...</div><div class="line"># ...</div><div class="line">书名：活着</div><div class="line">作者：余华 / 南海出版公司 / 1998-5 / 12.00元</div><div class="line">评分：9.1</div><div class="line">简介：活着本身就是人生最大的意义</div><div class="line">==================</div></pre></td></tr></table></figure><p>但是，我们要的是 250 条数据，而不是一页的十几条数据，那么要怎么获得到所有的数据呢。我们可以检查页面的信息，可以看到页面一共 10 页，第一页的URL是<code>https://book.douban.com/top250?start=0</code>。而最后一页的 URL 是<code>https://book.douban.com/top250?start=225</code><br>我们接着多看几页，第二页是<code>https://book.douban.com/top250?start=25</code>，第三页是<code>https://book.douban.com/top250?start=50</code>。</p><p>规律已经很清晰了，我们的页面的页数信息是最后的<code>start=</code>后面的数字。而且数字从0开始到225，每一页数字加 25.这就很简单了，我们以<code>https://book.douban.com/top250?start=</code>为基层URL，每一页在后面加页面的页数数字。就可以得到所有的页面 url 了。再以<code>for</code>循环迭代每一个 url，使用上面获取数据的方法，获得所有的数据信息。</p><p>获取所有页面URL的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">base_url = <span class="string">'https://book.douban.com/top250?start='</span></div><div class="line">urllist = []</div><div class="line"><span class="comment"># 从0到225，间隔25的数组</span></div><div class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">250</span>, <span class="number">25</span>):</div><div class="line">    allurl = base_url + str(page)</div><div class="line">    urllist.append(allurl)</div></pre></td></tr></table></figure><p>我们把他保存在 list 里面，好用循环迭代。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>那么，所有的功能都实现了。现在，我们只要将所有的代码组合起来，就可以实现我们需要的所有功能了。<br>上代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding:utf-8 -*-</span></div><div class="line"><span class="comment">#  author: yukun</span></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 发出请求获得HTML源码的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_html</span><span class="params">(url)</span>:</span></div><div class="line"><span class="comment"># 伪装成浏览器访问</span></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(url, headers=headers).text</div><div class="line"></div><div class="line"><span class="keyword">return</span> resp</div><div class="line"></div><div class="line"><span class="comment"># 解析页面，获得数据信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_parse</span><span class="params">()</span>:</span></div><div class="line"><span class="comment"># 调用函数，for循环迭代出所有页面</span></div><div class="line"><span class="keyword">for</span> url <span class="keyword">in</span> all_page():</div><div class="line"><span class="comment"># BeautifulSoup的解析</span></div><div class="line">soup = BeautifulSoup(get_html(url), <span class="string">'lxml'</span>)</div><div class="line"><span class="comment"># 书名</span></div><div class="line">alldiv = soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'pl2'</span>)</div><div class="line">names = [a.find(<span class="string">'a'</span>)[<span class="string">'title'</span>] <span class="keyword">for</span> a <span class="keyword">in</span> alldiv]</div><div class="line"><span class="comment"># 作者</span></div><div class="line">allp = soup.find_all(<span class="string">'p'</span>, class_=<span class="string">'pl'</span>)</div><div class="line">authors = [p.get_text() <span class="keyword">for</span> p <span class="keyword">in</span> allp]</div><div class="line"><span class="comment"># 评分</span></div><div class="line">starspan = soup.find_all(<span class="string">'span'</span>, class_=<span class="string">'rating_nums'</span>)</div><div class="line">scores = [s.get_text() <span class="keyword">for</span> s <span class="keyword">in</span> starspan]</div><div class="line"><span class="comment"># 简介</span></div><div class="line">sumspan = soup.find_all(<span class="string">'span'</span>, class_=<span class="string">'inq'</span>)</div><div class="line">sums = [i.get_text() <span class="keyword">for</span> i <span class="keyword">in</span> sumspan]</div><div class="line"><span class="keyword">for</span> name, author, score, sum <span class="keyword">in</span> zip(names, authors, scores, sums):</div><div class="line">name = <span class="string">'书名：'</span> + str(name) + <span class="string">'\n'</span></div><div class="line">author = <span class="string">'作者：'</span> + str(author) + <span class="string">'\n'</span></div><div class="line">score = <span class="string">'评分：'</span> + str(score) + <span class="string">'\n'</span></div><div class="line">sum = <span class="string">'简介：'</span> + str(sum) + <span class="string">'\n'</span></div><div class="line">data = name + author + score + sum</div><div class="line"><span class="comment"># 保存数据</span></div><div class="line">f.writelines(data + <span class="string">'======================='</span> + <span class="string">'\n'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 获得所有页面的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">all_page</span><span class="params">()</span>:</span></div><div class="line">base_url = <span class="string">'https://book.douban.com/top250?start='</span></div><div class="line">urllist = []</div><div class="line"><span class="comment"># 从0到225，间隔25的数组</span></div><div class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">250</span>, <span class="number">25</span>):</div><div class="line">allurl = base_url + str(page)</div><div class="line">urllist.append(allurl)</div><div class="line"></div><div class="line"><span class="keyword">return</span>  urllist</div><div class="line"></div><div class="line"><span class="comment"># 文件名</span></div><div class="line">filename = <span class="string">'豆瓣图书Top250.txt'</span></div><div class="line"><span class="comment"># 保存文件操作</span></div><div class="line">f = open(filename, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</div><div class="line"><span class="comment"># 调用函数</span></div><div class="line">html_parse()</div><div class="line">f.close()</div><div class="line">print(<span class="string">'保存成功。'</span>)</div></pre></td></tr></table></figure><p>我们只用了36行的代码（去掉空行和注释）就实现了抓取豆瓣图书的数据了。大家是不是觉得很简单了，不要兴奋，这只是一个小白最基础的练手项目，大家快去找更有挑战性的项目实现吧。大家加油。</p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们在之前的文章中基本上掌握了&lt;code&gt;Python&lt;/code&gt;爬虫的原理和方法，不知道大家有没有练习呢。今天我就来找一个简单的网页进行爬取，就当是给之前的兵书做一个实践。不然不就是纸上谈兵的赵括了吗。&lt;/p&gt;
&lt;p&gt;好了，我们这次的目标是&lt;code&gt;豆瓣图书Top250&lt;/code&gt;，地址是：&lt;code&gt;https://book.douban.com/top250?start=0&lt;/code&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>行走世间都是妖怪</title>
    <link href="https://wyukun.coding.me/2017/06/02/%E8%A1%8C%E8%B5%B0%E4%B8%96%E9%97%B4%E9%83%BD%E6%98%AF%E5%A6%96%E6%80%AA/"/>
    <id>https://wyukun.coding.me/2017/06/02/行走世间都是妖怪/</id>
    <published>2017-06-02T07:05:48.000Z</published>
    <updated>2017-09-16T07:10:08.724Z</updated>
    
    <content type="html"><![CDATA[<p>热的时候喊热是无济于事的。顶着火红的太阳，看着姗姗来迟的 138 路公交，无奈的翻翻白眼。</p><p>南京的天气一如既往的热，由于科目二考试将至，连安静呆在宿舍的想法都无法满足。不过年轻人就是应该多出来走走，不然就无法看到外面美丽的姑娘们了。钱钟书说暴露的姑娘是局部的真理，我觉得这再合适不过了，真理总是赤裸的。正值躁动青春期的室友，掩饰不住自己对真理渴望的心，时不时露出莫名其妙的笑。<br><a id="more"></a></p><iframe src="//music.163.com/outchain/player?type=2&amp;id=28953457&amp;auto=0&amp;height=66" width="330" height="86" frameborder="no" marginwidth="0" marginheight="0"></iframe><p>倒车，转弯，刹车，踩离合。时间就在这些机械的步骤中流过，真让人兴奋，终于可以回到凉爽的宿舍了。室友说他要在中途有个事，说不坐公交回去了。我说开什么玩笑，不坐公交怎么回去，难道请我打车。然后在我惊讶的目光下，他推来辆自行车。</p><p>其实我一直搞不清楚共享单车的盈利方式，扫个码就可以免费骑，而且还可以扫两辆。于是我们骑着自行车，行驶在烈日之下，行驶在公交车行驶30分钟的路程上，挥汗如雨。路边不断的有单身的或者不是单身的姑娘走过，为了自己的形象，即使在疲惫也要摆出轻松的姿势，面露沉稳的微笑，匆匆驶过，流下一阵燥热的风。</p><p>我今年二十多岁了，一直单身。我仍然不明白有些人究竟是有多么的空虚，多么的寂寞。分个手吵个架，遇到了不舒服的事就发个朋友圈，发个微博，感时运之不济，叹人性之不真。然而我在这条通往目的地的道路上，面对着周围美丽的真理，我竟没有一点想法。</p><p>我想那些天天矫情的分享人生感悟的人，并不是他们在人生的道路上遇到了磨难，而是他们都太闲了。走出房门，扫个码骑个自行车，行驶个40分钟，自然会明白还是屋里舒服。</p><p><img src="http://imgout.ph.126.net/55964042/timg.jpg" alt="image"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;热的时候喊热是无济于事的。顶着火红的太阳，看着姗姗来迟的 138 路公交，无奈的翻翻白眼。&lt;/p&gt;
&lt;p&gt;南京的天气一如既往的热，由于科目二考试将至，连安静呆在宿舍的想法都无法满足。不过年轻人就是应该多出来走走，不然就无法看到外面美丽的姑娘们了。钱钟书说暴露的姑娘是局部的真理，我觉得这再合适不过了，真理总是赤裸的。正值躁动青春期的室友，掩饰不住自己对真理渴望的心，时不时露出莫名其妙的笑。&lt;br&gt;
    
    </summary>
    
      <category term="生活" scheme="https://wyukun.coding.me/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="随笔" scheme="https://wyukun.coding.me/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(4):Beautiful Soup的常用方法</title>
    <link href="https://wyukun.coding.me/2017/06/01/Python%E7%88%AC%E8%99%AB%E5%9B%9B%E4%B9%8BBeautifulSoup%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://wyukun.coding.me/2017/06/01/Python爬虫四之BeautifulSoup的常用方法/</id>
    <published>2017-06-01T06:25:07.000Z</published>
    <updated>2017-09-16T07:08:51.236Z</updated>
    
    <content type="html"><![CDATA[<p><code>Requests</code>库的用法大家肯定已经熟练掌握了，但是当我们使用<code>Requests</code>获取到网页的 HTML 代码信息后，我们要怎样才能抓取到我们想要的信息呢？我相信大家肯定尝试过很多办法，比如字符串的 find 方法，还有高级点的正则表达式。虽然正则可以匹配到我们需要的信息，但是我相信大家在匹配某个字符串一次一次尝试着正则匹配的规则时，一定很郁闷。</p><p>那么，我们就会想有没有方便点的工具呢。答案是肯定的，我们还有一个强大的工具，叫<code>BeautifulSoup</code>。有了它我们可以很方便地提取出<code>HTML</code>或<code>XML</code>标签中的内容，这篇文章就让我们了解下<code>BeautifulSoup</code>的常用方法吧。<br><a id="more"></a></p><h2 id="什么是BeautifulSoup？"><a href="#什么是BeautifulSoup？" class="headerlink" title="什么是BeautifulSoup？"></a>什么是BeautifulSoup？</h2><p><code>Python</code>的网页解析可以用正则表达式去完成，那么我们在写的时候，要挨个的去把代码拿出来匹配，而且还要写匹配的规则，整体实现起来就很复杂。<code>BeautifulSoup</code>呢，它是一个方便的网页解析库，处理高效，支持多种解析器。大部分情况下，利用它我们不在需要编写正则表达式就可以方便的实现网页信息的提取。</p><p><a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="external">官方文档</a></p><p>安装：<code>$ pip install beautifulsoup4</code></p><p><code>BeautifulSoup</code>是一个网页解析库，它支持很多解析器，不过最主流的有两个。一个是<code>Python</code>标准库，一个是<code>lxml</code> HTML 解析器。两者的使用方法相似：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line"><span class="comment"># Python的标准库</span></div><div class="line">BeautifulSoup(html, <span class="string">'html.parser'</span>)</div><div class="line"></div><div class="line"><span class="comment"># lxml</span></div><div class="line">BeautifulSoup(html, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure><p><code>Python</code>内置标准库的执行速度一般，但是低版本的<code>Python</code>中，中文的容错能力比较差。<code>lxml</code>HTML 解析器的执行速度快，但是需要安装 C语言的依赖库。</p><h2 id="lxml的安装"><a href="#lxml的安装" class="headerlink" title="lxml的安装"></a>lxml的安装</h2><p>由于<code>lxml</code>安装需要依赖C语言库，所以当<code>lxml</code>在<code>Windows</code>上安装时，我们会发现各种奇怪的报错，当然脸好的使用<code>pip install lxml</code></p><p>安装也是可以成功的。不过大部分人都是会倒在这里。</p><p>这里推荐大家使用<code>lxml</code>的<code>.whl</code>文件来安装。首先我们需要安装一下<code>wheel</code>库，有了这个库我们才可以正常安装<code>.whl</code>文件。<code>pip install wheel</code></p><p>从官方网站下载与系统，<code>Python</code>版本匹配的lxml文件：<a href="https://pypi.python.org/pypi/lxml/3.6.0" target="_blank" rel="external">地址</a>。</p><p>另外，不知道自己系统和<code>python</code>版本信息的伙伴。需要进入系统管理员工具（CMD）或者python的 IDLE，输入以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pip</div><div class="line"></div><div class="line">print(pip.pep425tags.get_supported())</div></pre></td></tr></table></figure><p>这时我们就可以看到打印出来的<code>Python</code>版本信息了。<br>下载好<code>lxml</code>的文件后，我们需要找到文件的位置，然后进入管理员工具，使用<code>pip</code>安装：<code>pip install whl文件的全名</code></p><p>安装完成后，可以进入<code>Python</code>，<code>import</code>一下，如果没有报错，那么恭喜你安装成功。<br>如果有的伙伴觉得麻烦，那我推荐大家安装<code>anaconda</code> <a href="https://www.continuum.io/downloads" target="_blank" rel="external">下载地址</a>（如果安装速度慢，可以找国内镜像），不知道是什么的小伙伴可以谷歌一下，有了他，那些在<code>windows</code>上<code>pip</code>安装出错的问题将不再存在。</p><h2 id="BeautifulSoup的基本标签选择方法"><a href="#BeautifulSoup的基本标签选择方法" class="headerlink" title="BeautifulSoup的基本标签选择方法"></a>BeautifulSoup的基本标签选择方法</h2><p>虽然<code>Python</code>内置的标准库解析器还不错，但是我还是推荐大家使用<code>lxml</code>，因为它够快。那么后面的代码我们都是用<code>lxml</code>解析器来进行演示。<br>我们先导入官方文档的例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">html_doc = <span class="string">"""</span></div><div class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</span></div><div class="line"><span class="string">&lt;body&gt;</span></div><div class="line"><span class="string">&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were</span></div><div class="line"><span class="string">&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="string">&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and</span></div><div class="line"><span class="string">&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;</span></div><div class="line"><span class="string">and they lived at the bottom of a well.&lt;/p&gt;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;</span></div><div class="line"><span class="string">"""</span></div></pre></td></tr></table></figure><p>HTML 代码,我们能够得到一个<code>BeautifulSoup</code>的对象,并能按照标准的缩进格式的结构输出:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">soup = BeautifulSoup(html_doc, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure><p>我们可以看到上面的 HTML 代码并不完整，接下来我们使用<code>prettify()</code>方法来进行自动补全，注释部分就是运行的输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">print(soup.prettify())</div><div class="line"><span class="comment"># &lt;html&gt;</span></div><div class="line"><span class="comment">#  &lt;head&gt;</span></div><div class="line"><span class="comment">#   &lt;title&gt;</span></div><div class="line"><span class="comment">#    The Dormouse's story</span></div><div class="line"><span class="comment">#   &lt;/title&gt;</span></div><div class="line"><span class="comment">#  &lt;/head&gt;</span></div><div class="line"><span class="comment">#  &lt;body&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="title"&gt;</span></div><div class="line"><span class="comment">#    &lt;b&gt;</span></div><div class="line"><span class="comment">#     The Dormouse's story</span></div><div class="line"><span class="comment">#    &lt;/b&gt;</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="story"&gt;</span></div><div class="line"><span class="comment">#    Once upon a time there were three little sisters; and their names were</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;</span></div><div class="line"><span class="comment">#     Elsie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    ,</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;</span></div><div class="line"><span class="comment">#     Lacie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    and</span></div><div class="line"><span class="comment">#    &lt;a class="sister" href="http://example.com/tillie" id="link2"&gt;</span></div><div class="line"><span class="comment">#     Tillie</span></div><div class="line"><span class="comment">#    &lt;/a&gt;</span></div><div class="line"><span class="comment">#    ; and they lived at the bottom of a well.</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#   &lt;p class="story"&gt;</span></div><div class="line"><span class="comment">#    ...</span></div><div class="line"><span class="comment">#   &lt;/p&gt;</span></div><div class="line"><span class="comment">#  &lt;/body&gt;</span></div><div class="line"><span class="comment"># &lt;/html&gt;</span></div></pre></td></tr></table></figure><h3 id="获取标签"><a href="#获取标签" class="headerlink" title="获取标签"></a>获取标签</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(soup.title)</div><div class="line"><span class="comment"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></div></pre></td></tr></table></figure><p>通过输出结果，我们可以看到获取内容的属性，实际上就是 HTML 代码里的一个<code>title</code>标签。</p><h3 id="获取名称"><a href="#获取名称" class="headerlink" title="获取名称"></a>获取名称</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(soup.title.name)</div><div class="line"><span class="comment"># 'title'</span></div></pre></td></tr></table></figure><p>实际上就是标签的名称。</p><h3 id="获取属性"><a href="#获取属性" class="headerlink" title="获取属性"></a>获取属性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">print(soup.p.attrs[<span class="string">'class'</span>])</div><div class="line"><span class="comment"># 'title'</span></div><div class="line"></div><div class="line">print(soup.p[<span class="string">'class'</span>])</div><div class="line"><span class="comment"># 'title'</span></div></pre></td></tr></table></figure><p>获取标签的属性我们可以使用<code>attrs</code>方法，传给他属性名，就可以得到标签的属性。通过结果我们可以看到，直接传给p标签属性名，一样可以获取到标签属性。</p><h3 id="获取内容"><a href="#获取内容" class="headerlink" title="获取内容"></a>获取内容</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(soup.title.string)</div><div class="line"><span class="comment"># 'The Dormouse's story'</span></div></pre></td></tr></table></figure><p>我们还可以使用嵌套的选择，比如我们获得body标签里面p标签的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(soup.body.p.string)</div><div class="line"><span class="comment"># 'The Dormouse's story'</span></div></pre></td></tr></table></figure><h2 id="常见用法"><a href="#常见用法" class="headerlink" title="常见用法"></a>常见用法</h2><h3 id="标准选择器"><a href="#标准选择器" class="headerlink" title="标准选择器"></a>标准选择器</h3><p>虽然<code>BeautifulSoup</code>的基本用法，标签获取，内容获取，可以解析一些 html代码。但是在遇到很多复杂的页面时，上面的方法是完全不足的，或者是很繁琐的，因为有时候有的标签会有几个属性（class、id等）。</p><p>索性<code>BeautifulSoup</code>给我们提供了很方便的标准选择器，也就是 API 方法，这里着重介绍2个: <code>find()</code> 和 <code>find_all()</code> 。其它方法的参数和用法类似,大家举一反三吧。</p><h3 id="find-all"><a href="#find-all" class="headerlink" title="find_all()"></a>find_all()</h3><p><code>find_all(name, attrs, recursive, text, **kwargs)</code>可以根据标签，属性，内容查找文档。<br><code>find_all()</code>其实和正则表达式的原理很相似，他能找出所有能满足匹配模式的结果，在把结果以列表的形式返回。<br>仍然是文档的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">html_doc = <span class="string">"""</span></div><div class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</span></div><div class="line"><span class="string">&lt;body&gt;</span></div><div class="line"><span class="string">&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were</span></div><div class="line"><span class="string">&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="string">&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and</span></div><div class="line"><span class="string">&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;</span></div><div class="line"><span class="string">and they lived at the bottom of a well.&lt;/p&gt;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;</span></div><div class="line"><span class="string">"""</span></div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line">soup = BeautifulSoup(html_doc, <span class="string">'lxml'</span>)</div></pre></td></tr></table></figure><h3 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h3><p><a href="https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/#id28" target="_blank" rel="external">文档参考</a><br>介绍 <code>find_all()</code> 方法前,大家可以参考一下过滤器的类型。过滤器只能作为搜索文档的参数,或者说应该叫参数类型更为贴切。这些过滤器贯穿整个搜索的API。过滤器可以被用在 tag 的<code>name</code>中,节点的属性中,字符串中或他们的混合中。</p><p><code>find_all()</code> 方法搜索当前 tag 的所有 tag 子节点,并判断是否符合过滤器的条件。这里有几个例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div><div class="line"></div><div class="line">soup.find_all(<span class="string">"p"</span>, <span class="string">"title"</span>)</div><div class="line"><span class="comment"># [&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;]</span></div><div class="line"></div><div class="line">soup.find_all(<span class="string">"a"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div><div class="line"></div><div class="line">soup.find_all(id=<span class="string">"link2"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><p>有几个方法很相似,还有几个方法是新的,参数中的 <code>string</code> 和<code>id</code>是什么含义? 为什么 <code>find_all(&quot;p&quot;, &quot;title&quot;)</code> 返回的是CSS Class为”title”的标签? 我们来仔细看一下<code>find_all()</code>的参数:</p><h4 id="name参数"><a href="#name参数" class="headerlink" title="name参数"></a>name参数</h4><p>name 参数可以查找所有名字为 name 的 tag,字符串对象会被自动忽略掉。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># [The Dormouse's story]</span></div></pre></td></tr></table></figure><p>搜索 name 参数的值可以使任一类型的过滤器,字符窜,正则表达式,列表,方法或是<code>True</code> 。<br>我们常用的 name 参数是搜索文档的标签名。</p><h4 id="keyword参数"><a href="#keyword参数" class="headerlink" title="keyword参数"></a>keyword参数</h4><p>如果我们的 HTML代码中有几个<code>div</code>标签，但是我们只想获取到<code>class</code>属性为<code>top</code>的<code>div</code>标签，我们怎么出来呢。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'top'</span>)</div><div class="line"><span class="comment"># 这里注意下，class是Python的内部关键词，我们需要在css属性class后面加一个下划线'_'，不然会报错。</span></div></pre></td></tr></table></figure><p>仍然以上面的代码实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'a'</span>, id=<span class="string">'link2'</span>)</div><div class="line"><span class="comment"># [&lt;a id="link2" href="http://example.com/lacie"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><p>这样我们就只获取到<code>id</code>为<code>link2</code>的<code>a</code>标签。</p><h4 id="limit参数"><a href="#limit参数" class="headerlink" title="limit参数"></a>limit参数</h4><p><code>find_all()</code> 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢。如果我们不需要全部结果,可以使用 <code>limit</code> 参数限制返回结果的数量。效果与 SQL 中的<code>limit</code>关键字类似,当搜索到的结果数量达到<code>limit</code>的限制时,就停止搜索返回结果。</p><p>比如我们要搜索出<code>a</code>标签，但是满足的有3个，我们只想要得到2个：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">"a"</span>, limit=<span class="number">2</span>)</div><div class="line"><span class="comment"># [&lt;a id="link1" class="sister" href="http://example.com/elsie"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment"># &lt;a id="link2" class="sister" href="http://example.com/lacie"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><p>其他的参数，不是经常用到，大家如需了解可以参考官方文档。</p><h3 id="find"><a href="#find" class="headerlink" title="find()"></a>find()</h3><p><code>find_all()</code>返回的是所有元素列表，<code>find()</code>返回单个元素。</p><p><code>find( name , attrs , recursive , string , **kwargs )</code></p><p><code>find_all()</code>方法将返回文档中符合条件的所有 tag,尽管有时候我们只想得到一个结果。比如文档中只有一个标签,那么使用<code>find_all()</code> 方法来查找标签就不太合适, 使用<code>find_all</code>方法并设置<code>limit=1</code>参数不如直接使用<code>find()</code>方法。下面两行代码是等价的:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.find_all(<span class="string">'title'</span>, limit=<span class="number">1</span>)</div><div class="line"><span class="comment"># [The Dormouse's story]</span></div><div class="line"></div><div class="line">soup.find(<span class="string">'title'</span>)</div><div class="line"><span class="comment">#The Dormouse's story</span></div></pre></td></tr></table></figure><p>唯一的区别是<code>find_all()</code>方法的返回结果是值包含一个元素的列表,而<code>find()</code>方法直接返回结果。<code>find_all()</code>方法没有找到目标是返回空列表, <code>find()</code>方法找不到目标时,返回<code>None</code>。</p><h3 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h3><p><code>Beautiful Soup</code>支持大部分的 CSS选择器。在<code>Tag</code>或<code>BeautifulSoup</code>对象的<code>.select()</code>方法中传入字符串参数, 即可使用 CSS选择器的语法找到 tag。我们在写 css 时，标签 class类名加”<code>.</code>“，id属性加”<code>#</code>“。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">"title"</span>)</div><div class="line"><span class="comment"># [The Dormouse's story]</span></div></pre></td></tr></table></figure><p>通过 tag标签逐层查找:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">"body a"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/lacie"  id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div><div class="line"></div><div class="line">soup.select(<span class="string">"html head title"</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div></pre></td></tr></table></figure><p>找到某个 tag标签下的直接子标签:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">"head &gt; title"</span>)</div><div class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></div><div class="line"></div><div class="line">soup.select(<span class="string">"p &gt; a"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/lacie"  id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div><div class="line"></div><div class="line">soup.select(<span class="string">"p &gt; #link1"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]</span></div><div class="line"></div><div class="line">soup.select(<span class="string">"body &gt; a"</span>)</div><div class="line"><span class="comment"># []</span></div></pre></td></tr></table></figure><p>通过 CSS 的 class类名查找:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">".sister"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><p>通过 tag 的 id 查找:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">"#link1"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]</span></div><div class="line"></div><div class="line">soup.select(<span class="string">"a#link2"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><p>同时用多种 CSS选择器查询元素，使用逗号隔开:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">soup.select(<span class="string">"#link1,#link2"</span>)</div><div class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></div><div class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]</span></div></pre></td></tr></table></figure><h2 id="提取标签内容"><a href="#提取标签内容" class="headerlink" title="提取标签内容"></a>提取标签内容</h2><p>如果我们得到了几个标签：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">list = [&lt;a href="http://www.baidu.com/"&gt;百度&lt;/a&gt;,</div><div class="line"></div><div class="line">&lt;a href="http://www.163.com/"&gt;网易&lt;/a&gt;,</div><div class="line"></div><div class="line">&lt;a href="http://www.sina.com/"新浪&lt;/a&gt;]</div></pre></td></tr></table></figure><p>我们要怎样提取他里面的内容呢。我们开始的时候有提及。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> list:</div><div class="line">    print(i.get_text()) <span class="comment"># 我们使用get_text()方法获得标签内容</span></div><div class="line">    print(i.get[<span class="string">'href'</span>] <span class="comment"># get['attrs']方法获得标签属性</span></div><div class="line">    print(i[<span class="string">'href'</span>]) <span class="comment"># 简写结果一样</span></div></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">百度</div><div class="line">网易</div><div class="line">新浪</div><div class="line">http://www.baidu.com/</div><div class="line">http://www<span class="number">.163</span>.com/</div><div class="line">http://www.sina.com/</div><div class="line">http://www.baidu.com/</div><div class="line">http://www<span class="number">.163</span>.com/</div><div class="line">http://www.sina.com/</div></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><code>BeautifulSoup</code>的解析库，推荐使用<code>lxml</code>，如果出现乱码的情况下，可以使用<code>html.parser</code>；</li><li><code>BeautifulSoup</code>的标签选择筛选方法，虽然弱但是速度快；</li><li>推荐使用<code>find_all()</code>,<code>find()</code>方法搜索标签，当然如果对css选择器熟悉，推荐使用<code>.select()</code>方法；</li><li><code>get_text()</code>方法获取标签文本内容，<code>get[attrs]</code>方法获取标签属性值。</li></ul><p>本篇我们就基本上整理了<code>BeautifulSoup</code>的常用方法。如果大家希望了解更高级的用法，可以查看<code>BeautifulSoup</code>的官方文档。</p><p>最后，大家可以结合<code>Requests</code>库写出自己的爬虫吧。</p><p>谢谢阅读</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Requests&lt;/code&gt;库的用法大家肯定已经熟练掌握了，但是当我们使用&lt;code&gt;Requests&lt;/code&gt;获取到网页的 HTML 代码信息后，我们要怎样才能抓取到我们想要的信息呢？我相信大家肯定尝试过很多办法，比如字符串的 find 方法，还有高级点的正则表达式。虽然正则可以匹配到我们需要的信息，但是我相信大家在匹配某个字符串一次一次尝试着正则匹配的规则时，一定很郁闷。&lt;/p&gt;
&lt;p&gt;那么，我们就会想有没有方便点的工具呢。答案是肯定的，我们还有一个强大的工具，叫&lt;code&gt;BeautifulSoup&lt;/code&gt;。有了它我们可以很方便地提取出&lt;code&gt;HTML&lt;/code&gt;或&lt;code&gt;XML&lt;/code&gt;标签中的内容，这篇文章就让我们了解下&lt;code&gt;BeautifulSoup&lt;/code&gt;的常用方法吧。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(3):Requests的高级用法</title>
    <link href="https://wyukun.coding.me/2017/05/29/Python%E7%88%AC%E8%99%AB%E4%B8%89%E4%B9%8BRequests%E7%9A%84%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/"/>
    <id>https://wyukun.coding.me/2017/05/29/Python爬虫三之Requests的高级用法/</id>
    <published>2017-05-29T15:28:36.000Z</published>
    <updated>2017-09-16T04:56:30.899Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇文章我们整理了<code>Requests</code>库的基本用法，相信大家已经经过爬取一些简单网页的练习，已经很熟练了。</p><p>这一篇文章我们来 看一下<code>Requests</code>库的高级操作。<br><a id="more"></a></p><h2 id="高级操作"><a href="#高级操作" class="headerlink" title="高级操作"></a>高级操作</h2><h3 id="1-文件上传"><a href="#1-文件上传" class="headerlink" title="1.文件上传"></a>1.文件上传</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">files = &#123;<span class="string">'file'</span> : open(<span class="string">'logo.gif'</span>,<span class="string">'rb'</span>)&#125;</div><div class="line">resp = requests.post(<span class="string">'http://httpbin.org/post'</span>, files=files)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>文件上传的操作只要我们从文件夹中把文件读取出来，并且赋值给 files 参数，就可以了，打印出源代码我们就可以看待上传文件的字节流了。</p><h3 id="2-获取Cookie"><a href="#2-获取Cookie" class="headerlink" title="2.获取Cookie"></a>2.获取Cookie</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">import</span> requests</div><div class="line"></div><div class="line">&gt;&gt;&gt;resp = requests.get(<span class="string">'http://www.baidu.com'</span>)</div><div class="line">&gt;&gt;&gt;print(resp.cookies)</div><div class="line">&lt;RequestsCookieJar[]&gt;</div><div class="line">&gt;&gt;&gt;<span class="keyword">for</span> key, value <span class="keyword">in</span> resp.cookies.items():</div><div class="line"><span class="meta">... </span>   print(key + <span class="string">'='</span> + value)</div><div class="line">BDORZ=<span class="number">27315</span></div></pre></td></tr></table></figure><p>我们可以通过获取字典的键值对来查看<code>cookie</code>.</p><h3 id="3-会话维持"><a href="#3-会话维持" class="headerlink" title="3.会话维持"></a>3.会话维持</h3><p>我们获得到了<code>cookie</code>就可以做一个会话维持，可以维持一个登录的状态，也就是做模拟登录。我们来看实现方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">s = requests.Session()</div><div class="line">s.get(<span class="string">'http://httpbin.org/cookies/set/number/123456789'</span>) <span class="comment"># 设置了一个cookie</span></div><div class="line">resp = s.get(<span class="string">'http://httpbin.org/cookies'</span>)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>这就相当于模拟了一个会话，比如做登陆验证，可以用<code>session</code>，POST 一下，登陆一下，然后保持会话信息，在访问登录过页面的话，就可以正常获取登录后的页面了。如果你要模拟登录，可以通过申明<code>Session</code>对象，再用<code>Session</code>对象发起两次get请求，那么这两次请求相当于在一个浏览器里面，先访问<code>set cookie</code>页面，在访问<code>get cookie</code>页面。当然，<code>cookie</code>是自动处理的，不需要担心写一些处理<code>cookies</code>的方法。</p><p>建议模拟登录用<code>requests</code>的<code>Session</code>对象。</p><h3 id="4-SSL证书验证"><a href="#4-SSL证书验证" class="headerlink" title="4.SSL证书验证"></a>4.SSL证书验证</h3><p><code>Requests</code>可以为 HTTPS 请求验证 SSL 证书，就像 web浏览器一样。要想检查某个主机的 SSL证书，你可以使用 verify参数:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">import</span> requests</div><div class="line"></div><div class="line">&gt;&gt;&gt;requests.get(<span class="string">'https://kennethreitz.com'</span>, verify=<span class="keyword">True</span>) <span class="comment"># verify参数默认值为True</span></div><div class="line">requests.exceptions.SSLError: hostname <span class="string">'kennethreitz.com'</span> doesn<span class="string">'t match either of '</span>*.herokuapp.com<span class="string">', '</span>herokuapp.com<span class="string">'</span></div></pre></td></tr></table></figure><p>如果不想他报这个错误，我们可以把参数<code>verify</code>的值设为<code>False</code>.运行后发现程序没有报错，但是会出现警告信息，警告我们要验证 SSL证书。如果要消除这个警告，我们需要调用原生包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">import</span> requests</div><div class="line">&gt;&gt;&gt;<span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</div><div class="line"></div><div class="line">urllib3.disable_warnings()</div><div class="line">&gt;&gt;&gt;requests.get(<span class="string">'https://kennethreitz.com'</span>, verify=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><p>我们还可以自己指定一个证书：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">import</span> requests</div><div class="line"></div><div class="line">&gt;&gt;&gt;resp = requests.get(<span class="string">'https://kennethreitz.com'</span>, cert=(<span class="string">'/path/server.crt'</span>, <span class="string">'/path/key'</span>))</div><div class="line">&gt;&gt;&gt;print(resp.status_code)</div><div class="line"><span class="number">200</span></div></pre></td></tr></table></figure><h3 id="5-代理设置"><a href="#5-代理设置" class="headerlink" title="5.代理设置"></a>5.代理设置</h3><p>有些网站会限制 IP 访问频率，超过频率就断开连接。这个时候我们就需要使用到代理，我们可以通过为任意请求方式提供<code>proxies</code>参数来配置单个请求。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">proxies = &#123;</div><div class="line">    <span class="string">"http"</span>: <span class="string">"http://10.10.1.10:3128"</span>,</div><div class="line">    <span class="string">"https"</span>: <span class="string">"http://10.10.1.10:1080"</span>,</div><div class="line">&#125;</div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com'</span>, proxies=proxies)</div><div class="line">print(resp.status_code)</div></pre></td></tr></table></figure><p>也可以通过环境变量 <code>HTTP_PROXY</code> 和 <code>HTTPS_PROXY</code> 来配置代理。<br>有些代理需要加上用户名和密码的，代理可以使用<code>http://user:password@host/</code>语法，比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">proxies = &#123;</div><div class="line">    <span class="string">"http"</span>: <span class="string">"http://user:pass@10.10.1.10:3128/"</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>除了基本的 HTTP代理，<code>Requests</code>还支持<code>SOCKS</code>协议的代理，如果需要用的，可以安装带三方库：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ pip install requests[socks]</div></pre></td></tr></table></figure><p>安装好依赖以后，使用 SOCKS 代理和使用 HTTP 代理一样简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">proxies = &#123;</div><div class="line">    <span class="string">"http"</span>: <span class="string">'socks5://user:pass@host:port'</span>,</div><div class="line">    <span class="string">"https"</span>: <span class="string">'socks5://user:pass@host:port'</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="6-超时设置"><a href="#6-超时设置" class="headerlink" title="6.超时设置"></a>6.超时设置</h3><p>超时设置就是设置请求的时间，如果在规定的时间内没有返回应答，就抛出异常.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com'</span>, timeout=<span class="number">0.5</span>)</div><div class="line">print(resp.status_code)</div></pre></td></tr></table></figure><p>如果在0.5秒内没有返回，就会报出<code>ReadTimeout</code>的异常。<br>如果远端服务器很慢，你可以让<code>Request</code>永远等待，传入一个<code>None</code>作为<code>timeout</code>值，然后就冲咖啡去吧。</p><h3 id="7-认证设置"><a href="#7-认证设置" class="headerlink" title="7.认证设置"></a>7.认证设置</h3><p>有一些网站在访问的时候需要我们输入用户名和密码，那么这种网站我们要怎样处理呢。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> requests.auth <span class="keyword">import</span> HTTPBasicAuth</div><div class="line"></div><div class="line">resp = requests.get(url, auth=HTTPBasicAuth(<span class="string">'username'</span>,<span class="string">'password'</span>))</div><div class="line">print(resp.status_code)</div></pre></td></tr></table></figure><p>调用<code>HTTPBasicAuth</code>类，直接传入用户名和密码就可以了。</p><h3 id="8-异常处理"><a href="#8-异常处理" class="headerlink" title="8.异常处理"></a>8.异常处理</h3><p>如果你遇到无法访问的网站，或者是你的网速不够快，你的访问超时，就会导致程序的中断。显然我们在实际的抓取中不愿意看到爬取到一半的程序突然中断的情况，那么我们能够避免这种程序中断的情况吗，答案是肯定的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ReadTimeout, ConnectionError, RequestException</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    resp = requests.get(<span class="string">'http://httpbin.org/get'</span>, timeout=<span class="number">0.5</span>)</div><div class="line">    print(resp.status_code)</div><div class="line"><span class="keyword">except</span> ReadTimeout： <span class="comment"># 访问超时的错误</span></div><div class="line">    print(<span class="string">'Timeout'</span>)</div><div class="line"><span class="keyword">except</span> ConnectionError: <span class="comment"># 网络中断连接错误</span></div><div class="line">    print(<span class="string">'Connect error'</span>)</div><div class="line"><span class="keyword">except</span> RequestException: <span class="comment"># 父类错误</span></div><div class="line">    print(<span class="string">'Error'</span>)</div></pre></td></tr></table></figure><p>这样我们就可以把<code>requests</code>抓取过程中常见的异常都处理捕获了，捕获错误应该先捕获子类异常在捕获父类异常，这样做能够更加直观清楚的应对程序中出现的错误了。</p><p>如果我们能够自己捕获了这些异常，就可以保证我们的爬虫一直运行了。</p><p>好了，<code>Requests</code>的大部分用法已经全部说完了，大家是否已经学会了这门屠龙之术了呢。快找个网页练练手吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇文章我们整理了&lt;code&gt;Requests&lt;/code&gt;库的基本用法，相信大家已经经过爬取一些简单网页的练习，已经很熟练了。&lt;/p&gt;
&lt;p&gt;这一篇文章我们来 看一下&lt;code&gt;Requests&lt;/code&gt;库的高级操作。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(2):Requests的基本用法</title>
    <link href="https://wyukun.coding.me/2017/05/29/Python%E7%88%AC%E8%99%AB%E4%BA%8C%E4%B9%8BRequests%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/"/>
    <id>https://wyukun.coding.me/2017/05/29/Python爬虫二之Requests的基本用法/</id>
    <published>2017-05-29T15:08:56.000Z</published>
    <updated>2017-09-16T04:33:48.308Z</updated>
    
    <content type="html"><![CDATA[<p>虽然Python有内置的<code>urllib</code>库，可以实现网络的请求，但是我并不推荐。因为<code>urllib</code>在很多时候使用起来不方便，比如加一个代理，处理<code>Cookie</code>时API都很繁琐，再比如发送一个<code>POST</code>请求也很麻烦。</p><p>而<code>Requests</code>就相当于<code>urllib</code>的升级版本，简化了<code>urllib</code>的使用方法。有了<code>Requests</code>，我们可以用几句代码实现代理的设置，<code>Cookie</code>的设置，非常方便。下面我就给大家整理了<code>Requests</code>库的使用方法和细节。详细可以参考<code>Requests</code><a href="http://docs.python-requests.org/zh_CN/latest/" target="_blank" rel="external">官方文档</a>。<br><a id="more"></a></p><h2 id="什么是Requests？"><a href="#什么是Requests？" class="headerlink" title="什么是Requests？"></a>什么是Requests？</h2><p><code>Requests</code>是<code>Python</code>语言编写，基于<code>urllib3</code>，采用<code>Apache2 Licensed</code>开源协议的HTTP库。</p><p>它比<code>urllib</code>更加方便，可以节约我们大量的工作，完全满足<code>HTTP</code>测试需求。是<code>Python</code>实现的简单易用的<code>HTTP</code>库。</p><p>安装也很简单：<code>pip install requests</code></p><h2 id="Requests的语法操作"><a href="#Requests的语法操作" class="headerlink" title="Requests的语法操作"></a>Requests的语法操作</h2><h3 id="1-实例引入"><a href="#1-实例引入" class="headerlink" title="1.实例引入"></a>1.实例引入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">response = requests.get(<span class="string">'http://www.baidu.com/'</span>)</div><div class="line">print(response.status_code)</div><div class="line">print(type(response.text))</div><div class="line">print(response.text)</div><div class="line">print(response.cookies)</div></pre></td></tr></table></figure><p>运行结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">200</div><div class="line">&lt;class &apos;str&apos;&gt;</div><div class="line"> </div><div class="line"># ...HTML网页源码..</div><div class="line">&lt;RequestsCookieJar[]&gt;</div></pre></td></tr></table></figure></p><p>可以看到，我们非常方便的就获取到了<code>Cookies</code>.</p><h3 id="2-各种请求方式"><a href="#2-各种请求方式" class="headerlink" title="2.各种请求方式"></a>2.各种请求方式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">requests.get(<span class="string">'http://httpbin.org/get'</span>) <span class="comment"># 发送get请求</span></div><div class="line">requests.post(<span class="string">'http://httpbin.org/post'</span>) <span class="comment"># 发送post请求，只要调用post方法，传入一个url参数</span></div><div class="line">requests.put(<span class="string">'http://httpbin.org/put'</span>)</div><div class="line">requests.delete(<span class="string">'http://httpbin.org/delete'</span>)</div></pre></td></tr></table></figure><p>官方文档里提供的这个网址足够我们测试这些请求方式了。</p><h2 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h2><h3 id="1-基本GET请求"><a href="#1-基本GET请求" class="headerlink" title="1.基本GET请求"></a>1.基本GET请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'http://httpbin.org/get'</span>)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>这个我们前面有使用过，也是最常用的方法。运行成功就可以看到网页的源码了。</p><h3 id="2-带参数的GET请求"><a href="#2-带参数的GET请求" class="headerlink" title="2.带参数的GET请求"></a>2.带参数的GET请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">data = &#123;</div><div class="line">    <span class="string">'name'</span> : <span class="string">'jack'</span>,</div><div class="line">    <span class="string">'age'</span> : <span class="number">20</span></div><div class="line">&#125;</div><div class="line">resp = requests.get(<span class="string">'http://httpbin.org/get'</span>, params=data)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>传入参数只需要我们把数据生成一个字典，然后调用<code>params</code>参数，赋值给他就可以，是不是很方便。</p><h3 id="3-解析json"><a href="#3-解析json" class="headerlink" title="3.解析json"></a>3.解析json</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> json</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'http://httpbin.org/get'</span>)</div><div class="line">print(resp.text)</div><div class="line">print(resp.json())</div><div class="line">print(json.loads(resp.text))</div><div class="line">print(type(resp.json()))</div></pre></td></tr></table></figure><p>运行结果：</p><p><img src="http://imgout.ph.126.net/55901018/QQCDBCC6AC20170529120909.jpg" alt="image"></p><p>可以看出<code>Requests</code>的<code>jaon</code>解析和<code>json</code>的<code>loads</code>方法解析出来的结果是完全一样的。所以<code>Requests</code>可以很方便的解析<code>json</code>数据。</p><h3 id="4-获取二进制数据"><a href="#4-获取二进制数据" class="headerlink" title="4.获取二进制数据"></a>4.获取二进制数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com/img/baidu_jgylogo3.gif'</span>)</div><div class="line">print(resp.content)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>运行成功我们可以看到<code>content</code>方法获取的图片页面源码是二进制数据，而<code>text</code>获取的则是字符串代码。显然获取图片这种二进制数据需要使用<code>content</code>方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> open(<span class="string">'logo.gif'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(resp.content)</div></pre></td></tr></table></figure><p>这样我们就保存了图片，我们可以在文件夹下看到这张图片。</p><h3 id="5-添加headers"><a href="#5-添加headers" class="headerlink" title="5.添加headers"></a>5.添加headers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com'</span>, headers=headers)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>有些网页如果我们直接去请求的话，他会查看请求的对象是不是浏览器，如果没有浏览器信息就会禁止我们爬虫的访问，这个时候我们就要给爬虫加一个<code>headers</code>，加一个浏览器的<code>user-agent</code>信息。这样我们就可以正常访问了。如果有的伙伴不知道怎么得到<code>User-Agent</code>，可以打开浏览器的审查元素，找到<code>network</code>，随便点击一个链接就可以看到<code>User-Agent</code>的信息了。</p><p><img src="http://imgout.ph.126.net/55898004/QQCDBCC6AC20170529131930.jpg" alt="image"></p><h3 id="6-基本POST请求"><a href="#6-基本POST请求" class="headerlink" title="6.基本POST请求"></a>6.基本POST请求</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">data = &#123;</div><div class="line">    <span class="string">'name'</span> : <span class="string">'jack'</span>,</div><div class="line">    <span class="string">'age'</span> : <span class="number">20</span></div><div class="line">&#125;</div><div class="line">resp = requests.post(<span class="string">'http://httpbin.org/post'</span>, data=data)</div><div class="line">print(resp.text)</div></pre></td></tr></table></figure><p>一个<code>POST</code>必然是要有一个<code>Form Data</code>的表单提交的，我们只要把信息传给<code>data</code>参数就可以了。一个<code>POST</code>请求只需要调用<code>post</code>方法，是不是特别方便呢。如果不觉得方便的话，可以去参考<code>urllib</code>的使用方法。</p><h2 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h2><h3 id="1-response属性"><a href="#1-response属性" class="headerlink" title="1.response属性"></a>1.response属性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">response = requests.get(<span class="string">'http://www.baidu.com/'</span>)</div><div class="line">print(type(response.status_code)) <span class="comment"># 状态码</span></div><div class="line">print(type(response.text)) <span class="comment"># 网页源码</span></div><div class="line">print(type(response.headers)) <span class="comment"># 头部信息</span></div><div class="line">print(type(response.cookies)) <span class="comment"># Cookie</span></div><div class="line">print(type(response.url)) <span class="comment"># 请求的url</span></div><div class="line">print(type(response.history)) <span class="comment"># 访问的历史记录</span></div></pre></td></tr></table></figure><p>获取这些信息只需要简单的调用就可以实现了。</p><h3 id="2-状态码判断"><a href="#2-状态码判断" class="headerlink" title="2.状态码判断"></a>2.状态码判断</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">import</span> requests</div><div class="line"> </div><div class="line">&gt;&gt;&gt;response = requests.get(<span class="string">'http://www.baidu.com/'</span>)</div><div class="line">&gt;&gt;&gt;exit() <span class="keyword">if</span> <span class="keyword">not</span> resp.status_code == <span class="number">200</span> <span class="keyword">else</span> print(<span class="string">'Sucessful'</span>)</div><div class="line">Sucessful</div></pre></td></tr></table></figure><p>如果发送了一个错误请求(一个4XX客户端错误，或者5XX服务器错误响应)，我们可以通过 <code>Response.raise_for_status()</code> 来抛出异常：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;bad_r = requests.get(<span class="string">'http://httpbin.org/status/404'</span>)</div><div class="line">&gt;&gt;&gt;bad_r.status_code</div><div class="line"><span class="number">404</span></div><div class="line">&gt;&gt;&gt;bad_r.raise_for_status()</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"requests/models.py"</span>, line <span class="number">832</span>, <span class="keyword">in</span> raise_for_status</div><div class="line">    <span class="keyword">raise</span> http_error</div><div class="line">requests.exceptions.HTTPError: <span class="number">404</span> Client Error</div></pre></td></tr></table></figure><p>好了，这篇文章我们了解了<code>Requests</code>库的基本语法操作，相信大家对<code>Requests</code>库的请求和响应已经很清楚了，大家完全可以抓取一些网页了。</p><p>纸上得来终觉浅，绝知此事要躬行，大家加油！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然Python有内置的&lt;code&gt;urllib&lt;/code&gt;库，可以实现网络的请求，但是我并不推荐。因为&lt;code&gt;urllib&lt;/code&gt;在很多时候使用起来不方便，比如加一个代理，处理&lt;code&gt;Cookie&lt;/code&gt;时API都很繁琐，再比如发送一个&lt;code&gt;POST&lt;/code&gt;请求也很麻烦。&lt;/p&gt;
&lt;p&gt;而&lt;code&gt;Requests&lt;/code&gt;就相当于&lt;code&gt;urllib&lt;/code&gt;的升级版本，简化了&lt;code&gt;urllib&lt;/code&gt;的使用方法。有了&lt;code&gt;Requests&lt;/code&gt;，我们可以用几句代码实现代理的设置，&lt;code&gt;Cookie&lt;/code&gt;的设置，非常方便。下面我就给大家整理了&lt;code&gt;Requests&lt;/code&gt;库的使用方法和细节。详细可以参考&lt;code&gt;Requests&lt;/code&gt;&lt;a href=&quot;http://docs.python-requests.org/zh_CN/latest/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方文档&lt;/a&gt;。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(1):基本原理</title>
    <link href="https://wyukun.coding.me/2017/05/28/Python%E7%88%AC%E8%99%AB%E4%B8%80%E4%B9%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/"/>
    <id>https://wyukun.coding.me/2017/05/28/Python爬虫一之基本原理/</id>
    <published>2017-05-28T14:44:37.000Z</published>
    <updated>2017-09-16T06:25:37.932Z</updated>
    
    <content type="html"><![CDATA[<p>大家好，今天周末，希望大家周末愉快。</p><p>这篇文章我来梳理一下爬虫的基本原理。用过<code>Python</code>的伙伴都知道<code>Python</code>用来写爬虫是件很简单很爽的事情。但是有些伙伴不了解爬虫到底是什么，会纳闷为什么爬虫要设置什么请求头、请求体啊，到底要怎么样去发送这个请求啊。当然有的伙伴使用过很多爬虫的请求库，解析库，写过很多爬虫，但是可能他们的思路脉络也不是太清晰。那么，这篇文章就来帮大家理清爬虫原理的脉络。<br><a id="more"></a></p><h2 id="什么是爬虫？"><a href="#什么是爬虫？" class="headerlink" title="什么是爬虫？"></a>什么是爬虫？</h2><p><img src="http://imgout.ph.126.net/55876004/spider-1673428_960_720.jpg" alt="spider"></p><p>爬虫即网络爬虫，英文是<code>Web Spider</code>。翻译过来就是网络上爬行的蜘蛛，如果把互联网看作一张大网，那么爬虫就是在大网上爬来爬去的蜘蛛，碰到想要的食物，就把他抓取出来。</p><p>我们在浏览器中输入一个网址，敲击回车，看到网站的页面信息。这就是浏览器请求了网站的服务器，获取到网络资源。那么，爬虫也相当于模拟浏览器发送请求，获得到<code>HTML</code>代码。<code>HTML</code>代码里通常包含了标签和文字信息，我们就从中提取到我们想要的信息。</p><p>通常爬虫是从某个网站的某个页面开始，爬取这个页面的内容，找到网页中的其他链接地址，然后从这个地址爬到下一个页面，这样一直不停的爬下去，进去批量的抓取信息。那么，我们可以看出网络爬虫就是一个不停爬取网页抓取信息的程序。</p><h2 id="爬虫的基本流程"><a href="#爬虫的基本流程" class="headerlink" title="爬虫的基本流程"></a>爬虫的基本流程</h2><h3 id="1-发起请求："><a href="#1-发起请求：" class="headerlink" title="1.发起请求："></a>1.发起请求：</h3><p>通过HTTP库向目标站点发起请求，即发送一个<code>Request</code>，请求可以包含额外的<code>headers</code>等信息，然后等待服务器响应。这个请求的过程就像我们打开浏览器，在浏览器地址栏输入网址：<code>www.baidu.com</code>，然后点击回车。这个过程其实就相当于浏览器作为一个浏览的客户端，向服务器端发送了 一次请求。</p><h3 id="2-获取响应内容："><a href="#2-获取响应内容：" class="headerlink" title="2.获取响应内容："></a>2.获取响应内容：</h3><p>如果服务器能正常响应，我们会得到一个<code>Response</code>，<code>Response</code>的内容便是所要获取的内容，类型可能有<code>HTML</code>、<code>Json</code>字符串，二进制数据(图片，视频等）等类型。这个过程就是服务器接收客户端的请求，进过解析发送给浏览器的网页<code>HTML</code>文件。</p><h3 id="3-解析内容："><a href="#3-解析内容：" class="headerlink" title="3.解析内容："></a>3.解析内容：</h3><p>得到的内容可能是<code>HTML</code>，可以使用正则表达式，网页解析库进行解析。也可能是<code>Json</code>，可以直接转为<code>Json</code>对象解析。可能是二进制数据，可以做保存或者进一步处理。这一步相当于浏览器把服务器端的文件获取到本地，再进行解释并且展现出来。</p><h3 id="4-保存数据："><a href="#4-保存数据：" class="headerlink" title="4.保存数据："></a>4.保存数据：</h3><p>保存的方式可以是把数据存为文本，也可以把数据保存到数据库，或者保存为特定的jpg，mp4 等格式的文件。这就相当于我们在浏览网页时，下载了网页上的图片或者视频。</p><h2 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h2><h3 id="1-什么是Request？"><a href="#1-什么是Request？" class="headerlink" title="1.什么是Request？"></a>1.什么是Request？</h3><p>浏览器发送信息给该网址所在的服务器，这个过程就叫做<code>HTTP Request</code>。</p><h3 id="2-Request中包含什么？"><a href="#2-Request中包含什么？" class="headerlink" title="2.Request中包含什么？"></a>2.Request中包含什么？</h3><ul><li>请求方式：请求方式的主要类型是<code>GET</code>，<code>OST</code>两种，另外还有<code>HEAD</code>、<code>PUT</code>、<code>DELETE</code>等。GET 请求的请求参数会显示在URL链接的后面，比如我们打开百度，搜索“图片”，我们会看到请求的URL链接为<code>https://www.baidu.com/s?wd=图片</code>。而 POST 请求的请求参数会存放在<code>Request</code>内，并不会出现在 URL 链接的后面，比如我们登录知乎，输入用户名和密码，我们会看到浏览器开发者工具的<code>Network</code>页，<code>Request</code>请求有<code>Form Data</code>的键值对信息，那里就存放了我们的登录信息，有利于保护我们的账户信息安全；</li><li>请求 URL：URL 全称是统一资源定位符，也就是我们说的网址。比如一张图片，一个音乐文件，一个网页文档等都可以用唯一URL来确定，它包含的信息指出文件的位置以及浏览器应该怎么去处理它；</li><li>请求头<code>(Request Headers)</code>：请求头包含请求时的头部信息，如<code>User-Agent</code>（指定浏览器的请求头），<code>Host</code>，<code>Cookies</code>等信息；</li><li>请求体：请求体是请求是额外携带的数据，比如登录表单提交的登录信息数据。</li></ul><h2 id="Response"><a href="#Response" class="headerlink" title="Response"></a>Response</h2><h3 id="1-什么是Response？"><a href="#1-什么是Response？" class="headerlink" title="1.什么是Response？"></a>1.什么是Response？</h3><p>服务器收到浏览器发送的信息后，能够根据浏览器发送信息的内容，做出相应的处理，然后把消息回传给浏览器，这个过程就叫做<code>HTTP Response</code>。</p><h3 id="2-Response中包含什么？"><a href="#2-Response中包含什么？" class="headerlink" title="2.Response中包含什么？"></a>2.Response中包含什么？</h3><ul><li>响应状态：有多种响应状态，比如200代表成功，301 跳转页面，404 表示找不到页面，502 表示服务器错误；</li><li>响应头<code>(Response Headers)</code>：比如内容类型，内容长度，服务器信息，设置<code>Cookie</code>等；</li><li>响应体：响应体最主要的部分，包含了请求资源的内容，比如网页 HTML 代码，图片二进制数据等。</li></ul><h2 id="简单演示"><a href="#简单演示" class="headerlink" title="简单演示"></a>简单演示</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests <span class="comment"># 导入requests库，需要安装</span></div><div class="line"></div><div class="line"><span class="comment"># 模拟成浏览器访问的头</span></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(<span class="string">'https://www.baidu.com'</span>,headers=headers)</div><div class="line">print(resp.text) <span class="comment"># 打印出网页源代码</span></div><div class="line">print(resp.status_code) <span class="comment"># 打印出状态码</span></div></pre></td></tr></table></figure><p>运行成功后可以看到打印出来的 html 源代码和 200 状态码了。这就基本上实现了爬虫的<code>Request</code>和<code>Response</code>的过程。</p><h2 id="能抓到什么样的数据？"><a href="#能抓到什么样的数据？" class="headerlink" title="能抓到什么样的数据？"></a>能抓到什么样的数据？</h2><ul><li>网页文本：如 HTML 文档，<code>Ajax</code>加载的<code>Json</code>格式文本等；</li><li>图片，视频等：获取到的是二进制文件，保存为图片或视频格式；</li><li>其他只要能请求到的，都能获取。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>&#125;</div><div class="line">resp = requests.get(<span class="string">'http://www.baidu.com/img/baidu_jgylogo3.gif'</span>,headers=headers)</div><div class="line">print(resp.content) <span class="comment"># 二进制文件使用content</span></div><div class="line"><span class="comment"># 保存图片</span></div><div class="line"><span class="keyword">with</span> open(<span class="string">'logo.gif'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(resp.content)</div><div class="line">    print(<span class="string">'Ok'</span>)</div></pre></td></tr></table></figure><p>成功运行就可以看到打印出来的图片的二进制数据，可保存成功后打印的 OK，这个时候我们打开文件夹就可以看到下载下来的图片了。这几行代码就简单的演示了爬虫保存文件的过程。</p><h2 id="解析方式有哪些？"><a href="#解析方式有哪些？" class="headerlink" title="解析方式有哪些？"></a>解析方式有哪些？</h2><ul><li>直接处理，比如简单的页面文档，只要去除一些空格的数据；</li><li>Json解析，处理Ajax加载的页面；</li><li>正则表达式；</li><li>BeautifulSoup库；</li><li>PyQuery；</li><li>XPath。</li></ul><p>看到这里，大家是不是已经对爬虫的基本工作原理有了清晰的认识了呢。当然，罗马并不是一天建成的，只要积累了足够多的经验，大家肯定能成为爬虫大神的。</p><p>接下来，我会给大家继续分享一些我在学习爬虫时的经验总结。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大家好，今天周末，希望大家周末愉快。&lt;/p&gt;
&lt;p&gt;这篇文章我来梳理一下爬虫的基本原理。用过&lt;code&gt;Python&lt;/code&gt;的伙伴都知道&lt;code&gt;Python&lt;/code&gt;用来写爬虫是件很简单很爽的事情。但是有些伙伴不了解爬虫到底是什么，会纳闷为什么爬虫要设置什么请求头、请求体啊，到底要怎么样去发送这个请求啊。当然有的伙伴使用过很多爬虫的请求库，解析库，写过很多爬虫，但是可能他们的思路脉络也不是太清晰。那么，这篇文章就来帮大家理清爬虫原理的脉络。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="https://wyukun.coding.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://wyukun.coding.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://wyukun.coding.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>五月的事</title>
    <link href="https://wyukun.coding.me/2017/05/25/%E4%BA%94%E6%9C%88%E7%9A%84%E4%BA%8B/"/>
    <id>https://wyukun.coding.me/2017/05/25/五月的事/</id>
    <published>2017-05-25T11:43:14.000Z</published>
    <updated>2017-09-16T04:33:43.158Z</updated>
    
    <content type="html"><![CDATA[<p>之前无聊的时候写过很多博客，但是那时候这个网站还没有上线，大多博客都写在知乎上。但是一些牢骚话自然不会发在知乎的，都是保存了草稿，然后自己删掉，发布的都是 Python 的分享。毕竟申请的是 Python 专栏，<a href="https://zhuanlan.zhihu.com/yukun" target="_blank" rel="external">专栏地址</a>。其实这种感觉其实特别难受，好比打了一半的嗝又被咽回去了。<br><a id="more"></a></p><p><img src="http://imgout.ph.126.net/55814031/55814031.jpg" alt="image"></p><iframe src="//music.163.com/outchain/player?type=2&amp;id=31445772&amp;auto=0&amp;height=66" width="330" height="86" frameborder="no" marginwidth="0" marginheight="0"></iframe><p>转眼五月就要结束了，时间就像一匹欢快的小马，你永远抓不住他的尾巴。人家说天上一日，地上一年，大概就是说快乐地时光总是过得很快。上一次有时间的概念是网站备案的时候，阿里云说要等十几天才能备案成功，我才感觉时间漫长。一天一天过去，备案的十几天似乎还很遥远。可渐渐的忘记这件事的时候，时间的小马就匆匆跑过来告诉你，十几天又过去了，备案通过了，马上到六月了。</p><p>回想这个月，我也想不到自己到底干了些什么。处了这个网站成功上线，在知乎写了两篇博客，其他都没什么印象。每天重复的3点式生活占去了大部分记忆。不过也很好，毕竟这样也让我少去很多不必要的打扰。这个月窝在宿舍倒是把廖雪峰 JavaScript 的教程看了一遍，也算不枉这个月的时光。写到这我突然明白，如果一直这样与美丽的姑娘们毫无交集，可能就永远单身了=。=。</p><p>这个月开始天气开始热起来，越来越有夏天的感觉了。天气一热，心就开始浮躁，连看一部完整电影的耐心都没有。希望今后能静下心来，非宁静无以致远吗。毕竟专业课还有很多作业要补，下个月在不全部上交估计这门课要挂，欠下的总是要还的，这话说的一点没错。</p><p>今天是五月二十五日，在过五天就是端午节，虽然不会有多少人可以看到，我还是要提前衷心的祝大家端午快乐。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前无聊的时候写过很多博客，但是那时候这个网站还没有上线，大多博客都写在知乎上。但是一些牢骚话自然不会发在知乎的，都是保存了草稿，然后自己删掉，发布的都是 Python 的分享。毕竟申请的是 Python 专栏，&lt;a href=&quot;https://zhuanlan.zhihu.com/yukun&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;专栏地址&lt;/a&gt;。其实这种感觉其实特别难受，好比打了一半的嗝又被咽回去了。&lt;br&gt;
    
    </summary>
    
      <category term="生活" scheme="https://wyukun.coding.me/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="随笔" scheme="https://wyukun.coding.me/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
</feed>
